

<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="robots" content="noodp" />
    <title>Pytorch快速入门0 - Gsscsd</title><meta name="Description" content="时光划过指缝-阅读挽留时光"><meta property="og:title" content="Pytorch快速入门0" />
<meta property="og:description" content="为什么选择PyTorch 简洁：PyTorch的设计追求最少的封装，尽量避免重复造轮子。不像TensorFlow中充斥着session、gra" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gsscsd.github.io/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/" /><meta property="og:image" content="https://cdn.jsdelivr.net/gh/gsscsd/BlogImg/20220628173721.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-01-05T17:52:39+00:00" />
<meta property="article:modified_time" content="2019-01-05T17:52:39+00:00" /><meta property="og:site_name" content="Gsscsd" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/gsscsd/BlogImg/20220628173721.png"/>

<meta name="twitter:title" content="Pytorch快速入门0"/>
<meta name="twitter:description" content="为什么选择PyTorch 简洁：PyTorch的设计追求最少的封装，尽量避免重复造轮子。不像TensorFlow中充斥着session、gra"/>
<meta name="application-name" content="Gsscsd">
<meta name="apple-mobile-web-app-title" content="Gsscsd">

<meta name="theme-color" content="#f8f8f8"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="canonical" href="https://gsscsd.github.io/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/" /><link rel="prev" href="https://gsscsd.github.io/tensorflow%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B%E4%B9%8Bmnist/" /><link rel="next" href="https://gsscsd.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/" /><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/normalize.css@8.0.1/normalize.min.css"><link rel="stylesheet" href="/css/color.346090a8e4618da38ff45853421bbd1d5ffa3c25e678a3f0131c9f7d5300e15669f91c74f1ed5fab76e7424a2a6a5619a9dc45f99e04f246e37d8af51295b6ae.css" integrity="sha512-NGCQqORhjaOP9FhTQhu9HV/6PCXmeKPwExyffVMA4VZp&#43;Rx08e1fq3bnQkoqalYZqdxF&#43;Z4E8kbjfYr1EpW2rg=="><link rel="stylesheet" href="/css/style.min.4f2deec9ba839d309c76dabead5bd259854c6fea2f78f1b9e47e833e7e3cf0c5e44f9a4a044f2bad8ca08dddc52c872601c501966bb44b2b73968730a68fd556.css" integrity="sha512-Ty3uybqDnTCcdtq&#43;rVvSWYVMb&#43;ovePG55H6DPn488MXkT5pKBE8rrYygjd3FLIcmAcUBlmu0Sytzlocwpo/VVg=="><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css"></noscript><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Pytorch快速入门0",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/gsscsd.github.io\/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80\/"
        },"image": ["https:\/\/gsscsd.github.io\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "python, 深度学习, Pytorch","wordcount":  18825 ,
        "url": "https:\/\/gsscsd.github.io\/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80\/","datePublished": "2019-01-05T17:52:39+00:00","dateModified": "2019-01-05T17:52:39+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "Gsscsd","logo": "https:\/\/cdn.jsdelivr.net\/gh\/gsscsd\/BlogImg\/G_128px.ico"},"author": {
                "@type": "Person",
                "name": "Gsscsd"
            },"description": ""
    }
    </script><script src="//instant.page/5.1.1" defer type="module" integrity="sha384-MWfCL6g1OTGsbSwfuMHc8+8J2u71/LA8dzlIN3ycajckxuZZmF+DNjdm7O6H3PSq"></script>
</head>

<body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">
        function setTheme(theme) {document.body.setAttribute('theme', theme); document.documentElement.style.setProperty('color-scheme', theme === 'light' ? 'light' : 'dark'); window.theme = theme;   window.isDark = window.theme !== 'light' }
        function saveTheme(theme) {window.localStorage && localStorage.setItem('theme', theme);}
        function getMeta(metaName) {const metas = document.getElementsByTagName('meta'); for (let i = 0; i < metas.length; i++) if (metas[i].getAttribute('name') === metaName) return metas[i]; return '';}
        if (window.localStorage && localStorage.getItem('theme')) {let theme = localStorage.getItem('theme');theme === 'light' || theme === 'dark' || theme === 'black' ? setTheme(theme) : (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light')); } else { if ('auto' === 'light' || 'auto' === 'dark' || 'auto' === 'black') setTheme('auto'), saveTheme('auto'); else saveTheme('auto'), window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches ? setTheme('dark') : setTheme('light');}
        let metaColors = {'light': '#f8f8f8','dark': '#252627','black': '#000000'}
        getMeta('theme-color').content = metaColors[document.body.getAttribute('theme')];
        window.switchThemeEventSet = new Set()
    </script>
    <div id="back-to-top"></div>
    <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Gsscsd">Gsscsd</a>
        </div>
        <div class="menu">
            <div class="menu-inner">
                <a class="menu-item"
                    href="/posts/" > 文章 
                </a><a class="menu-item"
                    href="/tags/" > 标签 
                </a><a class="menu-item"
                    href="/categories/" > 分类 
                </a><a class="menu-item"
                    href="/about/" > 关于 
                </a>
                <div class="dropdown">
                    <a href="javascript:void(0);" 
                        class="menu-item menu-more dropbtn" title="" > 工具导航 
                    </a>
                    <div class="menu-more-content dropdown-content"><a href="http://www.chat.gsscsd.cn" title="" > chatgpt </a></div>
                </div>
                <a class="menu-item"
                    href="https://github.com/gsscsd"  title="GitHub" 
                    rel="noopener noreffer" target="_blank" ><i class='fab fa-github fa-fw' aria-hidden='true'></i>  
                </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                    <input type="text"
                        placeholder="搜索文章标题或内容..."
                        id="search-input-desktop">
                    <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop"
                        title="搜索">
                        <i class="fas fa-search fa-fw"></i>
                    </a>
                    <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop"
                        title="清空">
                        <i class="fas fa-times-circle fa-fw"></i>
                    </a>
                    <span class="search-button search-loading" id="search-loading-desktop">
                        <i class="fas fa-spinner fa-fw fa-spin"></i>
                    </span>
                </span><a href="javascript:void(0);" class="menu-item theme-switch" title="">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Gsscsd">Gsscsd</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                <div class="search mobile" id="search-mobile">
                    <input type="text"
                        placeholder="搜索文章标题或内容..."
                        id="search-input-mobile">
                    <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile"
                        title="搜索">
                        <i class="fas fa-search fa-fw"></i>
                    </a>
                    <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile"
                        title="清空">
                        <i class="fas fa-times-circle fa-fw"></i>
                    </a>
                    <span class="search-button search-loading" id="search-loading-mobile">
                        <i class="fas fa-spinner fa-fw fa-spin"></i>
                    </span>
                </div>
                <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                    取消
                </a>
            </div><a class="menu-item" href="/posts/"> 文章 
                </a><a class="menu-item" href="/tags/"> 标签 
                </a><a class="menu-item" href="/categories/"> 分类 
                </a><a class="menu-item" href="/about/"> 关于 
                </a>
                <div class="dropdown">
                    <a href="javascript:void(0);" class="menu-item menu-more dropbtn" title="" > 工具导航 
                    </a>
                    <div class="menu-more-content dropdown-content"><a href="http://www.chat.gsscsd.cn" title="" > chatgpt </a></div>
                </div>
            <a class="menu-item" href="https://github.com/gsscsd" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  
                </a>
            
            
            
            
            
            
            <a href="javascript:void(0);" class="menu-item theme-switch" title="">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>


<main class="main">
            <div class="container"><div class="toc" id="toc-auto">
        <h2 class="toc-title">目录</h2>
        <div class="toc-content" id="toc-content-auto"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#为什么选择pytorch">为什么选择<code>PyTorch</code></a></li>
        <li><a href="#pytorch的安装"><code>PyTorch</code>的安装</a></li>
        <li><a href="#pytorch的核心概念"><code>PyTorch</code>的核心概念</a>
          <ul>
            <li><a href="#tensor张量"><code>Tensor</code>：张量</a>
              <ul>
                <li><a href="#tensor属性"><code>Tensor</code>属性</a>
                  <ul>
                    <li><a href="#torchdtype"><code>torch.dtype</code></a></li>
                    <li><a href="#torchdevice"><code>torch.device</code></a></li>
                    <li><a href="#torchlayout"><code>torch.layout</code></a></li>
                  </ul>
                </li>
                <li><a href="#tensor方法"><code>Tensor</code>方法</a>
                  <ul>
                    <li><a href="#tensorcopy_src-asyncfalse"><code>Tensor.copy_(src, async=False)</code></a></li>
                    <li><a href="#tensorcudadevicenone-asyncfalse"><code>Tensor.cuda(device=None, async=False)</code></a></li>
                    <li><a href="#heading"></a></li>
                    <li><a href="#tensorexpandsizes"><code>Tensor.expand(*sizes)</code></a></li>
                    <li><a href="#tensornarrowdimension-start-length"><code>Tensor.narrow(*dimension, start, length*)</code></a></li>
                    <li><a href="#tensorresize_sizes"><code>Tensor.resize_(*sizes)</code></a></li>
                  </ul>
                </li>
                <li><a href="#详解tensor操作">详解<code>Tensor</code>操作</a>
                  <ul>
                    <li><a href="#创建tensor">创建Tensor</a></li>
                    <li><a href="#tensor的基本操作">Tensor的基本操作</a></li>
                    <li><a href="#tensor索引操作">Tensor索引操作</a></li>
                    <li><a href="#tensor元素操作">Tensor元素操作</a></li>
                    <li><a href="#tensor归并操作">Tensor归并操作</a></li>
                    <li><a href="#tensor比较操作">Tensor比较操作</a></li>
                    <li><a href="#tensor线性代数">Tensor线性代数</a></li>
                    <li><a href="#tensor广播法则">Tensor广播法则</a></li>
                  </ul>
                </li>
                <li><a href="#tensor内部结构">Tensor内部结构</a></li>
                <li><a href="#其他tensor使用技巧">其他<code>Tensor</code>使用技巧</a>
                  <ul>
                    <li><a href="#gpucpu">GPU/CPU</a></li>
                    <li><a href="#持久化">持久化</a></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><a href="#autograd自动微分"><code>autograd</code>：自动微分</a>
              <ul>
                <li><a href="#variable"><code>Variable</code></a></li>
                <li><a href="#autograd常用方法"><code>autograd</code>常用方法</a>
                  <ul>
                    <li><a href="#torchautogradbackwardtensors-grad_tensorsnone-retain_graphnone-create_graphfalse-grad_variablesnone"><code>torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)</code></a></li>
                    <li><a href="#torchautogradgradoutputs-inputs-grad_outputsnone-retain_graphnone-create_graphfalse-only_inputstrue-allow_unusedfalse"><code>torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)&gt;</code></a></li>
                  </ul>
                </li>
                <li><a href="#计算图">计算图</a></li>
              </ul>
            </li>
            <li><a href="#autograd高级用法"><code>autograd</code>高级用法</a></li>
          </ul>
        </li>
        <li><a href="#pytorch实例线性回归"><code>Pytorch</code>实例线性回归</a>
          <ul>
            <li><a href="#线性回归0">线性回归0</a></li>
            <li><a href="#线性回归1">线性回归1</a></li>
            <li><a href="#线性回归2">线性回归2</a></li>
          </ul>
        </li>
        <li><a href="#参考">参考</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
    </div><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC", "true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Pytorch快速入门0</h1><div class="post-meta">
            <div class="post-meta-line">
                <span class="post-author"><i class="author fas fa-user-circle fa-fw"></i><a href="/" title="Author" rel=" author" class="author">Gsscsd</a>
                </span>&nbsp;<span class="post-category">收录于 </span>&nbsp;<span class="post-category">类别 <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><i class="far fa-folder fa-fw"></i>深度学习</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime="2019-01-05">2019-01-05</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime="2019-01-05">2019-01-05</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 18825 字&nbsp;
                <i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 38 分钟&nbsp;<span id="/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/" class="leancloud_visitors" data-flag-title="Pytorch快速入门0">
                        <i class="far fa-eye fa-fw"></i>&nbsp;<span class="leancloud-visitors-count"></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#为什么选择pytorch">为什么选择<code>PyTorch</code></a></li>
        <li><a href="#pytorch的安装"><code>PyTorch</code>的安装</a></li>
        <li><a href="#pytorch的核心概念"><code>PyTorch</code>的核心概念</a>
          <ul>
            <li><a href="#tensor张量"><code>Tensor</code>：张量</a>
              <ul>
                <li><a href="#tensor属性"><code>Tensor</code>属性</a>
                  <ul>
                    <li><a href="#torchdtype"><code>torch.dtype</code></a></li>
                    <li><a href="#torchdevice"><code>torch.device</code></a></li>
                    <li><a href="#torchlayout"><code>torch.layout</code></a></li>
                  </ul>
                </li>
                <li><a href="#tensor方法"><code>Tensor</code>方法</a>
                  <ul>
                    <li><a href="#tensorcopy_src-asyncfalse"><code>Tensor.copy_(src, async=False)</code></a></li>
                    <li><a href="#tensorcudadevicenone-asyncfalse"><code>Tensor.cuda(device=None, async=False)</code></a></li>
                    <li><a href="#heading"></a></li>
                    <li><a href="#tensorexpandsizes"><code>Tensor.expand(*sizes)</code></a></li>
                    <li><a href="#tensornarrowdimension-start-length"><code>Tensor.narrow(*dimension, start, length*)</code></a></li>
                    <li><a href="#tensorresize_sizes"><code>Tensor.resize_(*sizes)</code></a></li>
                  </ul>
                </li>
                <li><a href="#详解tensor操作">详解<code>Tensor</code>操作</a>
                  <ul>
                    <li><a href="#创建tensor">创建Tensor</a></li>
                    <li><a href="#tensor的基本操作">Tensor的基本操作</a></li>
                    <li><a href="#tensor索引操作">Tensor索引操作</a></li>
                    <li><a href="#tensor元素操作">Tensor元素操作</a></li>
                    <li><a href="#tensor归并操作">Tensor归并操作</a></li>
                    <li><a href="#tensor比较操作">Tensor比较操作</a></li>
                    <li><a href="#tensor线性代数">Tensor线性代数</a></li>
                    <li><a href="#tensor广播法则">Tensor广播法则</a></li>
                  </ul>
                </li>
                <li><a href="#tensor内部结构">Tensor内部结构</a></li>
                <li><a href="#其他tensor使用技巧">其他<code>Tensor</code>使用技巧</a>
                  <ul>
                    <li><a href="#gpucpu">GPU/CPU</a></li>
                    <li><a href="#持久化">持久化</a></li>
                  </ul>
                </li>
              </ul>
            </li>
            <li><a href="#autograd自动微分"><code>autograd</code>：自动微分</a>
              <ul>
                <li><a href="#variable"><code>Variable</code></a></li>
                <li><a href="#autograd常用方法"><code>autograd</code>常用方法</a>
                  <ul>
                    <li><a href="#torchautogradbackwardtensors-grad_tensorsnone-retain_graphnone-create_graphfalse-grad_variablesnone"><code>torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)</code></a></li>
                    <li><a href="#torchautogradgradoutputs-inputs-grad_outputsnone-retain_graphnone-create_graphfalse-only_inputstrue-allow_unusedfalse"><code>torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)&gt;</code></a></li>
                  </ul>
                </li>
                <li><a href="#计算图">计算图</a></li>
              </ul>
            </li>
            <li><a href="#autograd高级用法"><code>autograd</code>高级用法</a></li>
          </ul>
        </li>
        <li><a href="#pytorch实例线性回归"><code>Pytorch</code>实例线性回归</a>
          <ul>
            <li><a href="#线性回归0">线性回归0</a></li>
            <li><a href="#线性回归1">线性回归1</a></li>
            <li><a href="#线性回归2">线性回归2</a></li>
          </ul>
        </li>
        <li><a href="#参考">参考</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h3 id="为什么选择pytorch" class="headerLink">
    <a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e9%80%89%e6%8b%a9pytorch" class="header-mark"></a>为什么选择<code>PyTorch</code></h3><blockquote>
<ul>
<li>简洁：PyTorch的设计追求最少的封装，尽量避免重复造轮子。不像TensorFlow中充斥着<code>session、graph、operation、name_scope、variable、tensor、layer</code>等全新的概念，PyTorch的设计遵循<code>tensor→autograd→nn.Module </code>三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。</li>
<li>速度：PyTorch的灵活性不以速度为代价，在许多评测中，PyTorch的速度表现胜过TensorFlow和Keras等框架 。框架的运行速度和程序员的编码水平有极大关系，但同样的算法，使用PyTorch实现的那个更有可能快过用其他框架实现的。</li>
<li>易用：PyTorch是所有的框架中面向对象设计的最优雅的一个。PyTorch的面向对象的接口设计来源于Torch，而Torch的接口设计以灵活易用而著称，Keras作者最初就是受Torch的启发才开发了Keras。PyTorch继承了Torch的衣钵，尤其是API的设计和模块的接口都与Torch高度一致。PyTorch的设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法，即所思即所得，不需要考虑太多关于框架本身的束缚。</li>
<li>活跃的社区：PyTorch提供了完整的文档，循序渐进的指南，作者亲自维护的论坛 供用户交流和求教问题。Facebook 人工智能研究院对PyTorch提供了强力支持，作为当今排名前三的深度学习研究机构，FAIR的支持足以确保PyTorch获得持续的开发更新，不至于像许多由个人开发的框架那样昙花一现。</li>
</ul>
<!--more -->
<p>PyTorch还有一个优点就是<strong>Torch</strong>自称为神经网络界的<strong>Numpy</strong>，它能将<strong>torch</strong>产生的<strong>tensor</strong>放在GPU中加速运算，就想Numpy会把array放在CPU中加速运算。所以在神经网络中，用Torch的tensor形式更优。我们可以把Pytorch当做Numpy来用。</p>
<p>PyTorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建，所以它能够使用Python控制语句（如for、if等）根据需求创建计算图。这点在自然语言处理领域中很有用，它意味着你不需要事先构建所有可能用到的图的路径，图在运行时才构建。</p>
</blockquote>
<h3 id="pytorch的安装" class="headerLink">
    <a href="#pytorch%e7%9a%84%e5%ae%89%e8%a3%85" class="header-mark"></a><code>PyTorch</code>的安装</h3><blockquote>
<ul>
<li>pip安装方式</li>
<li>conda安装方式</li>
</ul>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl"><span class="c1"># win+python3.6</span>
</span></span><span class="line"><span class="cl">pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-win_amd64.whl
</span></span><span class="line"><span class="cl">pip3 install torchvision
</span></span><span class="line"><span class="cl"><span class="c1"># win+python3.6+conda</span>
</span></span><span class="line"><span class="cl">conda install pytorch torchvision cuda80 -c pytorch
</span></span></code></pre></td></tr></table>
</div>
</div><p><a href="https://pytorch.org/get-started/locally/" target="_blank" rel="noopener noreferrer">更多方法参见此处</a></p>
<h3 id="pytorch的核心概念" class="headerLink">
    <a href="#pytorch%e7%9a%84%e6%a0%b8%e5%bf%83%e6%a6%82%e5%bf%b5" class="header-mark"></a><code>PyTorch</code>的核心概念</h3><h4 id="tensor张量" class="headerLink">
    <a href="#tensor%e5%bc%a0%e9%87%8f" class="header-mark"></a><code>Tensor</code>：张量</h4><blockquote>
<p>Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。</p>
<p>Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。</p>
<p><code>torch.Tensor</code>是一种包含单一数据类型元素的多维矩阵。</p>
</blockquote>
<h5 id="tensor属性" class="headerLink">
    <a href="#tensor%e5%b1%9e%e6%80%a7" class="header-mark"></a><code>Tensor</code>属性</h5><blockquote>
<p>每个<code>torch.Tensor</code>都有<code>torch.dtype</code>, <code>torch.device</code>,和<code>torch.layout</code>。</p>
</blockquote>
<h6 id="torchdtype" class="headerLink">
    <a href="#torchdtype" class="header-mark"></a><code>torch.dtype</code></h6><blockquote>
<p>Torch定义了七种CPU张量类型和八种GPU张量类型：</p>
</blockquote>
<table>
<thead>
<tr>
<th>Data tyoe</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point</td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64-bit floating point</td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16-bit floating point</td>
<td>N/A</td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
</tbody>
</table>
<h6 id="torchdevice" class="headerLink">
    <a href="#torchdevice" class="header-mark"></a><code>torch.device</code></h6><blockquote>
<ul>
<li><code>torch.device</code>代表将<code>torch.Tensor</code>分配到的设备的对象。</li>
<li><code>torch.device</code>包含一个设备类型（<code>'cpu'</code>或<code>'cuda'</code>设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，<code>torch.Tensor</code>用设备构建<code>'cuda'</code>的结果等同于<code>'cuda:X'</code>,其中<code>X</code>是<code>torch.cuda.current_device()</code>的结果。</li>
<li><code>torch.Tensor</code>的设备可以通过<code>Tensor.device</code>访问属性。</li>
<li>构造<code>torch.device</code>可以通过字符串/字符串和设备编号。</li>
</ul>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># device(type=&#39;cuda&#39;, index=0)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># device(type=&#39;cpu&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># device(type=&#39;cuda&#39;, index=0)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><strong>注意</strong>
<code>torch.device</code>函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Example of a function that takes in a torch.device</span>
</span></span><span class="line"><span class="cl"><span class="n">cuda1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">cuda1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># You can substitute the torch.device with a string</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;cuda:1&#39;</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h6 id="torchlayout" class="headerLink">
    <a href="#torchlayout" class="header-mark"></a><code>torch.layout</code></h6><blockquote>
<ul>
<li><code>torch.layout</code>表示<code>torch.Tensor</code>内存布局的对象。目前，我们支持<code>torch.strided(dense Tensors)</code>并为<code>torch.sparse_coo(sparse COO Tensors)</code>提供实验支持。</li>
<li><code>torch.strided</code>代表密集张量，是最常用的内存布局。每个<code>strided</code>张量都会关联 一个<code>torch.Storage</code>，它保存着它的数据。这些张力提供了多维度， 存储的<code>strided</code>视图。<code>Strides</code>是一个整数型列表：<code>k-th stride</code>表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。</li>
</ul>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (5, 1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (1, 5)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="tensor方法" class="headerLink">
    <a href="#tensor%e6%96%b9%e6%b3%95" class="header-mark"></a><code>Tensor</code>方法</h5><blockquote>
<p>Tensor的方法中，带有<code>_</code>的方法代表能够修改Tensor本身。比如，<code>torch.FloatTensor.abs_()</code>会在原地计算绝对值并返回修改的张量，而<code>tensor.FloatTensor.abs()</code>将会在新张量中计算结果。</p>
</blockquote>
<h6 id="tensorcopy_src-asyncfalse" class="headerLink">
    <a href="#tensorcopy_src-asyncfalse" class="header-mark"></a><code>Tensor.copy_(src, async=False)</code></h6><blockquote>
<p>将<code>src</code>中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。</p>
<p>参数：</p>
<ul>
<li>src（Tensor） - 要复制的源张量</li>
<li>async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。</li>
<li>broadcast（bool） - 如果为True，src将广播到底层张量的形状。</li>
</ul>
</blockquote>
<h6 id="tensorcudadevicenone-asyncfalse" class="headerLink">
    <a href="#tensorcudadevicenone-asyncfalse" class="header-mark"></a><code>Tensor.cuda(device=None, async=False)</code></h6><h6 id="heading" class="headerLink">
    <a href="#heading" class="header-mark"></a></h6><blockquote>
<p>返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。</p>
<p>参数：</p>
<ul>
<li>device（int） ：目标GPU ID。默认为当前设备。</li>
<li>async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。</li>
</ul>
</blockquote>
<h6 id="tensorexpandsizes" class="headerLink">
    <a href="#tensorexpandsizes" class="header-mark"></a><code>Tensor.expand(*sizes)</code></h6><blockquote>
<p>返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将<code>stride</code>设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。</p>
<p>参数：</p>
<ul>
<li>sizes(torch.Size or int&hellip;)-需要扩展的大小</li>
</ul>
</blockquote>
<h6 id="tensornarrowdimension-start-length" class="headerLink">
    <a href="#tensornarrowdimension-start-length" class="header-mark"></a><code>Tensor.narrow(*dimension, start, length*)</code></h6><blockquote>
<p>返回这个张量的缩小版本的新张量。维度<code>dim</code>缩小范围是<code>start</code>到<code>start+length</code>。返回的张量和该张量共享相同的底层存储。</p>
<p>参数：</p>
<ul>
<li>dimension (<em>int</em>)-需要缩小的维度</li>
<li>start (<em>int</em>)-起始维度</li>
<li>length (<em>int</em>)-长度</li>
</ul>
</blockquote>
<h6 id="tensorresize_sizes" class="headerLink">
    <a href="#tensorresize_sizes" class="header-mark"></a><code>Tensor.resize_(*sizes)</code></h6><blockquote>
<p>将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。</p>
<p>参数：</p>
<ul>
<li>sizes (torch.Size or int&hellip;)-需要调整的大小</li>
</ul>
</blockquote>
<p><a href="https://ptorch.com/docs/8/torch-tensor" target="_blank" rel="noopener noreferrer">更多的方法参考此处</a></p>
<p>下面通过一些实例学习，Tensor的使用。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 构建 5x3 矩阵，只是分配了空间，未初始化</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 1.,  2.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 3.,  4.]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 使用[0,1]均匀分布随机初始化二维数组</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.8052,  0.7188,  0.0332],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.6054,  0.8955,  0.8972],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.1107,  0.3319,  0.0336],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.2394,  0.5188,  0.2201],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.9730,  0.9370,  0.5677]])</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 查看列的个数, 两种写法等价</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (3,3)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><code>torch.Size</code> 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0]</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 加减乘除运算</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 加法的第一种写法</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">+</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.9639,  0.8763,  0.2834],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.3785,  1.5090,  1.3919],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7139,  0.6348,  0.8439],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7022,  1.5079,  0.4776],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.7892,  1.6383,  0.7774]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 加法的第二种写法</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.9639,  0.8763,  0.2834],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.3785,  1.5090,  1.3919],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7139,  0.6348,  0.8439],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7022,  1.5079,  0.4776],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.7892,  1.6383,  0.7774]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 加法的第三种写法：指定加法结果的输出目标为result</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 预先分配空间</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">result</span><span class="p">)</span> <span class="c1"># 输入到result</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.9639,  0.8763,  0.2834],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.3785,  1.5090,  1.3919],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7139,  0.6348,  0.8439],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7022,  1.5079,  0.4776],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.7892,  1.6383,  0.7774]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;最初y&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.1587,  0.1575,  0.2501],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7732,  0.6135,  0.4947],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.6033,  0.3029,  0.8103],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.4628,  0.9891,  0.2575],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.8163,  0.7013,  0.2097]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;第一种加法，y的结果&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># 普通加法，不改变y的内容</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第一种加法，y的结果</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.1587,  0.1575,  0.2501],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7732,  0.6135,  0.4947],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.6033,  0.3029,  0.8103],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.4628,  0.9891,  0.2575],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.8163,  0.7013,  0.2097]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;第二种加法，y的结果&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># inplace 加法，y变了</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第二种加法，y的结果</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.9639,  0.8763,  0.2834],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.3785,  1.5090,  1.3919],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7139,  0.6348,  0.8439],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.7022,  1.5079,  0.4776],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.7892,  1.6383,  0.7774]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意，函数名后面带下划线**<code>_</code>** 的函数会修改Tensor本身。例如，<code>x.add_(y)</code>和<code>x.t_()</code>会改变 <code>x</code>，但<code>x.add(y)</code>和<code>x.t()</code>返回一个新的Tensor， 而<code>x</code>不变。</p>
<p>Tensor还支持很多操作，包括数学运算、线性代数、选择、切片等等，其接口设计与Numpy极为相似。更详细的使用方法。</p>
<p>Tensor和numpy对象共享内存，所以他们之间的转换很快，而且几乎不会消耗什么资源。但这也意味着，如果其中一个变了，另外一个也会随之改变。</p>
<p>Tensor和Numpy的数组之间的互操作非常容易且快速。对于Tensor不支持的操作，可以先转为Numpy数组处理，之后再转回Tensor。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 新建一个全1的Tensor</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 1.,  1.,  1.,  1.,  1.])</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># Tensor -&gt; Numpy</span>
</span></span><span class="line"><span class="cl"><span class="c1"># array([1., 1., 1., 1., 1.], dtype=float32)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="c1"># Numpy-&gt;Tensor</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [1. 1. 1. 1. 1.]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 1.,  1.,  1.,  1.,  1.], dtype=torch.float64)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 共享内存，修改numpy会修改tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 以`_`结尾的函数会修改自身</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [2. 2. 2. 2. 2.]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="c1"># Tensor和Numpy共享内存</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>如果你想获取某一个元素的值，可以使用<code>scalar.item</code>。 直接<code>tensor[idx]</code>得到的还是一个tensor: 一个0-dim 的tensor，一般称为scalar.</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">scalar</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(2., dtype=torch.float64)</span>
</span></span><span class="line"><span class="cl"><span class="n">scalar</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="c1">#0-dim</span>
</span></span><span class="line"><span class="cl"><span class="c1"># torch.Size([])</span>
</span></span><span class="line"><span class="cl"><span class="n">scalar</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="c1"># 使用scalar.item()能从中取出python对象的数值</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 2.0</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">])</span> <span class="c1"># 注意和scalar的区别</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span><span class="p">,</span><span class="n">scalar</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (tensor([ 2]), tensor(2., dtype=torch.float64))</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span><span class="n">scalar</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (torch.Size([1]), torch.Size([]))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 只有一个元素的tensor也可以调用`tensor.item()`</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">scalar</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (2, 2.0)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>PyTorch中还有一个和<code>np.array</code> 很类似的接口: <code>torch.tensor</code>, 二者的使用十分类似。</p>
<p>需要注意的是，<code>t.tensor()</code>总是会进行数据拷贝，新tensor和原来的数据不再共享内存。所以如果你想共享内存的话，建议使用<code>torch.from_numpy()</code>或者<code>tensor.detach()</code>来新建一个tensor, 二者共享内存。</p>
<p>Tensor可通过<code>.cuda</code> 方法转为GPU的Tensor，从而享受GPU带来的加速运算。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">tensor</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span> <span class="c1"># 新建一个包含 3，4 两个元素的tensor</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 以下可以看到，新建的tensor与原来的数据不共享内存</span>
</span></span><span class="line"><span class="cl"><span class="n">old_tensor</span> <span class="o">=</span> <span class="n">tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">new_tensor</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">old_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">new_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1111</span>
</span></span><span class="line"><span class="cl"><span class="c1"># old_tensor, new_tensor</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (tensor([ 3,  4]), tensor([ 1111,     4]))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 以下使用detach新建tensor会共享内存</span>
</span></span><span class="line"><span class="cl"><span class="n">new_tensor</span> <span class="o">=</span> <span class="n">old_tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">new_tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1111</span>
</span></span><span class="line"><span class="cl"><span class="c1"># old_tensor, new_tensor</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (tensor([ 1111,     4]), tensor([ 1111,     4]))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="详解tensor操作" class="headerLink">
    <a href="#%e8%af%a6%e8%a7%a3tensor%e6%93%8d%e4%bd%9c" class="header-mark"></a>详解<code>Tensor</code>操作</h5><blockquote>
<p>接口的角度来讲，对tensor的操作可分为两类：</p>
<ol>
<li><code>torch.function</code>，如<code>torch.save</code>等。</li>
<li>另一类是<code>tensor.function</code>，如<code>tensor.view</code>等。</li>
</ol>
<p>为方便使用，对tensor的大部分操作同时支持这两类接口，</p>
<p>而从存储的角度来讲，对tensor的操作又可分为两类：</p>
<ol>
<li>不会修改自身的数据，如 <code>a.add(b)</code>， 加法的结果会返回一个新的tensor。</li>
<li>会修改自身的数据，如 <code>a.add_(b)</code>， 加法的结果仍存储在a中，a被修改了。</li>
</ol>
<p>函数名以<code>_</code>结尾的都是inplace方式, 即会修改调用者自己的数据，在实际应用中需加以区分。</p>
</blockquote>
<h6 id="创建tensor" class="headerLink">
    <a href="#%e5%88%9b%e5%bb%batensor" class="header-mark"></a>创建Tensor</h6><blockquote>
<p>在PyTorch中新建tensor的方法有很多，具体可以参见下表。</p>
</blockquote>
<p>常见新建tensor的方法</p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Tensor(*sizes)</td>
<td style="text-align:center">基础构造函数</td>
</tr>
<tr>
<td style="text-align:center">tensor(data,)</td>
<td style="text-align:center">类似np.array的构造函数</td>
</tr>
<tr>
<td style="text-align:center">ones(*sizes)</td>
<td style="text-align:center">全1Tensor</td>
</tr>
<tr>
<td style="text-align:center">zeros(*sizes)</td>
<td style="text-align:center">全0Tensor</td>
</tr>
<tr>
<td style="text-align:center">eye(*sizes)</td>
<td style="text-align:center">对角线为1，其他为0</td>
</tr>
<tr>
<td style="text-align:center">arange(s,e,step</td>
<td style="text-align:center">从s到e，步长为step</td>
</tr>
<tr>
<td style="text-align:center">linspace(s,e,steps)</td>
<td style="text-align:center">从s到e，均匀切分成steps份</td>
</tr>
<tr>
<td style="text-align:center">rand/randn(*sizes)</td>
<td style="text-align:center">均匀/标准分布</td>
</tr>
<tr>
<td style="text-align:center">normal(mean,std)/uniform(from,to)</td>
<td style="text-align:center">正态分布/均匀分布</td>
</tr>
<tr>
<td style="text-align:center">randperm(m)</td>
<td style="text-align:center">随机排列</td>
</tr>
</tbody>
</table>
<blockquote>
<p>这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu).</p>
<p>其中使用<code>Tensor</code>函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor。</p>
<p>PS:<code>t.Tensor(*sizes)</code>创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配，而其它操作都是在创建完tensor之后马上进行空间分配。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 1.指定tensor的形状</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 2.用list的数据创建tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],[</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">b_size</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="c1"># 把tensor转为list</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="c1"># b中元素总个数，2*3，等价于b.nelement()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 3.创建一个和b形状一样的tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">b_size</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 4.创建一个元素为2和3的tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">d</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 其他的新建Tensor的操作</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 全1</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1">#　全0</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1">#　生成序列</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 生成序列</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span> <span class="c1"># 生成随机数</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># 长度为5的随机排列</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">int</span><span class="p">)</span> <span class="c1"># 对角线为1, 不要求行列数一致</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># torch.tensor()是新增加的函数，使用的方法，和参数几乎和`np.array`完全一致</span>
</span></span><span class="line"><span class="cl"><span class="n">scalar</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.14159</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;scalar: </span><span class="si">%s</span><span class="s1">, shape of sclar: </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span><span class="p">(</span><span class="n">scalar</span><span class="p">,</span> <span class="n">scalar</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1">#　scalar: tensor(3.1416), shape of sclar: torch.Size([])</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.11111</span><span class="p">,</span> <span class="mf">0.222222</span><span class="p">,</span> <span class="mf">0.3333333</span><span class="p">]],</span>
</span></span><span class="line"><span class="cl">                     <span class="n">dtype</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">                     <span class="n">device</span><span class="o">=</span><span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64)</span>
</span></span><span class="line"><span class="cl"><span class="n">empty_tensor</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">tensor</span><span class="p">([])</span>
</span></span><span class="line"><span class="cl"><span class="n">empty_tensor</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="c1"># torch.Size([0])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h6 id="tensor的基本操作" class="headerLink">
    <a href="#tensor%e7%9a%84%e5%9f%ba%e6%9c%ac%e6%93%8d%e4%bd%9c" class="header-mark"></a>Tensor的基本操作</h6><blockquote>
<p>通过<code>tensor.view</code>方法可以调整tensor的形状，但必须保证调整前后元素总数一致。<code>view</code>不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。</p>
<p>在实际应用中可能经常需要添加或减少某一维度，这时候<code>squeeze</code>和<code>unsqueeze</code>两个函数就派上用场了。<code>squeeze</code>降维，<code>unsqueeze</code>升维。</p>
<p><code>resize</code>是另一种可用来调整<code>size</code>的方法，但与<code>view</code>不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.,  1.,  2.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 3.,  4.,  5.]])</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 当某一维为-1的时候，会自动计算它的大小</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="c1"># torch.Size([2, 3])</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 注意形状，在第1维（下标从0开始）上增加“１” 维</span>
</span></span><span class="line"><span class="cl"><span class="c1">#等价于 b[:,None]</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="c1"># torch.Size([2, 1, 3])</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># -2表示倒数第二个维度上面增加“1’维</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="c1"># torch.Size([1, 1, 1, 2, 3])</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 压缩第0维的“１”</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="c1"># torch.Size([1, 1, 2, 3])</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="c1"># 把所有维度为“1”的压缩降维</span>
</span></span><span class="line"><span class="cl"><span class="c1"># view之后和原来的数据共享</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="c1"># a修改，b作为view之后的，也会跟着修改</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[   0.,  100.,    2.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [   3.,    4.,    5.]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># reseize尺寸小于原尺寸，部分数据会被保留，不显示</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[   0.,  100.,    2.]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># resize尺寸大于原尺寸，如果有隐藏数据会显示，其他多出的大小会被分配新空间</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">resize_</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 旧的数据依旧保存着，多出的大小会分配新空间</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[   0.0000,  100.0000,    2.0000],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [   3.0000,    4.0000,    5.0000],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [  -0.0000,    0.0000,    0.0000]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h6 id="tensor索引操作" class="headerLink">
    <a href="#tensor%e7%b4%a2%e5%bc%95%e6%93%8d%e4%bd%9c" class="header-mark"></a>Tensor索引操作</h6><blockquote>
<p>Tensor支持与numpy.ndarray类似的索引操作，语法上也类似。</p>
</blockquote>
<h6 id="tensor元素操作" class="headerLink">
    <a href="#tensor%e5%85%83%e7%b4%a0%e6%93%8d%e4%bd%9c" class="header-mark"></a>Tensor元素操作</h6><blockquote>
<p>这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如下表所示。</p>
</blockquote>
<p>常见的逐元素操作</p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">abs/sqrt/div/exp/fmod/log/pow..</td>
<td style="text-align:center">绝对值/平方根/除法/指数/求余/求幂..</td>
</tr>
<tr>
<td style="text-align:center">cos/sin/asin/atan2/cosh..</td>
<td style="text-align:center">相关三角函数</td>
</tr>
<tr>
<td style="text-align:center">ceil/round/floor/trunc</td>
<td style="text-align:center">上取整/四舍五入/下取整/只保留整数部分</td>
</tr>
<tr>
<td style="text-align:center">clamp(input, min, max)</td>
<td style="text-align:center">超过min和max部分截断</td>
</tr>
<tr>
<td style="text-align:center">sigmod/tanh..</td>
<td style="text-align:center">激活函数</td>
</tr>
</tbody>
</table>
<p>对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如<code>a ** 2</code> 等价于<code>torch.pow(a,2)</code>, <code>a * 2</code>等价于<code>torch.mul(a,2)</code>。</p>
<p>其中<code>clamp(x, min, max)</code>的输出满足以下公式：</p>
<p>$$
y_i =
\begin{cases}
min,  &amp; \text{if  } x_i \lt min \
x_i,  &amp; \text{if  } min \le x_i \le max  \
max,  &amp; \text{if  } x_i \gt max\
\end{cases}
$$</p>
<p><code>clamp</code>常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成数据</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1.0000000000, 0.5403022766, -0.4161468446],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [-0.9899924994, -0.6536436081, 0.2836622000]])</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">%</span> <span class="mi">3</span> <span class="c1"># 等价于t.fmod(a, 3)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.,  1.,  2.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.,  1.,  2.]])</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 3.,  3.,  3.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 3.,  4.,  5.]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h6 id="tensor归并操作" class="headerLink">
    <a href="#tensor%e5%bd%92%e5%b9%b6%e6%93%8d%e4%bd%9c" class="header-mark"></a>Tensor归并操作</h6><blockquote>
<p>此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法<code>sum</code>，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如下表所示。</p>
</blockquote>
<p>常用归并操作</p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">mean/sum/median/mode</td>
<td style="text-align:center">均值/和/中位数/众数</td>
</tr>
<tr>
<td style="text-align:center">norm/dist</td>
<td style="text-align:center">范数/距离</td>
</tr>
<tr>
<td style="text-align:center">std/var</td>
<td style="text-align:center">标准差/方差</td>
</tr>
<tr>
<td style="text-align:center">cumsum/cumprod</td>
<td style="text-align:center">累加/累乘</td>
</tr>
</tbody>
</table>
<blockquote>
<p>以上大多数函数都有一个参数**<code>dim</code>**，用来指定这些操作是在哪个维度上执行的。关于<code>dim</code>(对应于<code>Numpy</code>中的<code>axis</code>)的解释众说纷纭，这里提供一个简单的记忆方式：</p>
<p>假设输入的形状是(m, n, k)</p>
<ul>
<li>如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)</li>
<li>如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)</li>
<li>如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)</li>
</ul>
<p>size中是否有&quot;1&quot;，取决于参数<code>keepdim</code>，<code>keepdim=True</code>会保留维度<code>1</code>。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如<code>cumsum</code>。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成数据</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 2.,  2.,  2.]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># keepdim=False，不保留维度&#34;1&#34;，注意形状</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 2.,  2.,  2.])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h6 id="tensor比较操作" class="headerLink">
    <a href="#tensor%e6%af%94%e8%be%83%e6%93%8d%e4%bd%9c" class="header-mark"></a>Tensor比较操作</h6><blockquote>
<p>比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如下表所示。</p>
</blockquote>
<p>常用比较函数</p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">gt/lt/ge/le/eq/ne</td>
<td style="text-align:center">大于/小于/大于等于/小于等于/等于/不等</td>
</tr>
<tr>
<td style="text-align:center">topk</td>
<td style="text-align:center">最大的k个数</td>
</tr>
<tr>
<td style="text-align:center">sort</td>
<td style="text-align:center">排序</td>
</tr>
<tr>
<td style="text-align:center">max/min</td>
<td style="text-align:center">比较两个tensor最大最小值</td>
</tr>
</tbody>
</table>
<blockquote>
<p>表中第一行的比较操作已经实现了运算符重载，因此可以使用<code>a&gt;=b</code>、<code>a&gt;b</code>、<code>a!=b</code>、<code>a==b</code>，其返回结果是一个<code>ByteTensor</code>，可用来选取元素。<code>max/min</code>这两个操作比较特殊，以max来说，它有以下三种使用情况：</p>
<ul>
<li>t.max(tensor)：返回tensor中最大的一个数</li>
<li>t.max(tensor,dim)：指定维上最大的数，返回tensor和下标</li>
<li>t.max(tensor1, tensor2): 比较两个tensor相比较大的元素</li>
</ul>
<p>至于比较一个tensor和一个数，可以使用clamp函数。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成数据a</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[  0.,   3.,   6.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [  9.,  12.,  15.]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 生成数据b</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 15.,  12.,   9.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [  6.,   3.,   0.]])</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="p">[</span><span class="n">a</span><span class="o">&gt;</span><span class="n">b</span><span class="p">]</span> <span class="c1"># a中大于b的元素</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([  9.,  12.,  15.])</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># 第一个返回值的15和6分别表示第0行和第1行最大的元素</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第二个返回值的0和0表示上述最大的数是该行第0个元素</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (tensor([ 15.,   6.]), tensor([ 0,  0]))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h6 id="tensor线性代数" class="headerLink">
    <a href="#tensor%e7%ba%bf%e6%80%a7%e4%bb%a3%e6%95%b0" class="header-mark"></a>Tensor线性代数</h6><blockquote>
<p><code>PyTorch</code>的线性函数主要封装了<code>Blas</code>和<code>Lapack</code>，其用法和接口都与<code>Numpy</code>类似。常用的线性代数函数如下表所示。</p>
</blockquote>
<p>常用的线性代数函数</p>
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">trace</td>
<td style="text-align:center">对角线元素之和(矩阵的迹)</td>
</tr>
<tr>
<td style="text-align:center">diag</td>
<td style="text-align:center">对角线元素</td>
</tr>
<tr>
<td style="text-align:center">triu/tril</td>
<td style="text-align:center">矩阵的上三角/下三角，可指定偏移量</td>
</tr>
<tr>
<td style="text-align:center">mm/bmm</td>
<td style="text-align:center">矩阵乘法，batch的矩阵乘法</td>
</tr>
<tr>
<td style="text-align:center">addmm/addbmm/addmv/addr/badbmm..</td>
<td style="text-align:center">矩阵运算</td>
</tr>
<tr>
<td style="text-align:center">t</td>
<td style="text-align:center">转置</td>
</tr>
<tr>
<td style="text-align:center">dot/cross</td>
<td style="text-align:center">内积/外积</td>
</tr>
<tr>
<td style="text-align:center">inverse</td>
<td style="text-align:center">求逆矩阵</td>
</tr>
<tr>
<td style="text-align:center">svd</td>
<td style="text-align:center">奇异值分解</td>
</tr>
</tbody>
</table>
<blockquote>
<p>具体使用说明请参见<a href="http://pytorch.org/docs/torch.html#blas-and-lapack-operations" target="_blank" rel="noopener noreferrer">官方文档</a>，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的<code>.contiguous</code>方法将其转为连续。</p>
</blockquote>
<h6 id="tensor广播法则" class="headerLink">
    <a href="#tensor%e5%b9%bf%e6%92%ad%e6%b3%95%e5%88%99" class="header-mark"></a>Tensor广播法则</h6><blockquote>
<p>广播法则(<code>broadcast</code>)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。
<code>Numpy</code>的广播法则定义如下：</p>
<ul>
<li>让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐</li>
<li>两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算</li>
<li>当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状</li>
</ul>
<p>PyTorch当前已经支持了自动广播法则，但是通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：</p>
<ul>
<li><code>unsqueeze</code>或者<code>view</code>，或者<code>tensor[None]</code>,：为数据某一维的形状补1，实现法则1</li>
<li><code>expand</code>或者<code>expand_as</code>，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。</li>
</ul>
<p>注意，<code>repeat</code>实现与<code>expand</code>相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 生成数据a和b</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 自动广播法则</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，</span>
</span></span><span class="line"><span class="cl"><span class="c1">#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第二步:   a和b在第一维和第三维形状不一样，其中一个为1 ，</span>
</span></span><span class="line"><span class="cl"><span class="c1">#               可以利用广播法则扩展，两个形状都变成了（2，3，2）</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="o">+</span><span class="n">b</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[[ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [ 1.,  1.]],</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#         [[ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [ 1.,  1.]]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 手动广播法则</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 或者 a.view(1,3,2).expand(2,3,2) + b.expand(2,3,2)</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="p">[</span><span class="kc">None</span><span class="p">]</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[[ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [ 1.,  1.]],</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#         [[ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [ 1.,  1.]]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存</span>
</span></span><span class="line"><span class="cl"><span class="n">e</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">10000000000000</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="tensor内部结构" class="headerLink">
    <a href="#tensor%e5%86%85%e9%83%a8%e7%bb%93%e6%9e%84" class="header-mark"></a>Tensor内部结构</h5><blockquote>
<p><code>tensor</code>的数据结构如下图所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。</p>
<p>一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。</p>
<p><figure><a class="lightgallery" href="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/tensor_data_structure.svg" title="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/tensor_data_structure.svg" data-thumbnail="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/tensor_data_structure.svg">
        <img
            
            loading="lazy"
            src="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/tensor_data_structure.svg"
            srcset="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/tensor_data_structure.svg, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/tensor_data_structure.svg 1.5x, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/tensor_data_structure.svg 2x"
            sizes="auto"
            alt="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/tensor_data_structure.svg">
    </a></figure></p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># 生成数据a</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 查看a的storage</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  0.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  1.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  2.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  3.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  4.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  5.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [torch.FloatStorage of size 6]</span>
</span></span><span class="line"><span class="cl"><span class="c1"># b由a view生成</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 查看b的storage</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  0.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  1.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  2.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  3.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  4.0</span>
</span></span><span class="line"><span class="cl"><span class="c1">#  5.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [torch.FloatStorage of size 6]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 一个对象的id值可以看作它在内存中的地址</span>
</span></span><span class="line"><span class="cl"><span class="c1"># storage的内存地址一样，即是同一个storage</span>
</span></span><span class="line"><span class="cl"><span class="nb">id</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># True</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># a改变，b也随之改变，因为他们共享storage</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[   0.,  100.,    2.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [   3.,    4.,    5.]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># c由a切片得到</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span> 
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 0.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 100.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 2.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 3.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 4.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 5.0</span>
</span></span><span class="line"><span class="cl"><span class="c1"># [torch.FloatStorage of size 6]</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span> <span class="n">a</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="c1"># data_ptr返回tensor首元素的内存地址</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (93894489135160, 93894489135152)</span>
</span></span><span class="line"><span class="cl"><span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span> <span class="c1"># c[0]的内存地址对应a[2]的内存地址</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([   0.,  100., -100.,    3.,    4.,    5.])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 使用storage来初始化Tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">d</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="n">d</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">6666</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 6666.,   100.,  -100.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [    3.,     4.,     5.]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 下面４个tensor共享storage</span>
</span></span><span class="line"><span class="cl"><span class="nb">id</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># True</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span> <span class="n">c</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span> <span class="n">d</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (0, 2, 0)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 切片得到e</span>
</span></span><span class="line"><span class="cl"><span class="n">e</span> <span class="o">=</span> <span class="n">b</span><span class="p">[::</span><span class="mi">2</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="c1"># 隔2行/列取一个元素</span>
</span></span><span class="line"><span class="cl"><span class="nb">id</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">storage</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># True</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span> <span class="n">e</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ((3, 1), (6, 2))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># e的数据变得不连续</span>
</span></span><span class="line"><span class="cl"><span class="n">e</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># False</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>可见绝大多数操作并不修改<code>tensor</code>的数据，而只是修改了<code>tensor</code>的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。
此外有些操作会导致<code>tensor</code>不连续，这时需调用<code>tensor.contiguous</code>方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享<code>storage</code>。</p>
</blockquote>
<h5 id="其他tensor使用技巧" class="headerLink">
    <a href="#%e5%85%b6%e4%bb%96tensor%e4%bd%bf%e7%94%a8%e6%8a%80%e5%b7%a7" class="header-mark"></a>其他<code>Tensor</code>使用技巧</h5><h6 id="gpucpu" class="headerLink">
    <a href="#gpucpu" class="header-mark"></a>GPU/CPU</h6><blockquote>
<p><code>tensor</code>可以很随意的在<code>gpu/cpu</code>上传输。使用<code>tensor.cuda(device_id)</code>或者<code>tensor.cpu()</code>。另外一个更通用的方法是<code>tensor.to(device)</code>。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 生成数据</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="o">.</span><span class="n">device</span>
</span></span><span class="line"><span class="cl"><span class="c1"># device(type=&#39;cpu&#39;)</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;gpu&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">a</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p><strong>注意</strong></p>
<ul>
<li>尽量使用<code>tensor.to(device)</code>, 将<code>device</code>设为一个可配置的参数，这样可以很轻松的使程序同时兼容GPU和CPU</li>
<li>数据在GPU之中传输的速度要远快于内存(CPU)到显存(GPU), 所以尽量避免频繁的在内存和显存中传输数据。</li>
</ul>
</blockquote>
<h6 id="持久化" class="headerLink">
    <a href="#%e6%8c%81%e4%b9%85%e5%8c%96" class="header-mark"></a>持久化</h6><blockquote>
<p><code>Tensor</code>的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的<code>pickle</code>模块，在load时还可将GPU tensor映射到CPU或其它GPU上。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">if</span> <span class="n">t</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># 把a转为GPU1上的tensor,</span>
</span></span><span class="line"><span class="cl">    <span class="n">t</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="s1">&#39;a.pth&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;a.pth&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加载为c, 存储于CPU</span>
</span></span><span class="line"><span class="cl">    <span class="n">c</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;a.pth&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="k">lambda</span> <span class="n">storage</span><span class="p">,</span> <span class="n">loc</span><span class="p">:</span> <span class="n">storage</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 加载为d, 存储于GPU0上</span>
</span></span><span class="line"><span class="cl">    <span class="n">d</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;a.pth&#39;</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">})</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h4 id="autograd自动微分" class="headerLink">
    <a href="#autograd%e8%87%aa%e5%8a%a8%e5%be%ae%e5%88%86" class="header-mark"></a><code>autograd</code>：自动微分</h4><blockquote>
<p>深度学习的算法本质上是通过反向传播求导数，而PyTorch的**<code>autograd</code><strong>模块则实现了此功能。在Tensor上的所有操作，</strong><code>autograd</code>**都能为它们自动提供微分，避免了手动计算导数的复杂过程。</p>
<p><del><code>autograd.Variable</code>是Autograd中的核心类，它简单封装了Tensor，并支持几乎所有Tensor有的操作。Tensor在被封装为Variable之后，可以调用它的<code>.backward</code>实现反向传播，自动计算所有梯度</del> <del>Variable的数据结构如下图所示。</del></p>
<p><figure><a class="lightgallery" href="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/autograd_Variable.svg" title="图2-6:Variable的数据结构" data-thumbnail="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/autograd_Variable.svg">
        <img
            
            loading="lazy"
            src="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/autograd_Variable.svg"
            srcset="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/autograd_Variable.svg, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/autograd_Variable.svg 1.5x, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/autograd_Variable.svg 2x"
            sizes="auto"
            alt="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/autograd_Variable.svg">
    </a></figure></p>
<p><em>从0.4起, Variable 正式合并入Tensor, Variable 本来实现的自动微分功能，Tensor就能支持。读者还是可以使用Variable(tensor), 但是这个操作其实什么都没做。所以以后可以直接使用tensor，而不是Variable</em>.</p>
<p>要想使得Tensor使用autograd功能，只需要设置<code>tensor.requries_grad=True</code>.</p>
<p><del>Variable主要包含三个属性。</del>
<del>- <code>data</code>：保存Variable所包含的Tensor</del>
<del>- <code>grad</code>：保存<code>data</code>对应的梯度，<code>grad</code>也是个Variable，而不是Tensor，它和<code>data</code>的形状一样。</del>
<del>- <code>grad_fn</code>：指向一个<code>Function</code>对象，这个<code>Function</code>用来反向传播计算输入的梯度。</del></p>
</blockquote>
<h5 id="variable" class="headerLink">
    <a href="#variable" class="header-mark"></a><code>Variable</code></h5><blockquote>
<p><code>autograd</code>中的核心数据结构是<code>Variable</code>。从v0.4版本起，<code>Variable</code>和<code>Tensor</code>合并。我们可以认为需要求导(requires_grad)的tensor即Variable。autograd记录对tensor的操作记录用来构建计算图。</p>
<p>Variable提供了大部分tensor支持的函数，但其不支持部分<code>inplace</code>函数，因这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算反向传播梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的<code>backward</code>方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。</p>
<p><code>Tensor.backward(gradient=None, retain_graph=None, create_graph=None)</code>主要有如下参数：</p>
<ul>
<li>gradient：形状与variable一致，对于<code>y.backward()</code>，grad_variables相当于链式法则$${dz \over dx}={dz \over dy} \times {dy \over dx}$$中的$$\textbf {dz} \over \textbf {dy}$$。grad_variables也可以是tensor或序列。</li>
<li>retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。</li>
<li>create_graph：对反向传播过程再次构建计算图，可通过<code>backward of backward</code>实现求高阶导数。</li>
</ul>
</blockquote>
<blockquote>
<p>计算下面这个函数的导函数：
$$
y = x^2\bullet e^x
$$
它的导函数是：
$$
{dy \over dx} = 2x\bullet e^x + x^2 \bullet e^x
$$
来看看autograd的计算结果与手动求导计算结果的误差。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;计算y&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">*</span> <span class="n">t</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">y</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gradf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39;手动求导函数&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">dx</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span><span class="o">*</span><span class="n">t</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="o">*</span><span class="n">t</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">dx</span>
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># 生成数据，由于对x求导，所以设定requires_grad = True</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 0.0928,  0.1978,  0.6754,  0.8037],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.9882,  0.3546,  0.2380,  0.0002],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 0.2863,  0.0448,  0.1516,  2.9122]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 此处要注意，对于backward需要传入gradient的尺寸</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="c1"># gradient形状与y一致</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ -0.4611,  62.0520,  35.2313,   4.8159],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [  1.2937,   2.5127,  -0.2839,  -0.4043],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ -0.3389,   1.9795,   1.6028,   2.0199]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 可以看得出来，手动求导的结果和自动求导的结果一致</span>
</span></span><span class="line"><span class="cl"><span class="n">gradf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ -0.4611,  62.0520,  35.2313,   4.8159],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [  1.2937,   2.5127,  -0.2839,  -0.4043],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ -0.3389,   1.9795,   1.6028,   2.0199]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h5 id="autograd常用方法" class="headerLink">
    <a href="#autograd%e5%b8%b8%e7%94%a8%e6%96%b9%e6%b3%95" class="header-mark"></a><code>autograd</code>常用方法</h5><h6 id="torchautogradbackwardtensors-grad_tensorsnone-retain_graphnone-create_graphfalse-grad_variablesnone" class="headerLink">
    <a href="#torchautogradbackwardtensors-grad_tensorsnone-retain_graphnone-create_graphfalse-grad_variablesnone" class="header-mark"></a><code>torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)</code></h6><blockquote>
<p>计算给定变量计算图的梯度的总和。 该计算图使用链规则进行区分。如果任何Tensor非标量（即它们的数据具有多个元素）并且需要梯度，则该函数另外需要指定grad_tensors。它应该是一个匹配长度的序列，其包含差分函数对应变量的梯度（None对于不需要梯度张量的所有变量，它是可接受的值）。</p>
<p>此函数在树叶中累加梯度 - 在调用它之前可能需要将其清零。</p>
<p>参数:</p>
<ul>
<li>Tensors(Tensor列表) – 将计算导数的变量 。</li>
<li>grad_tensors(序列(<code>Tensor</code>或者 <code>None</code>)) – 相应张量的每个元素。 对于标量张量或不需要渐变的张量，不能指定任何值。 如果所有grad_tensors都可以接受None值，则此参数是可选的。</li>
<li>retain_graph（bool，可选） - 如果为False，则用于计算grad的图形将被释放。请注意，在几乎所有情况下，将此选项设置为True不是必需的，通常可以以更有效的方式解决。默认值为create_graph。</li>
<li>create_graph（bool，可选） - 如果为true，则构造导数的图形，允许计算更高阶的衍生产品。默认为False，除非grad_variables包含至少一个非易失性变量。</li>
</ul>
</blockquote>
<h6 id="torchautogradgradoutputs-inputs-grad_outputsnone-retain_graphnone-create_graphfalse-only_inputstrue-allow_unusedfalse" class="headerLink">
    <a href="#torchautogradgradoutputs-inputs-grad_outputsnone-retain_graphnone-create_graphfalse-only_inputstrue-allow_unusedfalse" class="header-mark"></a><code>torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)&gt;</code></h6><blockquote>
<p>计算并返回输入的输出梯度的总和。</p>
<p>grad<em>outputs应该是output 包含每个输出的预先计算的梯度的长度匹配序列。如果输出不需要</em> grad，则梯度可以是None）。当不需要派生图的图形时，梯度可以作为Tensors给出，或者作为Variables，在这种情况下将创建图形。</p>
<p>如果only_inputs为True，该函数将仅返回指定输入的渐变列表。如果它是False，则仍然计算所有剩余叶子的渐变度，并将累积到其.grad 属性中。</p>
<p>参数：</p>
<ul>
<li>outputs（可变序列） - 差分函数的输出。</li>
<li>inputs（可变序列） - 输入将返回梯度的积分（并不积累.grad）。</li>
<li>grad_outputs（Tensor 或Variable的序列） - 渐变wrd每个输出。任何张量将被自动转换为volatile，除非create_graph为True。可以为标量变量或不需要grad的值指定无值。如果所有grad_variables都可以接受None值，则该参数是可选的。</li>
<li>retain_graph（bool，可选） - 如果为False，则用于计算grad的图形将被释放。请注意，在几乎所有情况下，将此选项设置为True不是必需的，通常可以以更有效的方式解决。默认值为create_graph。</li>
<li>create_graph（bool，可选） - 如果为True，则构造导数的图形，允许计算高阶衍生产品。默认为False，除非grad_variables包含至少一个非易失性变量。</li>
<li>only_inputs（bool，可选） - 如果为True，则渐变wrt离开是图形的一部分，但不显示inputs不会被计算和累积。默认为True。</li>
</ul>
</blockquote>
<p><strong>例子</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 为tensor设置 requires_grad 标识，代表着需要求导数</span>
</span></span><span class="line"><span class="cl"><span class="c1"># pytorch 会自动调用autograd 记录操作</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 上一步等价于</span>
</span></span><span class="line"><span class="cl"><span class="c1"># x = t.ones(2,2)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># x.requires_grad = True</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.,  1.]])</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(4.)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span>
</span></span><span class="line"><span class="cl"><span class="c1"># &lt;SumBackward0 at 0x7ffaa589a780&gt;</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 反向传播,计算梯度</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y = x.sum() = (x[0][0] + x[0][1] + x[1][0] + x[1][1])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 每个值的梯度都为1</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.,  1.]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 反向传播的时候，梯度注意需要清零</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 未清零，此时的梯度为</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 2.,  2.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 2.,  2.]])</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 3.,  3.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 3.,  3.]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 清零之后的反向传播</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 以下划线结束的函数是inplace操作，会修改自身的值，就像add_</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 1.,  1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.,  1.]])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>注意：<code>grad</code>在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以反向传播之前需把梯度清零。</p>
</blockquote>
<h5 id="计算图" class="headerLink">
    <a href="#%e8%ae%a1%e7%ae%97%e5%9b%be" class="header-mark"></a>计算图</h5><blockquote>
<p><code>PyTorch</code>中<code>autograd</code>的底层采用了计算图，计算图是一种特殊的有向无环图（<code>DAG</code>），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$$ \textbf {z = wx + b}$$可分解为$$\textbf{y = wx}$$和$$\textbf{z = y + b}$$，其计算图如下图所示，图中<code>MUL</code>，<code>ADD</code>都是算子，$$\textbf{w}$$，$$\textbf{x}$$，$$\textbf{b}$$即变量。</p>
<p><figure><a class="lightgallery" href="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph.svg" title="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph.svg" data-thumbnail="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph.svg">
        <img
            
            loading="lazy"
            src="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph.svg"
            srcset="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph.svg, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph.svg 1.5x, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph.svg 2x"
            sizes="auto"
            alt="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph.svg">
    </a></figure></p>
</blockquote>
<blockquote>
<p>如上有向无环图中，$$\textbf{X}$$和$$\textbf{b}$$是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。$$\textbf{z}$$称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。</p>
</blockquote>
<p>$$
\begin{align}
&amp;{\partial z \over \partial b} = 1,\space {\partial z \over \partial y} = 1 \
&amp;{\partial y \over \partial w }= x,{\partial y \over \partial x}= w \
&amp;{\partial z \over \partial x}= {\partial z \over \partial y} {\partial y \over \partial x}=1 * w\
&amp;{\partial z \over \partial w}= {\partial z \over \partial y} {\partial y \over \partial w}=1 * x \
\end{align}
$$</p>
<blockquote>
<p>而有了计算图，上述链式求导即可利用计算图的反向传播自动完成。其计算过程如下图所示：</p>
<p><figure><a class="lightgallery" href="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph_backward.svg" title="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph_backward.svg" data-thumbnail="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph_backward.svg">
        <img
            
            loading="lazy"
            src="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph_backward.svg"
            srcset="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph_backward.svg, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph_backward.svg 1.5x, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph_backward.svg 2x"
            sizes="auto"
            alt="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/com_graph_backward.svg">
    </a></figure></p>
</blockquote>
<blockquote>
<p>在PyTorch实现中，autograd会随着用户的操作，记录生成当前variable的所有操作，并由此建立一个有向无环图。用户每进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作<code>Function</code>，每一个变量在图中的位置可通过其<code>grad_fn</code>属性在图中的位置推测得到。在反向传播过程中，autograd沿着这个图从当前变量（根节点$$\textbf{z}$$）溯源，可以利用链式求导法则计算所有叶子节点的梯度。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个<code>variable</code>的梯度，这些函数的函数名通常以<code>Backward</code>结尾。</p>
</blockquote>
<p><strong>例子</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 测试requires_grad</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="c1"># 等价于y=w.mul(x)</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">b</span> <span class="c1"># 等价于z=y.add(b)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">requires_grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (False, True, True)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 在默认情况下，requires_grad会自动传递</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 虽然未指定y.requires_grad为True，但由于y依赖于需要求导的w</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 故而y.requires_grad为True</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># True</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 计算图的叶子节点</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">is_leaf</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (True, True, True)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">is_leaf</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (False, False)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># grad_fn可以查看这个variable的反向传播函数，</span>
</span></span><span class="line"><span class="cl"><span class="c1"># z是add函数的输出，所以它的反向传播函数是AddBackward</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="o">.</span><span class="n">grad_fn</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># &lt;AddBackward1 at 0x7f60b09c2630&gt;</span>
</span></span><span class="line"><span class="cl"><span class="c1"># next_functions保存grad_fn的输入，是一个tuple，tuple的元素也是Function</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第一个是y，它是乘法(mul)的输出，所以对应的反向传播函数y.grad_fn是MulBackward</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第二个是b，它是叶子节点，由用户创建</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># ((&lt;MulBackward1 at 0x7f60b09c2278&gt;, 0),</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (&lt;AccumulateGrad at 0x7f60b09c2198&gt;, 0))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># variable的grad_fn对应着和图中的function相对应</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 第一个是w，叶子节点，需要求导，梯度是累加的</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第二个是x，叶子节点，不需要求导，所以为None</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">next_functions</span>
</span></span><span class="line"><span class="cl"><span class="c1"># ((&lt;AccumulateGrad at 0x7f60b09c2898&gt;, 0), (None, 0))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 叶子节点的grad_fn是None</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">,</span><span class="n">x</span><span class="o">.</span><span class="n">grad_fn</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (None, None)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>变量的<code>requires_grad</code>属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点<code>requires_grad</code>都是True。这其实很好理解，对于$$ \textbf{x}\to \textbf{y} \to \textbf{z}$$，x.requires_grad = True，当需要计算$$\partial z \over \partial x$$时，根据链式法则，$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$，自然也需要求$$ \frac{\partial z}{\partial y}$$，所以y.requires_grad会被自动标为True.</p>
<p>有些时候我们可能不希望autograd对tensor求导。认为求导需要缓存许多中间结构，增加额外的内存/显存开销，那么我们可以关闭自动求导。对于不需要反向传播的情景（如inference，即测试推理时），关闭自动求导可实现一定程度的速度提升，并节省约一半显存，因其不需要分配空间计算梯度。</p>
</blockquote>
<p><strong>例子</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 两种抑制requires_grad传递的方法</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y依赖于w，而w.requires_grad = True</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (True, True, True)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 1.第一种抑制的方法</span>
</span></span><span class="line"><span class="cl"><span class="k">with</span> <span class="n">t</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y依赖于w和x，虽然w.requires_grad = True，但是y的requires_grad依旧为False</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (False, True, False)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 2.第二种抑制的方法</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y依赖于w和x，虽然w.requires_grad = True，但是y的requires_grad依旧为False</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (False, True, False)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 恢复默认配置</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">set_grad_enabled</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有两种方法：</p>
<ul>
<li>使用autograd.grad函数</li>
<li>使用hook</li>
</ul>
<p><code>autograd.grad</code>和<code>hook</code>方法都是很强大的工具，更详细的用法参考官方api文档，这里举例说明基础的使用。推荐使用<code>hook</code>方法，但是在实际使用中应尽量避免修改grad的值。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span> 
</span></span><span class="line"><span class="cl"><span class="c1"># 中间节点，梯度自动清零</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y依赖于w，而w.requires_grad = True</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">requires_grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (True, True, True)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 非叶子节点grad计算完之后自动清空，y.grad是None</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (tensor([ 0.2709,  0.0473,  0.5052]), tensor([ 1.,  1.,  1.]), None)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 第一种方法：使用grad获取中间变量的梯度</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># z对y的梯度，隐式调用backward()</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (tensor([ 1.,  1.,  1.]),)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 第二种方法：使用hook</span>
</span></span><span class="line"><span class="cl"><span class="c1"># hook是一个函数，输入是梯度，不应该有返回值</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">variable_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;y的梯度：&#39;</span><span class="p">,</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 注册hook</span>
</span></span><span class="line"><span class="cl"><span class="n">hook_handle</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">variable_hook</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 除非你每次都要用hook，否则用完之后记得移除hook</span>
</span></span><span class="line"><span class="cl"><span class="n">hook_handle</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># y的梯度： tensor([ 1.,  1.,  1.])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>关于variable中grad属性和backward函数<code>grad_variables</code>参数的含义：</p>
<ul>
<li>variable $$\textbf{x}$$的梯度是目标函数$${f(x)} $$对$$\textbf{x}$$的梯度，$$\frac{df(x)}{dx} = (\frac {df(x)}{dx_0},\frac {df(x)}{dx_1},&hellip;,\frac {df(x)}{dx_N})$$，形状和$$\textbf{x}$$一致。</li>
<li>对于y.backward(grad_variables)中的grad_variables相当于链式求导法则中的$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y} \frac{\partial y}{\partial x}$$中的$$\frac{\partial z}{\partial y}$$。z是目标函数，一般是一个标量，故而$$\frac{\partial z}{\partial y}$$的形状与variable $$\textbf{y}$$的形状一致。<code>z.backward()</code>在一定程度上等价于y.backward(grad_y)。<code>z.backward()</code>省略了grad_variables参数，是因为$$z$$是一个标量，而$$\frac{\partial z}{\partial z} = 1$$</li>
</ul>
</blockquote>
<p><strong>例子</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 默认参数下的反向传播，默认情况下，z必须是标量</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="o">*</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># 从z开始反向传播</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 2.,  4.,  6.])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 指定Variable的反向传播，如果z不是标量，不指定Variable，会出错</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="o">*</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">y_gradient</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># dz/dy</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">y_gradient</span><span class="p">)</span> <span class="c1">#从y开始反向传播</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 2.,  4.,  6.])</span>
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>在<code>PyTorch</code>中计算图的特点可总结如下：</p>
<ul>
<li><code>autograd</code>根据用户对<code>variable</code>的操作构建其计算图。对变量的操作抽象为<code>Function</code>。</li>
<li>对于那些不是任何函数(<code>Function</code>)的输出，由用户创建的节点称为叶子节点，叶子节点的<code>grad_fn</code>为None。叶子节点中需要求导的<code>variable</code>，具有<code>AccumulateGrad</code>标识，因其梯度是累加的。</li>
<li><code>variable</code>默认是不需要求导的，即<code>requires_grad</code>属性默认为False，如果某一个节点<code>requires_grad</code>被设置为True，那么所有依赖它的节点<code>requires_grad</code>都为True。</li>
<li><del><code>variable</code>的<code>volatile</code>属性默认为False，如果某一个<code>variable</code>的<code>volatile</code>属性被设为True，那么所有依赖它的节点<code>volatile</code>属性都为True。<code>volatile</code>属性为True的节点不会求导，<code>volatile</code>的优先级比<code>requires_grad</code>高。</del></li>
<li>多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定<code>retain_graph=True</code>来保存这些缓存。</li>
<li>非叶子节点的梯度计算完之后即被清空，可以使用<code>autograd.grad</code>或<code>hook</code>技术获取非叶子节点的值。</li>
<li><code>variable</code>的<code>grad与data</code>形状一致，应避免直接修改<code>variable.data</code>，因为对<code>data</code>的直接操作无法利用autograd进行反向传播</li>
<li>反向传播函数<code>backward</code>的参数<code>grad_variables</code>可以看成链式求导的中间结果，如果是标量，可以省略，默认为1</li>
<li><code>PyTorch</code>采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。</li>
</ul>
</blockquote>
<h4 id="autograd高级用法" class="headerLink">
    <a href="#autograd%e9%ab%98%e7%ba%a7%e7%94%a8%e6%b3%95" class="header-mark"></a><code>autograd</code>高级用法</h4><blockquote>
<p><code>Pytorch</code>提供的大部分函数能自动实现反向传播，但如果需要自己写一个复杂的函数，不支持自动反向求导的时候，我们就需要手动实现反向传播函数。</p>
<p><code>Pytorch</code>提供了两种方法来扩展<code>autograd</code>:</p>
<p>第一种自定义<code>Function</code>：</p>
<blockquote>
<ul>
<li>自定义的Function需要继承<code>autograd.Function</code>，没有构造函数<code>__init__</code>，<code>forward</code>和<code>backward</code>函数都是静态方法</li>
<li><code>backward</code>函数的输出和<code>forward</code>函数的输入一一对应，<code>backward</code>函数的输入和forward函数的输出一一对应</li>
<li><code>backward</code>函数的<code>grad_output</code>参数即<code>t.autograd.backward</code>中的<code>grad_variables</code></li>
<li>如果某一个输入不需要求导，直接返回None，如<code>forward</code>中的输入参数<code>x_requires_grad</code>显然无法对它求导，直接返回None即可</li>
<li>反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放</li>
</ul>
<p><code>Function</code>的使用利用<code>Function.apply(variable)</code>。</p>
</blockquote>
<p>第二种方法：</p>
<blockquote>
<p><code>PyTorch</code>提供了一个装饰器<code>@once_differentiable</code>，能够在<code>backward</code>函数中自动将输入的<code>variable</code>提取成tensor，把计算结果的tensor自动封装成variable。有了这个特性我们就能够很方便的使用<code>numpy/scipy</code>中的函数，操作不再局限于variable所支持的操作。但是这种做法正如名字中所暗示的那样只能求导一次，它打断了反向传播图，不再支持高阶求导。</p>
</blockquote>
</blockquote>
<p><strong>例子</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="kn">import</span> <span class="n">Function</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">MultiplyAdd</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                                                            
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>                              
</span></span><span class="line"><span class="cl">        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">output</span> <span class="o">=</span> <span class="n">w</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">output</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">    <span class="nd">@staticmethod</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>                         
</span></span><span class="line"><span class="cl">        <span class="n">w</span><span class="p">,</span><span class="n">x</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
</span></span><span class="line"><span class="cl">        <span class="n">grad_w</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">x</span>
</span></span><span class="line"><span class="cl">        <span class="n">grad_x</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="n">w</span>
</span></span><span class="line"><span class="cl">        <span class="n">grad_b</span> <span class="o">=</span> <span class="n">grad_output</span> <span class="o">*</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="n">grad_w</span><span class="p">,</span> <span class="n">grad_x</span><span class="p">,</span> <span class="n">grad_b</span>             
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl">      
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 开始前向传播</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="o">=</span><span class="n">MultiplyAdd</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 开始反向传播</span>
</span></span><span class="line"><span class="cl"><span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># x不需要求导，中间过程还是会计算它的导数，但随后被清空</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
</span></span><span class="line"><span class="cl"><span class="c1"># (None, tensor([ 1.]), tensor([ 1.]))</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="pytorch实例线性回归" class="headerLink">
    <a href="#pytorch%e5%ae%9e%e4%be%8b%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92" class="header-mark"></a><code>Pytorch</code>实例线性回归</h3><blockquote>
<p>三种方法实现线性回归，第一种：自动计算导数，第二种：使用<code>autograd</code>来计算导数，第三种：使用优化器自动优化</p>
</blockquote>
<h4 id="线性回归0" class="headerLink">
    <a href="#%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%920" class="header-mark"></a>线性回归0</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">style</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
</span></span><span class="line"><span class="cl"><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&#34;ggplot&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span> <span class="c1">#如果你想用gpu，改成t.device(&#39;cuda:0&#39;)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 设置随机数种子，保证在不同电脑上运行时下面的输出一致</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; 产生随机数据：y=x*2+3，加上了一些噪声&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span>  <span class="n">t</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> 
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># 随机初始化参数</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">lr</span> <span class="o">=</span><span class="mf">0.02</span> <span class="c1"># 学习率</span>
</span></span><span class="line"><span class="cl"><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">500</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 训练500次</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">  	<span class="c1"># 获取数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># forward：计算loss</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="c1"># x@W等价于x.mm(w);for python3 only</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="c1"># 均方误差</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># backward：手动计算梯度</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 模拟计算图的方式计算误差</span>
</span></span><span class="line"><span class="cl">    <span class="n">dloss</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">dy_pred</span> <span class="o">=</span> <span class="n">dloss</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="n">dw</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">dy_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">db</span> <span class="o">=</span> <span class="n">dy_pred</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 更新参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">w</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">dw</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">db</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">ii</span><span class="o">%</span><span class="mi">50</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 画图</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 使用display实现动态图</span>
</span></span><span class="line"><span class="cl">        <span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;green&#39;</span><span class="p">)</span> <span class="c1"># predicted</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y2</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="c1"># true data</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">13</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;w: &#39;</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="s1">&#39;b: &#39;</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><figure><a class="lightgallery" href="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui.gif" title="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui.gif" data-thumbnail="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui.gif">
        <img
            
            loading="lazy"
            src="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui.gif"
            srcset="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui.gif, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui.gif 1.5x, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui.gif 2x"
            sizes="auto"
            alt="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui.gif">
    </a></figure></p>
<h4 id="线性回归1" class="headerLink">
    <a href="#%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%921" class="header-mark"></a>线性回归1</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">style</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
</span></span><span class="line"><span class="cl"><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&#34;ggplot&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 注意，此处如果使用cuda，那么计算梯度的时候，会出错</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 设置随机数种子，保证在不同电脑上运行时下面的输出一致</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; 产生随机数据：y=x*2+3，加上了一些噪声&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span>  <span class="n">t</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> 
</span></span><span class="line"><span class="cl">  
</span></span><span class="line"><span class="cl"><span class="c1"># 随机初始化参数</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">lr</span> <span class="o">=</span><span class="mf">0.02</span> <span class="c1"># 学习率</span>
</span></span><span class="line"><span class="cl"><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">500</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 训练500次</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># forward：计算loss</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">losses</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># backward：手动计算梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 更新参数</span>
</span></span><span class="line"><span class="cl">    <span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">lr</span> <span class="o">*</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># 梯度清零</span>
</span></span><span class="line"><span class="cl">    <span class="n">w</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">b</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">ii</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span><span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 画图</span>
</span></span><span class="line"><span class="cl">        <span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">)</span> <span class="c1"># predicted</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span><span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y2</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="c1"># true data</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">)</span>   
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 1.9642277956008911 3.0166714191436768</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><figure><a class="lightgallery" href="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_1.gif" title="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_1.gif" data-thumbnail="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_1.gif">
        <img
            
            loading="lazy"
            src="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_1.gif"
            srcset="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_1.gif, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_1.gif 1.5x, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_1.gif 2x"
            sizes="auto"
            alt="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_1.gif">
    </a></figure></p>
<h4 id="线性回归2" class="headerLink">
    <a href="#%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%922" class="header-mark"></a>线性回归2</h4><div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span> <span class="k">as</span> <span class="nn">t</span>
</span></span><span class="line"><span class="cl"><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span> 
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">style</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">IPython</span> <span class="kn">import</span> <span class="n">display</span>
</span></span><span class="line"><span class="cl"><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&#34;ggplot&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 不知道为啥使用cuda梯度为不存在了</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 设置随机数种子，保证在不同电脑上运行时下面的输出一致</span>
</span></span><span class="line"><span class="cl"><span class="n">t</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; 产生随机数据：y=x*2+3，加上了一些噪声&#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">+</span>  <span class="n">t</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> 
</span></span><span class="line"><span class="cl"> 
</span></span><span class="line"><span class="cl"><span class="c1"># 随机初始化参数</span>
</span></span><span class="line"><span class="cl"><span class="n">w</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0005</span> <span class="c1"># 学习率不能太大</span>
</span></span><span class="line"><span class="cl"><span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">500</span>
</span></span><span class="line"><span class="cl"><span class="n">losses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 导入优化器</span>
</span></span><span class="line"><span class="cl"><span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">],</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 引入mse损失函数</span>
</span></span><span class="line"><span class="cl"><span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">ii</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="c1"># forward：计算loss</span>
</span></span><span class="line"><span class="cl">    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">losses</span><span class="p">[</span><span class="n">ii</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 优化器梯度清零</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># backward：手动计算梯度</span>
</span></span><span class="line"><span class="cl">    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 逐步优化</span>
</span></span><span class="line"><span class="cl">    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">ii</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 画图</span>
</span></span><span class="line"><span class="cl">        <span class="n">display</span><span class="o">.</span><span class="n">clear_output</span><span class="p">(</span><span class="n">wait</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span><span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">)</span> <span class="c1"># predicted</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span><span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">get_fake_data</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span> 
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y2</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span> <span class="c1"># true data</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">)</span>   
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">pause</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><figure><a class="lightgallery" href="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_0.gif" title="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_0.gif" data-thumbnail="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_0.gif">
        <img
            
            loading="lazy"
            src="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_0.gif"
            srcset="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_0.gif, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_0.gif 1.5x, https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_0.gif 2x"
            sizes="auto"
            alt="https://blog-1253453438.cos.ap-beijing.myqcloud.com/pytorch/huigui_0.gif">
    </a></figure></p>
<h3 id="参考" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83" class="header-mark"></a>参考</h3><blockquote>
<p><a href="https://github.com/chenyuntc/pytorch-book/tree/master/chapter4-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7%E7%AE%B1nn" target="_blank" rel="noopener noreferrer">深度学习之Pytorch(陈云)</a></p>
</blockquote>
</div>

        <div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2019-01-05</span>
            </div>
            <div class="post-info-license"></div>
        </div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="#" title="分享到 微博" data-sharer="weibo" data-url="https://gsscsd.github.io/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/" data-title="Pytorch快速入门0"><i class="fab fa-weibo fa-fw"></i></a><a href="#" title="分享到 百度" data-sharer="baidu" data-url="https://gsscsd.github.io/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/" data-title="Pytorch快速入门0"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@v5.21.1/icons/baidu.svg"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw"></i>&nbsp;<a href="/tags/python/">python</a>,&nbsp;<a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>,&nbsp;<a href="/tags/pytorch/">Pytorch</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/tensorflow%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B%E4%B9%8Bmnist/" class="prev" rel="prev" title="tensorflow综合实例之MNIST"><i class="fas fa-angle-left fa-fw"></i>tensorflow综合实例之MNIST</a>
            <a href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/" class="next" rel="next" title="机器学习案例之Titanic生存预测分析">机器学习案例之Titanic生存预测分析<i class="fas fa-angle-right fa-fw"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
        </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">
                    由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreferrer" title="Hugo 0.100.1">Hugo</a> 强力驱动&nbsp;|&nbsp;主题 - <a href="https://github.com/HEIGE-PCloud/DoIt" target="_blank" rel="noopener noreferrer" title="DoIt 0.3.0"><i class="far fa-edit fa-fw"></i> DoIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2019 - 2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank" rel="noopener noreferrer">Gsscsd</a></span></div>
            <div class="footer-line"></div>
            <div class="footer-line">
            </div>
        </div></footer></div>

    <div id="fixed-buttons"><a href="#back-to-top" id="back-to-top-button" class="fixed-button" title="回到顶部">
            <i class="fas fa-arrow-up fa-fw"></i>
        </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
            <i class="fas fa-comment fa-fw"></i>
        </a>
    </div><div class="assets"><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><link rel="preload" as="style" onload="this.onload=null;this.rel='stylesheet'" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.css">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.css"></noscript><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"QGzwQXOqs5JOhN4RGPOkR2mR-MdYXbMMI","appKey":"WBmoGyJtbqUswvfLh6L8iEBr","avatar":"mp","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@5.0.1/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":false,"highlight":true,"lang":"zh-cn","pageSize":10,"placeholder":"你的评论 ...","recordIP":true,"serverURLs":"https://leancloud.hugoloveit.com","visitor":true}},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"distance":null,"findAllMatches":null,"fuseIndexURL":"/index.json","highlightTag":"em","ignoreFieldNorm":null,"ignoreLocation":null,"isCaseSensitive":null,"location":null,"maxResultLength":10,"minMatchCharLength":null,"noResultsFound":"没有找到结果","snippetLength":50,"threshold":null,"type":"fuse","useExtendedSearch":null},"sharerjs":true};</script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.4.16/dist/Valine.min.js" defer></script><script type="text/javascript" src="/js/valine.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.8/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.4.2/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/copy-tex.min.js" defer></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script type="text/javascript" src="/js/katex.min.js" defer></script><script type="text/javascript" src="/js/theme.min.js" defer></script></div>
</body>

</html>