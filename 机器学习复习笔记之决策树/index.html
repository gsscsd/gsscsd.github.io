<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>机器学习复习笔记之决策树 - Gsscsd</title><meta name="Description" content="时光划过指缝-阅读挽留时光"><meta property="og:title" content="机器学习复习笔记之决策树" />
<meta property="og:description" content="决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://gsscsd.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/" /><meta property="og:image" content="https://cdn.jsdelivr.net/gh/gsscsd/BlogImg/20220628173721.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-03-16T21:12:36+00:00" />
<meta property="article:modified_time" content="2019-03-16T21:12:36+00:00" /><meta property="og:site_name" content="Gsscsd" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/gsscsd/BlogImg/20220628173721.png"/>

<meta name="twitter:title" content="机器学习复习笔记之决策树"/>
<meta name="twitter:description" content="决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。"/>
<meta name="application-name" content="Gsscsd">
<meta name="apple-mobile-web-app-title" content="Gsscsd"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://gsscsd.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/" /><link rel="prev" href="https://gsscsd.github.io/%E4%BB%8E%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E7%9C%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" /><link rel="next" href="https://gsscsd.github.io/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/" /><link rel="stylesheet" href="/css/style.min.931bc4ad2d28eb74379d23c35d88889e10d86e4fb73a8e095952c2617800dcce223d542ddf6f22eb6db537ea777ccee425cbdcb03ad216de7941dc3a1574cdfc.css" integrity="sha512-kxvErS0o63Q3nSPDXYiInhDYbk+3Oo4JWVLCYXgA3M4iPVQt328i6221N+p3fM7kJcvcsDrSFt55Qdw6FXTN/A=="><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "机器学习复习笔记之决策树",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/gsscsd.github.io\/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91\/"
        },"image": ["https:\/\/gsscsd.github.io\/images\/Apple-Devices-Preview.png"],"genre": "posts","keywords": "机器学习","wordcount":  7856 ,
        "url": "https:\/\/gsscsd.github.io\/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91\/","datePublished": "2019-03-16T21:12:36+00:00","dateModified": "2019-03-16T21:12:36+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "Gsscsd","logo": "https:\/\/cdn.jsdelivr.net\/gh\/gsscsd\/BlogImg\/G_128px.ico"},"author": {
                "@type": "Person",
                "name": "Gsscsd"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Gsscsd">Gsscsd</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/gsscsd" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Gsscsd">Gsscsd</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/gsscsd" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">机器学习复习笔记之决策树</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Gsscsd</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>机器学习</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2019-03-16">2019-03-16</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 7856 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 16 分钟&nbsp;<span id="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/" class="leancloud_visitors" data-flag-title="机器学习复习笔记之决策树">
                        <i class="far fa-eye fa-fw" aria-hidden="true"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
                    </span>&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li>
          <ul>
            <li><a href="#回归与分类">回归与分类</a></li>
            <li><a href="#回归树与分类树">回归树与分类树</a>
              <ul>
                <li><a href="#信息熵">信息熵</a></li>
                <li><a href="#id3算法">ID3算法</a></li>
                <li><a href="#c45算法">C4.5算法</a></li>
                <li><a href="#cart算法">CART算法</a></li>
              </ul>
            </li>
            <li><a href="#决策树剪枝">决策树剪枝</a></li>
            <li><a href="#决策树算法小结">决策树算法小结</a></li>
            <li><a href="#决策树的优缺点">决策树的优缺点</a></li>
            <li><a href="#参考链接">参考链接</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><p>决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。</p>
<h4 id="回归与分类">回归与分类</h4>
<p>在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。</p>
<p>这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。<img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2019/03/16/5c8cad2947b0a.png"
        data-srcset="https://i.loli.net/2019/03/16/5c8cad2947b0a.png, https://i.loli.net/2019/03/16/5c8cad2947b0a.png 1.5x, https://i.loli.net/2019/03/16/5c8cad2947b0a.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2019/03/16/5c8cad2947b0a.png"
        title="https://i.loli.net/2019/03/16/5c8cad2947b0a.png" /></p>
<p>如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。</p>
<h4 id="回归树与分类树">回归树与分类树</h4>
<p>在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用<strong>最大均方误差</strong>来划分节点，并且每个节点样本的<strong>均值</strong>作为测试样本的<strong>回归预测值</strong>；而分类树是采用<strong>信息增益</strong>或者是<strong>信息增益比</strong>来划分节点，每个节点样本的类别情况投  票决定测试样本的类别。我们可以看到，这两者的区别主要在于<strong>划分方式</strong>与<strong>工作模式</strong>。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行<strong>预测</strong>；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的<strong>分类</strong>。</p>
<h5 id="信息熵">信息熵</h5>
<p>1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。</p>
<p>信息论中的熵表示的是不确定性的度量。<strong>越不确定的事物，它的熵就越</strong>大。</p>
<p>具体的，随机变量X的<strong>熵</strong>的表达式如下：</p>
<p>$$H(X) = -\sum_{i=1}^{n}p_i log(p_i)  \tag{1}$$</p>
<p>其中<code>n</code>代表<code>X</code>的<code>n</code>种不同的离散取值。而$p_i$代表了<code>X</code>取值为<code>i</code>的概率，<code>log</code>为以<code>2</code>或者<code>e</code>为底的对数。</p>
<p>联合熵描述的是一对随机变量X和Y的不确定性：</p>
<p>$$H(X,Y) = -\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \tag{2}$$</p>
<p>条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性:</p>
<p>$$H(X|Y) = -\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \tag{3}$$</p>
<p>H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为<code>I(X,Y)</code>。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2019/03/16/5c8cae5151d93.png"
        data-srcset="https://i.loli.net/2019/03/16/5c8cae5151d93.png, https://i.loli.net/2019/03/16/5c8cae5151d93.png 1.5x, https://i.loli.net/2019/03/16/5c8cae5151d93.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2019/03/16/5c8cae5151d93.png"
        title="https://i.loli.net/2019/03/16/5c8cae5151d93.png" /></p>
<p>用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益<code>I(X,Y)</code>, 左边的椭圆去掉重合部分就是<code>H(X|Y)</code>,右边的椭圆去掉重合部分就是<code>H(Y|X)</code>,两个椭圆的并就是<code>H(X,Y)</code>。</p>
<h5 id="id3算法">ID3算法</h5>
<blockquote>
<p><code>ID3</code>算法的核心实在决策树上的各个节点上用 <strong>信息增益</strong> 选择特征。在<code>ID3</code>算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。</p>
</blockquote>
<p>下面是《统计学习方法》的例子</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2019/03/16/5c8cafd93f53d.png"
        data-srcset="https://i.loli.net/2019/03/16/5c8cafd93f53d.png, https://i.loli.net/2019/03/16/5c8cafd93f53d.png 1.5x, https://i.loli.net/2019/03/16/5c8cafd93f53d.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2019/03/16/5c8cafd93f53d.png"
        title="https://i.loli.net/2019/03/16/5c8cafd93f53d.png" /></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2019/03/16/5c8cb04d8ba96.png"
        data-srcset="https://i.loli.net/2019/03/16/5c8cb04d8ba96.png, https://i.loli.net/2019/03/16/5c8cb04d8ba96.png 1.5x, https://i.loli.net/2019/03/16/5c8cb04d8ba96.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2019/03/16/5c8cb04d8ba96.png"
        title="https://i.loli.net/2019/03/16/5c8cb04d8ba96.png" /></p>
<p><code>ID3</code>算法步骤：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="err">输入：训练数据集</span><span class="n">D</span><span class="p">,</span><span class="err">特征集</span><span class="n">A</span><span class="err">，阈值</span><span class="n">e</span><span class="err">；</span>   <span class="err">输出</span><span class="o">:</span><span class="err">决策树</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">1</span><span class="p">)</span><span class="err">若</span><span class="n">D中所有实例属于同一类Ck</span><span class="err">，则</span><span class="n">T为单结点树</span><span class="err">，并将</span><span class="n">Ck作为该结点的类标记</span><span class="err">，返回</span><span class="n">T</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">2</span><span class="p">)</span><span class="err">若</span><span class="n">A</span><span class="o">=</span><span class="err">空集，则</span><span class="n">T为单结点树</span><span class="err">，并将</span><span class="n">D中的实例数最大的类Ck作为该结点的类标记</span><span class="err">，返回</span><span class="n">T</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">3</span><span class="p">)</span><span class="err">否则，计算</span><span class="n">A中各特征对D的信息增益比</span><span class="err">，选择信息增益最大的特征</span><span class="n">Ag</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">4</span><span class="p">)</span><span class="err">如果</span><span class="n">Ag的信息增益小于阈值e</span><span class="err">，则置</span><span class="n">T为单结点树</span><span class="err">，并将</span><span class="n">D中实例数最大的类Ck作为该结点的类标记</span><span class="err">，返回</span><span class="n">T</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">5</span><span class="p">)</span><span class="err">否则，对</span><span class="n">Ag的每一个可能值ai</span><span class="err">；依</span><span class="n">Ag</span><span class="o">=</span><span class="n">ai将D分割为若干非空子集Di</span><span class="err">；将</span><span class="n">Di中实例数最大的类作为标记</span><span class="err">，构建子结点，由结点及其子结点构成树</span><span class="n">T</span><span class="err">，返回</span><span class="n">T</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">6</span><span class="p">)</span><span class="err">对第</span><span class="n">i个子结点</span><span class="err">，以</span><span class="n">Di为训练集</span><span class="err">，以</span><span class="n">A</span><span class="o">-</span><span class="p">{</span><span class="n">Ag</span><span class="p">}</span><span class="err">为特征集，递归调用（</span><span class="mi">1</span><span class="err">）</span><span class="o">~</span><span class="err">（</span><span class="mi">5</span><span class="err">），得到子树</span><span class="n">Ti</span><span class="err">，返回</span><span class="n">Ti</span><span class="err">。</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>ID3</code>的缺点：　　</p>
<blockquote>
<ul>
<li>ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。</li>
<li>ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。</li>
<li>ID3算法对于缺失值的情况没有做考虑</li>
<li>没有考虑过拟合的问题</li>
</ul>
</blockquote>
<p>ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。</p>
<h5 id="c45算法">C4.5算法</h5>
<blockquote>
<p>ID3算法有四个主要的不足，一是不能处理<strong>连续特征</strong>，第二个就是用信息增益作为标准容易偏向<strong>于取值较多的特征</strong>，最后两个是<strong>缺失值处理</strong>的问和<strong>过拟合</strong>问题。C4.5算法中改进了上述4个问题。</p>
</blockquote>
<p><strong>问题一：连续特征处理：</strong></p>
<blockquote>
<p>C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为<code>a1,a2,...,am</code>,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第<code>i</code>个划分点<code>Ti</code>表示为：$Ti = \frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p>
</blockquote>
<p><strong>问题二：，信息增益作为标准容易偏向于取值较多的特征的问题：</strong></p>
<blockquote>
<p>C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下：</p>
<p>$$I_R(D,A)=\frac{I(A,D)}{H_A(D)} \tag{4}$$</p>
<p>其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下:</p>
<p>$$H_A(D)=-\sum_{i=1}^n \frac{D_i}{D}log_2\frac{D_i}{D}\tag{5}$$</p>
<p>其中n为特征A的类别数， Di为特征A的第<code>i</code>个取值对应的样本个数。D为样本个数。</p>
<p>特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。</p>
<p>通过对比信息增益公式与信息增益比公式，我们可以看出，<strong>信息增益就是特征与训练集的互信息</strong>，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而<strong>信息增益比则是这一个互信息与D的不确定性的比值</strong>。</p>
</blockquote>
<p><strong>问题三：缺失值处理:</strong></p>
<blockquote>
<p>对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。</p>
<p>对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。</p>
<p>对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值<code>A1,A2,A3</code>。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入<code>A1，A2，A3</code>。对应权重调节为<code>2/9,3/9,4/9</code>。</p>
</blockquote>
<p>问题四：过拟合问题：</p>
<blockquote>
<p>C4.5引入了正则化系数进行初步的剪枝。</p>
</blockquote>
<p><strong><code>C4.5</code>算法步骤：</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="err">输入：训练数据集</span><span class="n">D</span><span class="p">,</span><span class="err">特征集</span><span class="n">A</span><span class="err">，阈值</span><span class="n">e</span><span class="err">；</span>   <span class="err">输出</span><span class="o">:</span><span class="err">决策树</span><span class="n">T</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">1</span><span class="p">)</span><span class="err">若</span><span class="n">D中所有实例属于同一类Ck</span><span class="err">，则</span><span class="n">T为单结点树</span><span class="err">，并将</span><span class="n">Ck作为该结点的类标记</span><span class="err">，返回</span><span class="n">T</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">2</span><span class="p">)</span><span class="err">若</span><span class="n">A</span><span class="o">=</span><span class="err">空集，则</span><span class="n">T为单结点树</span><span class="err">，并将</span><span class="n">D中的实例数最大的类Ck作为该结点的类标记</span><span class="err">，返回</span><span class="n">T</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">3</span><span class="p">)</span><span class="err">否则，计算</span><span class="n">A中各特征对D的信息增益比</span><span class="err">，选择信息增益比最大的特征</span><span class="n">Ag</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">4</span><span class="p">)</span><span class="err">如果</span><span class="n">Ag的信息增益小于阈值e</span><span class="err">，则置</span><span class="n">T为单结点树</span><span class="err">，并将</span><span class="n">D中实例数最大的类Ck作为该结点的类标记</span><span class="err">，返回</span><span class="n">T</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">5</span><span class="p">)</span><span class="err">否则，对</span><span class="n">Ag的每一个可能值ai</span><span class="err">；依</span><span class="n">Ag</span><span class="o">=</span><span class="n">ai将D分割为若干非空子集Di</span><span class="err">；将</span><span class="n">Di中实例数最大的类作为标记</span><span class="err">，构建子结点，由结点及其子结点构成树</span><span class="n">T</span><span class="err">，返回</span><span class="n">T</span><span class="err">；</span>
</span></span><span class="line"><span class="cl"><span class="mi">6</span><span class="p">)</span><span class="err">对第</span><span class="n">i个子结点</span><span class="err">，以</span><span class="n">Di为训练集</span><span class="err">，以</span><span class="n">A</span><span class="o">-</span><span class="p">{</span><span class="n">Ag</span><span class="p">}</span><span class="err">为特征集，递归调用（</span><span class="mi">1</span><span class="err">）</span><span class="o">~</span><span class="err">（</span><span class="mi">5</span><span class="err">），得到子树</span><span class="n">Ti</span><span class="err">，返回</span><span class="n">Ti</span><span class="err">。</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong><code>C4.5</code>的缺点：</strong></p>
<blockquote>
<ul>
<li>由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。</li>
<li>C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。</li>
<li>C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。</li>
<li>C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了。</li>
</ul>
</blockquote>
<h5 id="cart算法">CART算法</h5>
<blockquote>
<p>在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会涉及大量的对数运算。</p>
<p>为了减少运算，CART分类树算法使用<strong>基尼系数</strong>来代替信息增益比，<strong>基尼系数</strong>代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。</p>
</blockquote>
<p>在分类问题中，假设有K个类别，第k个类别的概率为$p_k$, 则基尼系数的表达式为：</p>
<p>$$Gini(p) = \sum\limits_{k=1}^{K}p_k(1-p_k) = 1- \sum\limits_{k=1}^{K}p_k^2 \tag{6}$$</p>
<p>二类分类问题，计算就更加简单了，如果属于第一个样本输出的概率是p，则基尼系数的表达式为：</p>
<p>$$Gini(p) = 2p(1-p)  \tag{7}$$</p>
<p>对于个给定的样本D,假设有K个类别, 第k个类别的数量为$C_k$,则样本D的基尼系数表达式为：</p>
<p>$$Gini(D) = 1-\sum\limits_{k=1}^{K}(\frac{|C_k|}{|D|})^2  \tag{8}$$</p>
<p>对于样本D,如果根据特征A的某个值a,把D分成D1和D2两部分，则在特征A的条件下，D的基尼系数表达式为：</p>
<p>$$Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)  \tag{9}$$</p>
<p>对比基尼系数表达式和熵模型的表达式，可以发现，相对来说，基尼系数的运算更简单一些，但是其误差却增加了。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2019/03/16/5c8ce46b21589.png"
        data-srcset="https://i.loli.net/2019/03/16/5c8ce46b21589.png, https://i.loli.net/2019/03/16/5c8ce46b21589.png 1.5x, https://i.loli.net/2019/03/16/5c8ce46b21589.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2019/03/16/5c8ce46b21589.png"
        title="https://i.loli.net/2019/03/16/5c8ce46b21589.png" /></p>
<p>从上图可以看出，基尼系数和熵之半的曲线非常接近，仅仅在45度角附近误差稍大。因此，基尼系数可以做为熵模型的一个近似替代。而CART分类树算法就是使用的基尼系数来选择决策树的特征。同时，为了进一步简化，CART分类树算法每次仅仅对某个特征的值进行二分，而不是多分，这样<strong>CART分类树算法建立起来的是二叉树</strong>，而不是多叉树。这样一可以进一步简化基尼系数的计算，二可以建立一个更加优雅的二叉树模型。</p>
<p>下面是<code>CART</code>的例子</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://i.loli.net/2019/03/22/5c948366cc126.png"
        data-srcset="https://i.loli.net/2019/03/22/5c948366cc126.png, https://i.loli.net/2019/03/22/5c948366cc126.png 1.5x, https://i.loli.net/2019/03/22/5c948366cc126.png 2x"
        data-sizes="auto"
        alt="https://i.loli.net/2019/03/22/5c948366cc126.png"
        title="https://i.loli.net/2019/03/22/5c948366cc126.png" /></p>
<p><strong>CART分类树对连续和离散特征的处理：</strong></p>
<blockquote>
<p>CART分类树连续值的处理问题，其思想和C4.5是相同的，都是将连续的特征离散化。唯一的区别在于在选择划分点时的度量方式不同，<strong>C4.5使用的是信息增益比，则CART分类树使用的是基尼系数。</strong></p>
<p>连续特征的处理是：假设m个样本的连续特征A有m个，从小到大排列为<code>a1,a2,...,am</code>,则CART取相邻两样本值的平均数，一共取得m-1个划分点，其中第<code>i</code>个划分点<code>Ti</code>表示为：$Ti = \frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的基尼系数最小的点为$a_t$，则小于$a_t$值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与ID3或者C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。</p>
<p>离散特征的处理是：不停的二分离散特征。在ID3和C4.5算法中，如果某个特征A被选取建立决策树节点，如果它有A1,A2,A3三种类别，我们会在决策树上一下建立一个三叉的节点。这样导致决策树是多叉树。CART分类树使用的方法不同，他采用的是不停的二分，还是这个例子，CART分类树会考虑把<code>A分成{A1}和{A2,A3}{A1}和{A2,A3}, {A2}和{A1,A3}{A2}和{A1,A3}, {A3}和{A1,A2}{A3}和{A1,A2}</code>三种情况，找到基尼系数最小的组合，比如<code>{A2}和{A1,A3}{A2}和{A1,A3}</code>，然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的节点。同时，由于这次没有把特征A的取值完全分开，后面我们还有机会在子节点继续选择到特征A来划分A1和A3。这和ID3或者C4.5不同，在ID3或者C4.5的一棵子树中，离散特征只会<strong>参与一次节点</strong>的建立。</p>
</blockquote>
<p><strong>CART分类树的算法步骤</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-c" data-lang="c"><span class="line"><span class="cl"><span class="err">输入是训练集</span><span class="n">D</span><span class="err">，基尼系数的阈值，样本个数阈值。输出是决策树</span><span class="n">T</span><span class="err">。</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="mi">1</span><span class="p">)</span> <span class="err">对于当前节点的数据集为</span><span class="n">D</span><span class="err">，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。</span>
</span></span><span class="line"><span class="cl"><span class="mi">2</span><span class="p">)</span> <span class="err">计算样本集</span><span class="n">D的基尼系数</span><span class="err">，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。</span>
</span></span><span class="line"><span class="cl"><span class="mi">3</span><span class="p">)</span> <span class="err">计算当前节点现有的各个特征的各个特征值对数据集</span><span class="n">D的基尼系数</span><span class="err">，对于离散值和连续值的处理方法和基尼系数的计算见第二节。缺失值的处理方法和之前的</span><span class="n">C4</span><span class="mf">.5</span><span class="err">算法里描述的相同。</span>
</span></span><span class="line"><span class="cl"><span class="mi">4</span><span class="p">)</span> <span class="err">在计算出来的各个特征的各个特征值对数据集</span><span class="n">D的基尼系数中</span><span class="err">，选择基尼系数最小的特征</span><span class="n">A和对应的特征值a</span><span class="err">。根据这个最优特征和最优特征值，把数据集划分成两部分</span><span class="n">D1和D2</span><span class="err">，同时建立当前节点的左右节点，做节点的数据集</span><span class="n">D为D1</span><span class="err">，右节点的数据集</span><span class="n">D为D2</span><span class="p">.</span>
</span></span><span class="line"><span class="cl"><span class="mi">5</span><span class="p">)</span> <span class="err">对左右的子节点递归的调用</span><span class="mi">1</span><span class="o">-</span><span class="mi">4</span><span class="err">步，生成决策树。</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>对于生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别。</p>
<p><strong>CART回归树的算法步骤</strong></p>
<blockquote>
<p>CART回归树和CART分类树的建立和预测的区别主要有下面两点：</p>
<p>1)连续值的处理方法不同</p>
<p>2)决策树建立后做预测的方式不同。</p>
<p>对于连续值的处理，CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。这比较适合分类模型，但是对于<strong>回归模型</strong>，我们使用了常见的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小所对应的特征和特征值划分点。表达式为：</p>
<p>$$\underbrace{min}<em>{A,s}\Bigg[\underbrace{min}</em>{c_1}\sum\limits_{x_i \in D_1(A,s)}(y_i - c_1)^2 + \underbrace{min}<em>{c_2}\sum\limits</em>{x_i \in D_2(A,s)}(y_i - c_2)^2\Bigg]\tag{10}$$</p>
<p>其中，$c_1$为D1数据集的样本输出均值，$c_2$为D2数据集的样本输出均值。</p>
</blockquote>
<p>对于决策树建立后做预测的方式，回归树的输出是用最终叶子<strong>的均值或者中位数</strong>来预测输出结果。</p>
<h4 id="决策树剪枝">决策树剪枝</h4>
<blockquote>
<p>由于决策时算法很容易对训练集过拟合，而导致泛化能力差，为了解决这个问题，我们需要对CART树进行剪枝，即类似于线性回归的正则化，来增加决策树的泛化能力。CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略。</p>
<p>也就是说，CART树的剪枝算法可以概括为两步，第一步是从原始决策树生成各种剪枝效果的决策树，第二部是用交叉验证来检验剪枝后的预测能力，选择泛化预测能力最好的剪枝后的数作为最终的CART树。</p>
<p>剪枝的损失函数度量，在剪枝的过程中，对于任意的一刻子树T,其损失函数为：</p>
<p>$$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t| \tag{11}$$</p>
<p>其中，α为正则化参数，这和线性回归的正则化一样。$C(T_t)$为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。$|T_t|$是子树T的叶子节点的数量。</p>
<p>当α=0时，即没有正则化，原始的生成的CART树即为最优子树。当α=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的αα，一定存在使损失函数$C_α(T)$最小的唯一子树。</p>
<p>对于位于节点t的任意一颗子树Tt，如果没有剪枝，它的损失是：$C_{\alpha}(T_t) = C(T_t) + \alpha |T_t|$，如果将其剪掉，仅仅保留根节点，则损失是：$C_{\alpha}(T) = C(T) + \alpha$,当α=0或者α很小时，$C_α(T_t)&lt;C_α(T) $, 当α增大到一定的程度时，$C_{\alpha}(T_t) = C_{\alpha}(T)$。当α继续增大时不等式反向，也就是说，如果满足下式：$\alpha = \frac{C(T)-C(T_t)}{|T_t|-1}$，Tt和T有相同的损失函数，但是T节点更少，因此可以对子树Tt进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点T。</p>
<p>CART树的交叉验证策略：如果我们把所有的节点是否剪枝的值α都计算出来，然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。</p>
</blockquote>
<p><strong>剪枝算法步骤</strong></p>
<blockquote>
<p>输入是CART树建立算法得到的原始决策树T。输出是最优决策子树Tα。</p>
<p>算法过程如下：</p>
<ul>
<li>初始化$α_{min}=∞$， 最优子树集合ω={T}。</li>
<li>从叶子节点开始自下而上计算各内部节点t的训练误差损失函数$C_α(T_t)$（回归树为均方差，分类树为基尼系数）, 叶子节点数|Tt|，以及正则化阈值, 更新$α_{min}=α$</li>
<li>得到所有节点的α值的集合M。</li>
<li>从M中选择最大的值$α_k$，自上而下的访问子树t的内部节点，如果$C(T)−C(T_t)|T_t|−1≤α_k$时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到αk对应的最优子树$T_k$</li>
<li>最优子树集合$ω=ω∪T_k$，$ M=M−{α_k}$。</li>
<li>如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合ω.</li>
<li>采用交叉验证在ω选择最优子树Tα</li>
</ul>
</blockquote>
<h4 id="决策树算法小结">决策树算法小结</h4>
<table>
<thead>
<tr>
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr>
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr>
<td>CART</td>
<td>分类，回归</td>
<td>二叉树</td>
<td>基尼系数，均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
<h4 id="决策树的优缺点">决策树的优缺点</h4>
<p><strong>优点：</strong></p>
<blockquote>
<ol>
<li>简单直观，生成的决策树很直观。</li>
<li>基本不需要预处理，不需要提前归一化，处理缺失值。</li>
<li>使用决策树预测的代价是$O(log_2m)$。 m为样本数。</li>
<li>既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</li>
<li>可以处理多维度输出的分类问题。</li>
<li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li>
<li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li>
<li>对于异常点的容错能力好，健壮性高。</li>
</ol>
</blockquote>
<p><strong>缺点：</strong></p>
<blockquote>
<ol>
<li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li>
<li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li>
<li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li>
<li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li>
<li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li>
</ol>
</blockquote>
<h4 id="参考链接">参考链接</h4>
<blockquote>
<p><a href="https://www.cnblogs.com/pinard/p/6050306.html" target="_blank" rel="noopener noreffer ">决策树算法原理(上)</a></p>
<p><a href="https://www.cnblogs.com/pinard/p/6053344.html" target="_blank" rel="noopener noreffer ">决策树算法原理(下)</a></p>
</blockquote></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2019-03-16</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://gsscsd.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/" data-title="机器学习复习笔记之决策树"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 百度" data-sharer="baidu" data-url="https://gsscsd.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/" data-title="机器学习复习笔记之决策树"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.0.0/icons/baidu.svg" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E4%BB%8E%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E7%9C%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="prev" rel="prev" title="从一个例子看机器学习"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>从一个例子看机器学习</a>
            <a href="/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/" class="next" rel="next" title="剑指Offer之二叉树的深度">剑指Offer之二叉树的深度<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="valine" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://valine.js.org/">Valine</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.101.0">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2022</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Gsscsd</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/valine/valine.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.stemmer.support.min.6867e554c019e9277423b0f08fa2f10633c0b4a2e736319d9fe99f73a35a205705d41b0fa3615656587f72e0f073de501a6fc69f66f2aa479482864f959af053.js" integrity="sha512-aGflVMAZ6Sd0I7Dwj6LxBjPAtKLnNjGdn+mfc6NaIFcF1BsPo2FWVlh/cuDwc95QGm/Gn2byqkeUgoZPlZrwUw=="></script><script type="text/javascript" src="/lib/lunr/lunr.zh.min.918bdd059e2c518e24c32fb5fd89144a49778f21f2166db93bf1e2ed311b3589660feb5777210257aee209f6d8bdde8c296883e34ff8bf4d5338f4be53132976.js" integrity="sha512-kYvdBZ4sUY4kwy+1/YkUSkl3jyHyFm25O/Hi7TEbNYlmD+tXdyECV67iCfbYvd6MKWiD40/4v01TOPS+UxMpdg=="></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"valine":{"appId":"QGzwQXOqs5JOhN4RGPOkR2mR-MdYXbMMI","appKey":"WBmoGyJtbqUswvfLh6L8iEBr","avatar":"mp","el":"#valine","emojiCDN":"https://cdn.jsdelivr.net/npm/emoji-datasource-google@14.0.0/img/google/64/","emojiMaps":{"100":"1f4af.png","alien":"1f47d.png","anger":"1f4a2.png","angry":"1f620.png","anguished":"1f627.png","astonished":"1f632.png","black_heart":"1f5a4.png","blue_heart":"1f499.png","blush":"1f60a.png","bomb":"1f4a3.png","boom":"1f4a5.png","broken_heart":"1f494.png","brown_heart":"1f90e.png","clown_face":"1f921.png","cold_face":"1f976.png","cold_sweat":"1f630.png","confounded":"1f616.png","confused":"1f615.png","cry":"1f622.png","crying_cat_face":"1f63f.png","cupid":"1f498.png","dash":"1f4a8.png","disappointed":"1f61e.png","disappointed_relieved":"1f625.png","dizzy":"1f4ab.png","dizzy_face":"1f635.png","drooling_face":"1f924.png","exploding_head":"1f92f.png","expressionless":"1f611.png","face_vomiting":"1f92e.png","face_with_cowboy_hat":"1f920.png","face_with_hand_over_mouth":"1f92d.png","face_with_head_bandage":"1f915.png","face_with_monocle":"1f9d0.png","face_with_raised_eyebrow":"1f928.png","face_with_rolling_eyes":"1f644.png","face_with_symbols_on_mouth":"1f92c.png","face_with_thermometer":"1f912.png","fearful":"1f628.png","flushed":"1f633.png","frowning":"1f626.png","ghost":"1f47b.png","gift_heart":"1f49d.png","green_heart":"1f49a.png","grimacing":"1f62c.png","grin":"1f601.png","grinning":"1f600.png","hankey":"1f4a9.png","hear_no_evil":"1f649.png","heart":"2764-fe0f.png","heart_decoration":"1f49f.png","heart_eyes":"1f60d.png","heart_eyes_cat":"1f63b.png","heartbeat":"1f493.png","heartpulse":"1f497.png","heavy_heart_exclamation_mark_ornament":"2763-fe0f.png","hole":"1f573-fe0f.png","hot_face":"1f975.png","hugging_face":"1f917.png","hushed":"1f62f.png","imp":"1f47f.png","innocent":"1f607.png","japanese_goblin":"1f47a.png","japanese_ogre":"1f479.png","joy":"1f602.png","joy_cat":"1f639.png","kiss":"1f48b.png","kissing":"1f617.png","kissing_cat":"1f63d.png","kissing_closed_eyes":"1f61a.png","kissing_heart":"1f618.png","kissing_smiling_eyes":"1f619.png","laughing":"1f606.png","left_speech_bubble":"1f5e8-fe0f.png","love_letter":"1f48c.png","lying_face":"1f925.png","mask":"1f637.png","money_mouth_face":"1f911.png","nauseated_face":"1f922.png","nerd_face":"1f913.png","neutral_face":"1f610.png","no_mouth":"1f636.png","open_mouth":"1f62e.png","orange_heart":"1f9e1.png","partying_face":"1f973.png","pensive":"1f614.png","persevere":"1f623.png","pleading_face":"1f97a.png","pouting_cat":"1f63e.png","purple_heart":"1f49c.png","rage":"1f621.png","relaxed":"263a-fe0f.png","relieved":"1f60c.png","revolving_hearts":"1f49e.png","right_anger_bubble":"1f5ef-fe0f.png","robot_face":"1f916.png","rolling_on_the_floor_laughing":"1f923.png","scream":"1f631.png","scream_cat":"1f640.png","see_no_evil":"1f648.png","shushing_face":"1f92b.png","skull":"1f480.png","skull_and_crossbones":"2620-fe0f.png","sleeping":"1f634.png","sleepy":"1f62a.png","slightly_frowning_face":"1f641.png","slightly_smiling_face":"1f642.png","smile":"1f604.png","smile_cat":"1f638.png","smiley":"1f603.png","smiley_cat":"1f63a.png","smiling_face_with_3_hearts":"1f970.png","smiling_imp":"1f608.png","smirk":"1f60f.png","smirk_cat":"1f63c.png","sneezing_face":"1f927.png","sob":"1f62d.png","space_invader":"1f47e.png","sparkling_heart":"1f496.png","speak_no_evil":"1f64a.png","speech_balloon":"1f4ac.png","star-struck":"1f929.png","stuck_out_tongue":"1f61b.png","stuck_out_tongue_closed_eyes":"1f61d.png","stuck_out_tongue_winking_eye":"1f61c.png","sunglasses":"1f60e.png","sweat":"1f613.png","sweat_drops":"1f4a6.png","sweat_smile":"1f605.png","thinking_face":"1f914.png","thought_balloon":"1f4ad.png","tired_face":"1f62b.png","triumph":"1f624.png","two_hearts":"1f495.png","unamused":"1f612.png","upside_down_face":"1f643.png","weary":"1f629.png","white_frowning_face":"2639-fe0f.png","white_heart":"1f90d.png","wink":"1f609.png","woozy_face":"1f974.png","worried":"1f61f.png","yawning_face":"1f971.png","yellow_heart":"1f49b.png","yum":"1f60b.png","zany_face":"1f92a.png","zipper_mouth_face":"1f910.png","zzz":"1f4a4.png"},"enableQQ":false,"highlight":true,"lang":"zh-CN","pageSize":10,"placeholder":"你的评论 ...","recordIP":true,"serverURLs":"https://leancloud.hugoloveit.com","visitor":true}},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","lunrLanguageCode":"zh","lunrSegmentitURL":"/lib/lunr/lunr.segmentit.js","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.8f3907fa55b08d1250417a302a5836b4095aeba0e8de276226fbabed0058c004aec93be43d27a90cb1c7b80dffd331535aae064d507b1c9f140b42edb18d7d90.js" integrity="sha512-jzkH+lWwjRJQQXowKlg2tAla66Do3idiJvur7QBYwASuyTvkPSepDLHHuA3/0zFTWq4GTVB7HJ8UC0LtsY19kA=="></script></body>
</html>
