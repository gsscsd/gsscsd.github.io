[{"categories":null,"content":"\rQQ：1210588659\rEmail: gsscsd@qq.com ","date":"0001-01-01","objectID":"/about/:0:0","series":null,"tags":null,"title":"about","uri":"/about/#"},{"categories":["深度学习"],"content":" 前言：后续记录一下softmax函数与sigmoid函数的区别 参考链接： https://www.jianshu.com/p/36beb5ff76db ","date":"2021-03-14","objectID":"/softmax%E4%B8%8Esigmoid%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB/:1:0","series":null,"tags":["深度学习"],"title":"Softmax与sigmoid的区别与联系","uri":"/softmax%E4%B8%8Esigmoid%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB/#前言"},{"categories":["深度学习"],"content":" 到目前为止，word2vec算法不单单是nlp的基础，也成为推荐和搜索的基础，本文记录一下word2vec算法中的negative sampling方案，并基于此记录了其他的sampling方法。 参考链接： https://zhuanlan.zhihu.com/p/76568362/ https://blog.csdn.net/yimingsilence/article/details/105920987 https://zhuanlan.zhihu.com/p/129824834 https://narcissuscyn.github.io/2018/07/03/CandidateSampling/ https://www.zhihu.com/question/50043438 https://blog.csdn.net/wangpeng138375/article/details/75151064 https://zhuanlan.zhihu.com/p/45368976 https://zhuanlan.zhihu.com/p/45014864 https://zhuanlan.zhihu.com/p/27234078 https://www.cnblogs.com/pinard/p/7249903.html https://www.cnblogs.com/peghoty/p/3857839.html https://www.zhihu.com/question/386144477 https://blog.csdn.net/weixin_40901056/article/details/88568344 https://blog.csdn.net/u010223750/article/details/69948463 ","date":"2021-03-13","objectID":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/:0:0","series":null,"tags":["深度学习"],"title":"从word2vec到negative sampling","uri":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/#"},{"categories":["深度学习"],"content":" Skip-gram方法的word2vec 在word2vec出现之前，已经有用神经网络DNN来用训练词向量进而处理词与词之间的关系了。采用的方法一般是一个三层的神经网络结构（当然也可以多层），分为输入层，隐藏层和输出层(softmax层)。 这个模型是如何定义数据的输入和输出呢？一般分为CBOW(Continuous Bag-of-Words 与Skip-Gram两种模型。 CBOW模型的训练输入是某一个特征词的上下文相关的词对应的词向量，而输出就是这特定的一个词的词向量。 Skip-Gram模型和CBOW的思路是反着来的，即输入是特定的一个词的词向量，而输出是特定词对应的上下文词向量。 PS：skip-gram 出来的准确率比cbow 高，cbow比sg训练快，sg比cbow更好地处理生僻字（出现频率低的字）。 在词向量训练任务中，softmax函数有如下： $$p(w|c) = \\frac{\\exp(h^\\top v_w)}{\\sum_{w_i \\in V} \\exp(h^\\top v_{w_i})}=\\frac{\\exp(h^\\top v_w)}{Z(h)}$$ 其中，$h$是隐藏层的输出， $v_{w_i}$是w对应的输出词向量（即softmax的权重矩阵）,$V$是词典，$c$是上下文。 在神经网络语言模型中，一般会把$C$压缩为$h$。 从上面的公式可以看出，softmax函数的分母是对所有词典进行遍历求和，当$V$的size比较小的时候，softmax的求导以及梯度下降速度较快，但是当$V$的size比较大的时候，softmax的分母需要遍历所有的样本进行求和，因此速度较慢，对于此问题，业界提出了多种方法来解决该问题，常见的方法有Noise Contrastive Estimation(NCE)，negative sampling，sampling softmx算法等，接下来分别讲解一下两种算法。 ","date":"2021-03-13","objectID":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/:1:0","series":null,"tags":["深度学习"],"title":"从word2vec到negative sampling","uri":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/#skip-gram方法的word2vec"},{"categories":["深度学习"],"content":" Noise Contrastive Estimation算法 对于每一个训练样本（x, T)，我们训练binary classification，而不是multiclass classification。具体一点，我们对于每个样本，拆分成一个真实的（x,y) pair,另外我们随机产生k个Noise的（x,y）pair,这样我们就可以用来训练处这样的binary classifier。 用概率来表示，这个问题由之前的P(y|x) 通过x预测所有y，换成了P(x,y)，计算x,y同时存在的概率，换言之，从基于特征x求y的最大后验概率，变成基于特征X和y，共同出现的最大后验概率。 假设共有m个样$(l_i,c_i)$, 建模: \\begin{equation} P\\left(l_{i} \\mid c_{i}\\right)=\\frac{u_{\\theta}\\left(l_{i}, c_{i}\\right)}{\\sum_{i}^{n} u_{\\theta}\\left(l_{j}, c_{i}\\right)}=\\frac{u_{\\theta}\\left(l_{i}, c_{i}\\right)}{Z_{i}} \\end{equation} 假设负例label从某个分布$Q(l_i)$中抽取, 且抽取$k$次. 正例从上面的分布抽取, 则有: $(l_i,c_i)$真实样本的概率： \\begin{equation} P\\left(\\text { True } \\mid l_{i}, c_{i}\\right)=\\frac{P\\left(l_{i} \\mid c_{i}\\right)}{k Q\\left(l_{i}\\right)+P\\left(l_{i} \\mid c_{i}\\right)}=P\\left(T \\mid l_{i}, c_{i}\\right) \\end{equation} $(l_i,c_i)$负样本的概率： \\begin{equation} P\\left(\\text { False } \\mid l_{i}, c_{i}\\right)=\\frac{k Q\\left(c_{i}\\right)}{k Q\\left(l_{i}\\right)+P\\left(l_{i} \\mid c_{i}\\right)}=P\\left(F \\mid l_{i}, c_{i}\\right) \\end{equation} 最终最大化log似然估计, 损失函数: \\begin{equation} J(\\theta) = \\prod_{(w,c) \\in T} P(T|w,c;\\theta) \\prod_{(w,c) \\in Neg} P(F|w,c;\\theta) \\end{equation} \\begin{equation} L=\\sum_{i}^{n}\\left(\\log P\\left(T \\mid l_{i}, c_{i}\\right)+k \\sum_{i=0, L_{x} \\sim Q\\left(l_{i}\\right)}^{k} \\log P\\left(F \\mid L_{x}, c_{i}\\right)\\right) \\end{equation} ","date":"2021-03-13","objectID":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/:2:0","series":null,"tags":["深度学习"],"title":"从word2vec到negative sampling","uri":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/#noise-contrastive-estimation算法"},{"categories":["深度学习"],"content":" negative sampling算法负采样Negative Sampling是NCE的一个变种，概率的定义有所区别。 建模, 作为二分类softmax损失. \\begin{equation} P\\left(T \\mid l_{i}, c_{i}\\right)=\\frac{u_{\\theta}\\left(l_{i}, c_{i}\\right)}{1+u_{\\theta}\\left(l_{i}, c_{i}\\right)}=\\sigma\\left(u_{\\theta}\\left(l_{i}, c_{i}\\right)\\right) \\end{equation} \\begin{equation} P\\left(F \\mid l_{i}, c_{i}\\right)=1-P\\left(T \\mid l_{i}, c_{i}\\right)=\\frac{1}{1+u_{\\theta}\\left(l_{i}, c_{i}\\right)}=1-\\sigma\\left(u_{\\theta}\\left(l_{i}, c_{i}\\right)\\right) \\end{equation} 最终最大化log似然估计略(和NCE相同), 负例的采样时, 为全体样本的所有$l$不消重的均匀采样,或者每个$l$采到的概率为: \\begin{equation} P\\left(l_{x}\\right)=\\frac{\\operatorname{cnt}\\left(l_{x}\\right)^{0.75}}{\\sum_{y \\in L} \\operatorname{cnt}\\left(l_{y}\\right)^{0.75}} \\end{equation} 注意，构造样本时，要注意正负样本的比例，如果考虑所有的负样本，会导致正负比例失衡，模型权重会被负样本带偏。 ","date":"2021-03-13","objectID":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/:3:0","series":null,"tags":["深度学习"],"title":"从word2vec到negative sampling","uri":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/#negative-sampling算法"},{"categories":["深度学习"],"content":" sampling softmax算法(sampled_softmax_loss) Sampled softmax方法不同于nce方法，nce是把多分类问题转化成二分类，而sampled softmax方法则是只抽取一部分样本计算softmax。训练的时候不需要特别精准的softmax归一化概率，只需要一个粗略值做back propoagation就好了。这么粗糙的算法，可能会导致分布不一致问题？？？ 如果损失函数采用交叉熵损失函数: \\begin{equation} H(q,p) = - \\sum_x q(x) \\log p(x) \\end{equation} 这里q是真实期望分布,例如 $q=[0,…1,…,0]$，p是模型输出分布，对应最上面的softmax公式。 对于一个样本，可得交叉熵损失函数(这里把模型的参数统称为$\\theta$): \\begin{equation} J_\\theta = - \\text{log} \\dfrac{\\text{exp}({h^\\top v_{w}})}{\\sum_{w_i \\in V} \\text{exp}({h^\\top v_{w_i}})} \\end{equation} 假设：$\\mathcal{E}(w)=-h^\\top v_{w}$, 则： \\begin{equation} J_\\theta = \\mathcal{E}(w) + \\text{log} \\sum_{w_i \\in V} \\text{exp}( - \\mathcal{E}(w_i)) \\end{equation} 对$\\theta$求梯度得： \\begin{equation} \\nabla_\\theta J_\\theta = \\nabla_\\theta \\mathcal{E}(w) + \\sum_{w_i \\in V} \\dfrac{\\text{exp}(- \\mathcal{E}(w_i))}{\\sum_{w_i \\in V} \\text{exp}(- \\mathcal{E}(w_i))} \\nabla_\\theta (- \\mathcal{E}(w_i)) \\end{equation} 已知：$p(w_i) = \\dfrac{\\text{exp}(- \\mathcal{E}(w_i))}{\\sum_{w_i \\in V} \\text{exp}- \\mathcal{E}(w_i))}$, \\begin{equation} \\nabla_\\theta J_\\theta = \\nabla_\\theta \\mathcal{E}(w) - \\sum_{w_i \\in V} P(w_i) \\nabla_\\theta (\\mathcal{E}(w_i)) \\end{equation} 对于梯度公式的第二部分，可以认为是$\\nabla_\\theta (\\mathcal{E}(w_i))$对于softmax输出$P(w_i)$的期望，即： \\begin{equation} \\sum_{w_i \\in V} P(w_i) \\nabla_\\theta \\mathcal{E}(w_i) = \\mathbb{E}{w_i \\sim P}[\\nabla\\theta \\mathcal{E}(w_i)] \\end{equation} 上面的这个公式就是控制softmax采样需要优化的部分。 根据传统的重要性采样方法，按照如下公式计算期望： \\begin{equation} \\frac{1}{N} \\sum_{w_i \\sim Q(w)}\\frac{P(w_i)}{Q(w_i)}\\nabla_\\theta \\mathcal{E}(w_i) \\approx \\mathbb{E}{w_i \\sim P}[\\nabla\\theta \\mathcal{E}(w_i)] \\end{equation} 其中$N$是从分布$Q$(我们自己定义的一个容易采样的分布)中采样的样本数，但是这种方法仍然需要计算$P(wi)$，而$P(wi)$的计算又需要softmax做归一化，这是我们不想看到的，所以要使用一种有偏估计的方法。 Softmax公式的分母部分： \\begin{equation} Z(h)=\\sum_{w_i \\in V} \\text{exp}(- \\mathcal{E}(w_i))=M\\sum_{w_i \\in V} (\\frac{1}{M})\\cdot \\text{exp}(- \\mathcal{E}(w_i)) \\end{equation} 公式中$\\sum_{w_i \\in V} (\\frac{1}{M})\\cdot \\text{exp}(- \\mathcal{E}(w_i))$是一种期望形式，因而可以通过采样方法进行估计得到$Z(h)$, 对于$Z(h)$的采样候选分布仍旧选择$Q$分布。 则可以得到： \\begin{equation} Z(h)=\\hat{Z}(h)=\\frac{M}{N}\\sum_{w_i \\sim Q(w)}\\frac{\\hat{R}(w_i)\\text{exp}(- \\mathcal{E}(w_i))}{Q(w_i)}=\\frac{M}{N}\\sum_{w_i \\sim Q(w)}\\frac{\\text{exp}(- \\mathcal{E}(w_i))}{M\\cdot Q(w_i)} \\end{equation} 上式中的$\\hat{R}(w_i)$代表概率$\\frac{1}{M}$，约去$M$可得： \\begin{equation} \\hat{Z}(h)=\\frac{1}{N}\\sum_{w_i \\sim Q(w)}\\frac{\\text{exp}(- \\mathcal{E}(w_i))}{ Q(w_i)} \\end{equation} 到这里，我们就可以用$\\hat{Z}(h)$去近似$Z(h)$了。 现在理一下思路：给定候选分布Q，传统采样方法需要计算P，也就是说需要计算分母Z，这是我们不想看到的。幸运的是分母Z仍然可以通过采样得到，采样Z的时候，仍然采用候选分布Q。 \\begin{equation} \\frac{1}{N} \\sum_{w_i \\sim Q(w)}\\frac{P(w_i)}{Q(w_i)} \\nabla \\theta \\mathcal{E}(w_i) \\approx \\mathbb{E}{w_i \\sim P}[\\nabla _\\theta \\mathcal{E}(w_i)] \\end{equation} \\begin{equation} \\frac{1}{N}\\sum_{w_i \\sim Q(w)}\\frac{\\hat{P}(w_i)}{Q(w_i)}\\nabla \\theta \\mathcal{E}(w_i) \\approx \\mathbb{E}{w_i \\sim P}[\\nabla \\theta \\mathcal{E}(w_i)] \\end{equation} 其中 $\\hat{P}(wi)$代表采样方式获得的概率： \\begin{equation} \\hat{P}(w_i)=\\frac{\\text{exp}(- \\mathcal{E}(w_i))}{\\hat{Z}(h)} \\end{equation} 可得： \\begin{equation} \\mathbb{E}{w_i \\sim P}[\\nabla_\\theta \\mathcal{E}(w_i)]\\approx \\frac{1}{N}\\sum_{w_i \\sim Q(w)}\\frac{\\text{exp}(- \\mathcal{E}(w_i))}{Q(w_i)\\hat{Z}(h)}\\nabla_\\theta \\mathcal{E}(w_i) \\end{equation} 现在我们就从$Q$分布中采样$N$个样本，组成集合$J$，最终得到： \\begin{equation} \\mathbb{E}{w_i \\sim P}[\\nabla\\theta \\mathcal{E}(w_i)]\\approx \\frac{\\sum_{w_j \\in J}\\text{exp}(- \\mathcal{E}(w_j))\\nabla_\\theta \\mathcal{E}(w_j)/Q(w_j)}{\\sum_{w_j \\in J}\\text{exp}(- \\mathcal{E}(w_j))/Q(w_j)} \\end{equation} 整体梯度为： \\begin{equation} \\nabla_\\theta J_\\theta = : \\nabla_\\theta \\mathcal{E}(w) - \\frac{\\sum_{w_j \\in J}\\text{exp}(- \\mathcal{E}(w_j))\\nabla_\\theta \\mathcal{E}(w_j)/Q(w_j)}{\\sum_{w_j \\in J}\\text{exp}(- \\mathcal{E}(w_j))/Q(w_j)} \\end{equation} ","date":"2021-03-13","objectID":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/:4:0","series":null,"tags":["深度学习"],"title":"从word2vec到negative sampling","uri":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/#sampling-softmax算法sampled_softmax_loss"},{"categories":["深度学习"],"content":" Tensorflow的采样方法：candidate sampling假如我们有一个多分类任务或者多标签分类任务，给定训练集$(x_i,T_i)$，其中xixi表示上下文，$T_i$表示目标类别(可能有多个).可以用word2vec中的negtive sampling方法来举例，使用cbow方法，也就是使用上下文$x_i$来预测中心词(单个target$T_i$)，或者使用skip-gram方法，也就是使用中心词$x_i$来预测上下文(多个target($T_i$)). 我们想学习到一个通用函数$F(x,y)$来表征上下文$x$和目标类$y$的关系，如Word2vec里面，使用上下文预测下个单词的概率。 完整的训练方法，如使用softmax或者Logistic回归需要对每个训练数据计算所有类$y\\in L$的概率$F(x,y)$，当$|L|$非常大的时候，训练将非常耗时。 “candidate sampling\"训练方法包括为每一个训练数据$(x_i,T_i)$构造一个训练任务，使得我们只需要使用一个较小的候选集合$C_i\\in L$，就能评估$F(x,y)$,典型的，candidate set $C_i$包含目标类别$T_i$和一些随机采样的类别$S_i\\in L$：$C_i = T_i \\cup S_i$ , $S_i$的选择可能依赖 $x_i$和 $T_i$，也可能不依赖。 $F(x,y)$可以使用神经网络计算来表征(也就是TensorFlow里面常用的logits) 其中： $Q(y|x)$表示的是给定context $x_i$采样到$y$的概率 $K(x)$表示任意不以来候选集的函数 $logistic-training-loss = \\sum_{i}(\\sum_{y \\in POS_i} log(1+exp(-G(x_i,y)) )+\\sum_{y \\in NEG_i} log(1+exp(G(x_i,y)) ))$ $softmax-training-loss = \\sum_{i}(-log(\\frac{exp(G(x_i,t_i))}{\\sum_{y \\in POS_i \\cup NEG_i} exp(G(x_i,y))}))$ 在使用tensoflow的时候，我们有时候会纠结选择什么样的损失函数比较好，softmax和logistic在表达形式上是有点区别的，但是也不是很大，而且对于普通的softmax_cross_entropy_with_logits和sigmoid_cross_entropy_with_logits也都能够进行多分类任务，那么他们之间的区别是什么的？ 就我个人所想到的，使用sigmoid_cross_entropy_with_logits和softmax_cross_entropy_with_logits的最大的区别是类别的排他性，在分类任务中，使用softmax_cross_entropy_with_logits我们一般是选择单个标签的分类，因为其具有排他性，说白了，softmax_cross_entropy_with_logits需要的是一个类别概率分布，其分布应该服从多项分布(也就是多项logistic regression)，我们训练是让结果尽量靠近这种概率分布，不是说softmax_cross_entropy_with_logits不能进行多分，事实上softmax_cross_entropy_with_logits是支持多个类别的，其参数labels也没有限制只使用一个类别，当使用softmax_cross_entropy_with_logits进行多分类时候，以二类为例，我们可以设置真实类别的对应labels上的位置是0.5,0.5，训练使得这个文本尽量倾向这种分布，在test阶段，可以选择两个计算概率最大的类作为类别标签，从这种角度说，使用softmax_cross_entropy_with_logits进行多分，实际上类似于计算文本的主题分布。 对于sigmoid_cross_entropy_with_logits，公式可以看出，sigmoid_cross_entropy_with_logits其实是训练出了多个分类器，对于有n个标签的分类问题，其实质是分成了n个二分类问题，这点和softmax_cross_entropy_with_logits有着本质的区别。 tensorflow提供了下面两种candidate sample方法 tf.nn.nce_loss tf.nn.sampled_softmax_loss tf.nn.nce_loss使用的是logistic, 而tf.nn.sampled_softmax_loss采用的是softmax loss，其实这两者的区别也主要在这儿，采用logistic loss的本质上还是训练n个分类器，而使用softmax loss的其实只是训练了一个主题分类器，tf.nn.nce_loss主要思路也是判断给定context $C_i$和训练数据$x_i$，判断每一个$y_i$是不是target label，而 tf.nn.sampled_softmax_loss则是使得在target label上的分布概率最大化。 对于多标签多类别的分类任务使用Logistic比较好，对于多标签单类别的分类任务使用softmax比较好，采样中，采用tf.nn.sampled_softmax_loss训练cbow模型比较好，而 tf.nn.nce_loss训练skip-gram比较好。 ","date":"2021-03-13","objectID":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/:5:0","series":null,"tags":["深度学习"],"title":"从word2vec到negative sampling","uri":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/#tensorflow的采样方法candidate-sampling"},{"categories":["深度学习"],"content":" tensorflow 源码解析： _compute_sampled_logits输入隐藏层输出和真标签，在里面采样获得S集，并计算，返回的就是F（x,y）-logQ，在nce_loss和sampled_softmax_loss中都调用它进行采样，详细的源码解释在下面 def sampled_softmax_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, partition_strategy=\"mod\", name=\"sampled_softmax_loss\", seed=None): logits, labels = _compute_sampled_logits( weights=weights, biases=biases, labels=labels, inputs=inputs, num_sampled=num_sampled, num_classes=num_classes, num_true=num_true, sampled_values=sampled_values, subtract_log_q=True, remove_accidental_hits=remove_accidental_hits, partition_strategy=partition_strategy, name=name, seed=seed) labels = array_ops.stop_gradient(labels, name=\"labels_stop_gradient\") sampled_losses = nn_ops.softmax_cross_entropy_with_logits_v2( labels=labels, logits=logits) # sampled_losses is a [batch_size] tensor. return sampled_losses _compute_sampled_logits的参数和返回 Args: weights: A `Tensor` of shape `[num_classes, dim]`, or a list of `Tensor` objects whose concatenation along dimension 0 has shape [num_classes, dim]. The (possibly-sharded) class embeddings. biases: A `Tensor` of shape `[num_classes]`. The class biases. 这里我用L指代所有的类别的集合，h指代隐藏层向量维度，这两个维度就是[L,h],[L] weights biases就是我们的上下文embedding,你把embedding传进去， 他采样之后就用输出h和这个embedding相乘，只计算那些被采样的样品就可以了。 还有一个就是注意weights的shape,dim是在后面的 他在sampled_softmax_loss调用中也强调了，你训练的时候才用，像下面这样有个选择 if mode == \"train\": loss = tf.nn.sampled_softmax_loss( weights=weights, biases=biases, labels=labels, inputs=inputs, ...) elif mode == \"eval\": logits = tf.matmul(inputs, tf.transpose(weights)) logits = tf.nn.bias_add(logits, biases) labels_one_hot = tf.one_hot(labels, n_classes) loss = tf.nn.softmax_cross_entropy_with_logits( labels=labels_one_hot, logits=logits) eval的时候乘法weights有一个transpose labels: A `Tensor` of type `int64` and shape `[batch_size, num_true]`. The target classes. Note that this format differs from the `labels` argument of `nn.softmax_cross_entropy_with_logits`. 这里labels就是标签，他也提到了与上面eval的时候输入是不一样的，那个需要你进行一个one_hot num_true如果我们用softmax就是1，其他的就是多标签 shape是[m,T],m是batch的大小 inputs: A `Tensor` of shape `[batch_size, dim]`. The forward activations of the input network. 这就是h 输出 [m,h] num_sampled: An `int`. The number of classes to randomly sample per batch. 在这里可以看出，是一个批次，用同样的sampled的类，这个就是集合S的大小 用指代S num_classes: An `int`. The number of possible classes. 就是所有的类别，词表L的大小 num_true: An `int`. The number of target classes per training example. 用T指代 源码里有一个注意： Note: In the case where num_true \u003e 1, we assign to each target class the target probability 1 / num_true so that the target probabilities sum to 1 per-example. 就是说如果有T个真目标类，那每个真类别的采样概率Q就是1/T,有点像上面文章里的1/|V|， 但这里我也有个疑问，如果我们是1个标签，那Q就是1了吗？但论文中也没提到正样本的Q怎么计算 sampled_values: a tuple of (`sampled_candidates`, `true_expected_count`, `sampled_expected_count`) returned by a `*_candidate_sampler` function. (if None, we default to `log_uniform_candidate_sampler`) 这里如果none，函数会自己调用的log_uniform_candidate_sampler。 但如果你要用其他采样，你就得把采样后的结果是一个元组给他，格式我们也下面介绍。 其余采样方法这个我们下文再详细介绍。 subtract_log_q: A `bool`. whether to subtract the log expected count of the labels in the sample to get the logits of the true labels. Default is True. Turn off for Negative Sampling. 是否减去logQ，按上面那个表格NCE和sampled softmax都要减的，负采样不减。 remove_accidental_hits: A `bool`. whether to remove \"accidental hits\" where a sampled class equals one of the target classes. Default is True. 如果采样到真标签了怎么办，是否删掉这次采样，默认是True要删掉 百度paddle里说如果为真，如果一个sample[i，j]意外地碰到了真标签， 那么相应的sampled_logits[i，j]将被减去1e20，使其SoftMax结果接近零。默认值为True。 框架多还是有好处的 https://www.paddlepaddle.org.cn/documentation/docs/zh/api_cn/layers_cn/sampled_softmax_with_cross_entropy_cn.html#sampled-softmax-with-cross-entropy 下面来自上面的博客https://narcissuscyn.github.io/2018/07/03/CandidateSampling/ 其实两个loss的核心代码都是_compute_sampled_logits，但是在实现上不同的地方有两点： sampled_softmax_loss是有去重的，也就是remove_accidental_hits=True, 但是nce_loss是不去重的，我们从上面的表也能看出来。 sampled_softmax_loss采用的是softmax+CE，但是nce_loss采用的是sigmod+CE partition_s","date":"2021-03-13","objectID":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/:6:0","series":null,"tags":["深度学习"],"title":"从word2vec到negative sampling","uri":"/%E4%BB%8Eword2vec%E5%88%B0negative_sampling/#tensorflow-源码解析"},{"categories":["Linux"],"content":" Tmux 的全称是 Terminal MUtipleXer，及终端复用软件。顾名思义，它的主要功能就是在你关闭终端窗口之后保持进程的运行，此外 Tmux 的另一个重大功能就是分屏 ","date":"2021-02-20","objectID":"/%E7%BB%88%E7%AB%AF%E5%B7%A5%E5%85%B7tmux/:0:0","series":null,"tags":["Linux","Tmux"],"title":"终端工具Tmux","uri":"/%E7%BB%88%E7%AB%AF%E5%B7%A5%E5%85%B7tmux/#"},{"categories":["Linux"],"content":" Tmux的基本配置 # ----------------------------------------------------------------------------- # Tmux 基本配置 - 要求 Tmux \u003e= 2.3 # 如果不想使用插件，只需要将此节的内容写入 ~/.tmux.conf 即可flow-qiso # ----------------------------------------------------------------------------- # https://gist.github.com/ryerh/14b7c24dfd623ef8edc7nf # C-b 和 VIM 冲突，修改 Prefix 组合键为 Control-X，按键距离近 set -g prefix C-x set -g base-index 1 # 窗口编号从 1 开始计数 set -g display-panes-time 10000 # PREFIX-Q 显示编号的驻留时长，单位 ms # set -g mouse on # 开启鼠标 set -g pane-base-index 1 # 窗格编号从 1 开始计数 set -g renumber-windows on # 关掉某个窗口后，编号重排 setw -g allow-rename off # 禁止活动进程修改窗口名 setw -g automatic-rename off # 禁止自动命名新窗口 setw -g mode-keys vi # 进入复制模式的时候使用 vi 键位（默认是 EMACS） # ----- Windows ----- # Use vi styled keys for scrolling \u0026 copying set-window-option -g mode-keys vi # ----- Panes ----- # Key bindings for switching panes bind -n M-h select-pane -L # left bind -n M-l select-pane -R # right bind -n M-k select-pane -U # up bind -n M-j select-pane -D # down # Key bindings for creating panes bind-key 1 split-window -h # horizontal bind-key 2 split-window -v # verticle # Contents on the right of the status bar set -g status-right \"#[fg=magenta,bold] #{prefix_highlight}#[fg=red,bold]CPU: #{cpu_percentage} #[fg=blue]Battery: #{battery_percentage} #[fg=green]%a %Y:%m:%d %H:%M:%S \" set -g status-interval 1 # refresh every second set -g status-right-length 100 # maximum length for the right content of the status bar # Contents on the left of the status bar set -g status-left \"#[fg=yellow,bold] ❐ #S \" # show the current session set -g status-left-length 8 # maximum length for the left content of the status bar #set -g default-terminal xterm-256color # ----------------------------------------------------------------------------- # 使用插件 - via tpm # 1. 执行 git clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm # 2. 执行 bash ~/.tmux/plugins/tpm/bin/install_plugins # clone \"Tmux Plugin Manager (TPM)\" https://github.com/tmux-plugins/tpm.git ~/.tmux/plugins/tpm # clone \"tmux-battery\" https://github.com/tmux-plugins/tmux-battery.git ~/.tmux/plugins/tmux-battery # clone \"tmux-cpu\" https://github.com/tmux-plugins/tmux-cpu.git ~/.tmux/plugins/tmux-cpu # clone \"tmux-prefix-highlight\" https://github.com/tmux-plugins/tmux-prefix-highlight.git ~/.tmux/plugins/ # ----------------------------------------------------------------------------- setenv -g TMUX_PLUGIN_MANAGER_PATH '~/.tmux/plugins' # 推荐的插件（请去每个插件的仓库下读一读使用教程） set -g @plugin 'seebi/tmux-colors-solarized' set -g @plugin 'tmux-plugins/tmux-pain-control' set -g @plugin 'tmux-plugins/tmux-prefix-highlight' set -g @plugin 'tmux-plugins/tmux-resurrect' set -g @plugin 'tmux-plugins/tmux-sensible' set -g @plugin 'tmux-plugins/tmux-yank' set -g @plugin 'tmux-plugins/tpm' # tmux-resurrect set -g @resurrect-dir '~/.tmux/resurrect' # tmux-prefix-highlight set -g status-right '#{prefix_highlight} #H | %a %Y-%m-%d %H:%M' set -g @prefix_highlight_show_copy_mode 'on' set -g @prefix_highlight_copy_mode_attr 'fg=white,bg=blue' # 初始化 TPM 插件管理器 (放在配置文件的最后) run '~/.tmux/plugins/tpm/tpm' # ----------------------------------------------------------------------------- # 结束 # ----------------------------------------------------------------------------- 上面是相对比较简约的配置，直接copy复制到.tmux.conf文件中即可。 在这个配置中，\u003cprefix\u003e被改为了 Ctrl + x ","date":"2021-02-20","objectID":"/%E7%BB%88%E7%AB%AF%E5%B7%A5%E5%85%B7tmux/:1:0","series":null,"tags":["Linux","Tmux"],"title":"终端工具Tmux","uri":"/%E7%BB%88%E7%AB%AF%E5%B7%A5%E5%85%B7tmux/#tmux的基本配置"},{"categories":["Linux"],"content":" 常用的快捷键 Pane \u003cprefix\u003e 1 在右侧添加 Pane \u003cprefix\u003e 2 在下方添加 Pane \u003cprefix\u003e 0 关闭 Pane \u003cprefix\u003e o 在 Pane 之间切换 \u003cprefix\u003e H 向左扩大 Pane \u003cprefix\u003e J 向下扩大 Pane \u003cprefix\u003e K 向上扩大 Pane \u003cprefix\u003e L 向右扩大 Pane \u003cprefix\u003e m 最大化/还原 Pane \u003cprefix\u003e h/j/k/l 在 Pane 之间切换 Window \u003cprefix\u003e c 创建新 Window \u003cprefix\u003e \u003cC-h\u003e 切换至左侧 Window \u003cprefix\u003e \u003cC-l\u003e 切换至右侧 Window ```shell ","date":"2021-02-20","objectID":"/%E7%BB%88%E7%AB%AF%E5%B7%A5%E5%85%B7tmux/:2:0","series":null,"tags":["Linux","Tmux"],"title":"终端工具Tmux","uri":"/%E7%BB%88%E7%AB%AF%E5%B7%A5%E5%85%B7tmux/#常用的快捷键"},{"categories":["Eclipse"],"content":" Theia 是 Eclipse 推出的云端和桌面 IDE 平台，完全开源。Theia 是基于 VS Code 开发的，它的模块化特性非常适合二次开发，比如华为云 CloudIDE、阿里云 Function Compute IDE 便是基于 Theia 开发。 参考链接： 打造你的 Could IDE 随时随地敲代码，基于Theia快速部署自己的云开发环境 Theia官方文档 ","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:0:0","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#"},{"categories":["Eclipse"],"content":" Theia前置软件 nodejs 10以上 curl -o- https://raw.githubusercontent.com/creationix/nvm/v0.33.5/install.sh | bash nvm install 10 yarn npm install -g yarn docker # centos 7版本 sudo yum install -y yum-utils device-mapper-persistent-data lvm2 sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo sudo yum install docker-ce docker-ce-cli containerd.io sudo service docker start sudo systemctl enable docker # or 脚本自动安装 curl -fsSL get.docker.com -o get-docker.sh sudo sh get-docker.sh # 如果连接速度慢，可加上--mirror参数指定镜像，如 sudo sh get-docker.sh --mirror Aliyun ","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:1:0","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#theia前置软件"},{"categories":["Eclipse"],"content":" 基于NodeJs版本","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:2:0","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#基于nodejs版本"},{"categories":["Eclipse"],"content":" 基础程序配置 新建一个目录：my-app mkdir my-app cd my-app 创建package.json文件 { \"private\": true, \"dependencies\": { \"@theia/callhierarchy\": \"next\", \"@theia/file-search\": \"next\", \"@theia/git\": \"next\", \"@theia/json\": \"next\", \"@theia/markers\": \"next\", \"@theia/messages\": \"next\", \"@theia/mini-browser\": \"next\", \"@theia/navigator\": \"next\", \"@theia/outline-view\": \"next\", \"@theia/plugin-ext-vscode\": \"next\", \"@theia/preferences\": \"next\", \"@theia/preview\": \"next\", \"@theia/search-in-workspace\": \"next\", \"@theia/terminal\": \"next\" }, \"devDependencies\": { \"@theia/cli\": \"next\" }, \"scripts\": { \"prepare\": \"yarn run clean \u0026\u0026 yarn build \u0026\u0026 yarn run download:plugins\", \"clean\": \"theia clean\", \"build\": \"theia build --mode development\", \"start\": \"theia start --plugins=local-dir:plugins\", \"download:plugins\": \"theia download:plugins\" }, \"theiaPluginsDir\": \"plugins\", \"theiaPlugins\": { \"vscode-builtin-css\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/css-1.39.1-prel.vsix\", \"vscode-builtin-html\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/html-1.39.1-prel.vsix\", \"vscode-builtin-javascript\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/javascript-1.39.1-prel.vsix\", \"vscode-builtin-json\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/json-1.39.1-prel.vsix\", \"vscode-builtin-markdown\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/markdown-1.39.1-prel.vsix\", \"vscode-builtin-npm\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/npm-1.39.1-prel.vsix\", \"vscode-builtin-scss\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/scss-1.39.1-prel.vsix\", \"vscode-builtin-typescript\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/typescript-1.39.1-prel.vsix\", \"vscode-builtin-typescript-language-features\": \"https://github.com/theia-ide/vscode-builtin-extensions/releases/download/v1.39.1-prel/typescript-language-features-1.39.1-prel.vsix\" } } 本质上，Theia应用程序和扩展包都是Node.js包。每一个包都包含一个package.json文件，里面列出了包的一些元数据，如name、version、运行时和构建时的依赖关系等。 name和version被省略了，因为我们不打算将它作为一个依赖项来使用。同时它被标记为private，因为不打算将它发布为一个独立的Node.js包。 我们在dependencies中列出了所有运行时依赖的扩展包，如@theia/navigator。 有些扩展包需要额外的工具来进行安装，例如，@theia/python需要Python Language Server来安装。此时你需要参考Theia官方文档。 我们将@theis/cli列为构建时的依赖项，它提供了构建和运行应用程序的脚本。 在上面的package.json文件中，我们加入了vscode插件。 ","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:2:1","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#基础程序配置"},{"categories":["Eclipse"],"content":" 构建 首先安装所有依赖 yarn 其次使用Theia Cli编译程序 yarn theia build yarn在我们的应用程序上下文中查找@theia/cli提供的theia可执行文件，然后使用theia执行build命令。 由于该应用程序默认情况下是在生产模式下构建的（即经过混淆和缩小），因此可能需要一段时间。 ","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:2:2","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#构建"},{"categories":["Eclipse"],"content":" 运行 对于程序，需要提供一个工作区路径作为第一个参数打开，并提供–hostname和–port选项以将应用程序部署在特定的网络接口和端口上，例如 在所有接口和端口8080上打开/workspace： yarn start /my-workspace --hostname 0.0.0.0 --port 8080 ","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:2:3","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#运行"},{"categories":["Eclipse"],"content":" 使用 Docker 构建","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:3:0","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#使用-docker-构建"},{"categories":["Eclipse"],"content":" 从 theia-apps 快速构建 Theia提供了不同版本的镜像，可以在theia-apps 选择自己需要的语言版本，可以支持 C++/Go/Python/Java/PHP/Ruby等多种语言。最简单的方法，就是直接获取镜像启动容器。 docker pull theiaide/theia-full docker run -it --init -p 3000:3000 -v \"$(pwd):/home/project:cached\" theiaide/theia-full:latest 其中，$(pwd) 代表的是将当前目录挂载到 Docker 容器中，也可以指定文件目录。 然而，需要特别注意的是，Theia 本身没有认证机制，任何知道公网 IP 和端口号的人都可使用。因此，不推荐这种方法。 ","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:3:1","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#从-theia-apps-快速构建"},{"categories":["Eclipse"],"content":" 构建更安全的版本 Theia-https-docker 增加了 token 认证和 https，可以在标准镜像中加入 security layer，强烈建议使用它构造自己的镜像。构建也非常简单，按以下三个步骤操作即可，其中第三步的 –build-arg app= 填入需要使用的语言版本，这里使用的也是 full 版本。 git clone https://github.com/theia-ide/theia-apps.git cd theia-apps/theia-https-docker docker build . --build-arg app=theia-full -t theiaide/theia-full-security 构建完成后，可以通过docke images查看镜像 docker run --init -itd -p 10443:10443 -e token=mysecrettoken -v \"/home/coding:/home/project:cached\" theiaide/theia-full-security token 后接的是访问口令,/home/coding是指定的目录，打开 https://ip地址:10443，输入 token 便可打开 Web IDE。也可直接使用 https://ip地址:10443/?token=mysecrettoken 直接打开。 ","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:3:2","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#构建更安全的版本"},{"categories":["Eclipse"],"content":" 解决权限问题 使用docker版的Theia，会发现Theia 无法写入文件。这是 Theia 默认使用了 1000 的 userid，跟宿主机不一样，从而造成的权限问题。 解决方法有这么几个： 将挂载的文件权限改为 777，这种方法不太安全： chmod -R 777 /home/coding 指定用户运行，但如果使用的是 root，仍会有些不安全：docker run --user=root --init -it -p 10443:10443 -e token=mysecrettoken -v \"/home/coding:/home/project:cached\" theiaide/theia-full-security 将挂载的文件夹属主改为1000，推荐这种方法： chown -R 1000 /home/coding ","date":"2021-01-23","objectID":"/eclipse-theia%E6%95%99%E7%A8%8B/:3:3","series":null,"tags":["Eclipse","docker"],"title":"Eclipse Theia教程","uri":"/eclipse-theia%E6%95%99%E7%A8%8B/#解决权限问题"},{"categories":["docker"],"content":" 近期用到docker做实验，因此记录一下学习docker的笔记. 参考链接： 一篇不一样的docker原理解析 一篇不一样的docker原理解析 提高篇 Docker 核心技术与实现原理 终于有人把 Docker 讲清楚了，万字详解！ docker 镜像基本原理和核心概念 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:0","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#"},{"categories":["docker"],"content":" docker的前世今生docker是一个开源的应用容器引擎，基于Go语言开发的一种虚拟化技术，可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的linux服务器。docker底层是一种LXC技术，在LXC的基础之上，docker提供了一系列更强大的功能。 LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。相当于C++中的NameSpace。容器有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。 与传统虚拟化技术相比，它的优势在于： 与宿主机使用同一个内核，性能损耗小； 不需要指令级模拟； 不需要即时(Just-in-time)编译； 容器可以在CPU核心的本地运行指令，不需要任何专门的解释机制； 避免了准虚拟化和系统调用替换中的复杂性； 轻量级隔离，在隔离的同时还提供共享机制，以实现容器与宿主机的资源共享。 总结：Linux Container是一种轻量级的虚拟化的手段。 Linux Container提供了在单一可控主机节点上支持多个相互隔离的server container同时执行的机制。Linux Container有点像chroot，提供了一个拥有自己进程和网络空间的虚拟环境，但又有别于虚拟机，因为lxc是一种操作系统层次上的资源的虚拟化。 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:1:0","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#docker的前世今生"},{"categories":["docker"],"content":" 传统虚拟机概念 广义来说，虚拟机是一种模拟系统，即在软件层面上通过模拟硬件的输入和输出，让虚拟机的操作系统得以运行在没有物理硬件的环境中（也就是宿主机的操作系统上），其中能够模拟出硬件输入输出，让虚拟机的操作系统可以启动起来的程序，被叫做hypervisor。 传统虚拟机的核心技术是通过软件模拟硬件，从而让虚拟机的操作系统运行起来，当宿主机的硬件架构和虚拟硬件架构一致时，不模拟硬件输入输出，只是做下真实硬件输入输出的搬运工，那么虚拟机的指令执行速度，就可以和宿主机一致了，而当宿主机的硬件架构和虚拟机硬件架构不一致时，需要模拟硬件的输入输出，此时虚拟机启动和运行速度相对就比较慢了。 经典的例子： 在linux系统上可以通过vm或者virtual软件安装Windows系统，虚拟windows系统速度和原生系统速度差不多 在linux或者Windows系统上模拟android系统，则很慢，原因是android架构和windows的架构不一致 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:1:1","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#传统虚拟机概念"},{"categories":["docker"],"content":" 容器化概念 一般来说，虚拟机都会有自己的kernel，自己的硬件，这样虚拟机启动的时候需要先做开机自检，启动kernel，启动用户进程等一系列行为，虽然现在电脑运行速度挺快，但是这一系列检查做下来，也要几十秒，也就是虚拟机需要几十秒来启动。 计算机科学家发现其实我们创建虚拟机也不一定需要模拟硬件的输入和输出，假如宿主机和虚拟机他们的kernel是一致的，就不用做硬件输入输出的搬运工了，只需要做kernel输入输出的搬运工即可，为了有别于硬件层面的虚拟机，这种虚拟机被命名为 操作系统层虚拟化，也被叫做容器。 在虚拟机的系统中，虚拟机认为自己有独立的文件系统，进程系统，内存系统，等等一系列，所以为了让容器接近虚拟机，也需要有独立的文件系统，进程系统，内存系统，等等一系列，为了达成这一目的，主机系统采用的办法是：只要隔离容器不让它看到主机的文件系统，进程系统，内存系统，等等一系列，那么容器系统就是一个接近虚拟机的玩意了 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:1:2","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#容器化概念"},{"categories":["docker"],"content":" docker的核心技术","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:0","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#docker的核心技术"},{"categories":["docker"],"content":" Namespaces 命名空间（namespaces）是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到完全隔离，就像运行在多台不同的机器上一样。 在这种情况下，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这也是我们不想看到的，而 Docker 其实就通过 Linux 的 Namespaces 对不同的容器实现了隔离。 Linux 的命名空间机制提供了以下七种不同的命名空间，包括 CLONE_NEWCGROUP、CLONE_NEWIPC、CLONE_NEWNET、CLONE_NEWNS、CLONE_NEWPID、CLONE_NEWUSER 和 CLONE_NEWUTS，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:1","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#namespaces"},{"categories":["docker"],"content":" 进程 进程是 Linux 以及现在操作系统中非常重要的概念，它表示一个正在执行的程序，也是在现代分时系统中的一个任务单元。在linux系统里面，有两个进程非常特殊，一个是 pid 为 1 的 /sbin/init 进程，另一个是 pid 为 2 的 kthreadd 进程，这两个进程都是被 Linux 中的上帝进程 idle 创建出来的，其中前者负责执行内核的一部分初始化工作和系统配置，也会创建一些类似 getty 的注册进程，而后者负责管理和调度其他的内核进程。 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:2","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#进程"},{"categories":["docker"],"content":" 网络 如果 Docker 的容器通过 Linux 的命名空间完成了与宿主机进程的网络隔离，但是却有没有办法通过宿主机的网络与整个互联网相连，就会产生很多限制，所以 Docker 虽然可以通过命名空间创建一个隔离的网络环境，但是 Docker 中的服务仍然需要与外界相连才能发挥作用。 每一个使用 docker run 启动的容器其实都具有单独的网络命名空间，Docker 为我们提供了四种不同的网络模式，Host、Container、None 和 Bridge 模式。 在默认情况下，每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在创建的容器中，会加入到名为 docker0 网桥中。docker0 会为每一个容器分配一个新的 IP 地址并将 docker0 的 IP 地址设置为默认的网关。网桥 docker0 通过 iptables 中的配置与宿主机器上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 docker0 并由网桥分发给对应的机器。 Docker 通过 Linux 的命名空间实现了网络的隔离，又通过 iptables 进行数据包转发，让 Docker 容器能够优雅地为宿主机器或者其他容器提供服务。 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:3","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#网络"},{"categories":["docker"],"content":" 挂载点 虽然我们已经通过 Linux 的命名空间解决了进程和网络隔离的问题，在 Docker 进程中我们已经没有办法访问宿主机器上的其他进程并且限制了网络的访问，但是 Docker 容器中的进程仍然能够访问或者修改宿主机器上的其他目录，这是我们不希望看到的。 在新的进程中创建隔离的挂载点命名空间需要在 clone 函数中传入 CLONE_NEWNS，这样子进程就能得到父进程挂载点的拷贝，如果不传入这个参数子进程对文件系统的读写都会同步回父进程以及整个主机的文件系统。 如果一个容器需要启动，那么它一定需要提供一个根文件系统（rootfs），容器需要使用这个文件系统来创建一个新的进程，所有二进制的执行都必须在这个根文件系统中。 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:4","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#挂载点"},{"categories":["docker"],"content":" CGroups 我们通过 Linux 的命名空间为新创建的进程隔离了文件系统、网络并与宿主机器之间的进程相互隔离，但是命名空间并不能够为我们提供物理资源上的隔离，比如 CPU 或者内存，如果在同一台机器上运行了多个对彼此以及宿主机器一无所知的『容器』，这些容器却共同占用了宿主机器的物理资源。 如果其中的某一个容器正在执行 CPU 密集型的任务，那么就会影响其他容器中任务的性能与执行效率，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了解决进程虚拟资源隔离之后的主要问题，而 Control Groups（简称 CGroups）就是能够隔离宿主机器上的物理资源，例如 CPU、内存、磁盘 I/O 和网络带宽。 每一个 CGroup 下面都有一个 tasks 文件，其中存储着属于当前控制组的所有进程的 pid，作为负责 cpu 的子系统，cpu.cfs_quota_us 文件中的内容能够对 CPU 的使用作出限制，如果当前文件的内容为 50000，那么当前控制组中的全部进程的 CPU 占用率不能超过 50%。 如果系统管理员想要控制 Docker 某个容器的资源使用率就可以在 docker 这个父控制组下面找到对应的子控制组并且改变它们对应文件的内容，当然我们也可以直接在程序运行时就使用参数，让 Docker 进程去改变相应文件中的内容。 当我们使用 Docker 关闭掉正在运行的容器时，Docker 的子控制组对应的文件夹也会被 Docker 进程移除，Docker 在使用 CGroup 时其实也只是做了一些创建文件夹改变文件内容的文件操作，不过 CGroup 的使用也确实解决了我们限制子容器资源占用的问题，系统管理员能够为多个容器合理的分配资源并且不会出现多个容器互相抢占资源的问题。 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:5","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#cgroups"},{"categories":["docker"],"content":" 存储驱动 想要理解 Docker 使用的存储驱动，我们首先需要理解 Docker 是如何构建并且存储镜像的，也需要明白 Docker 的镜像是如何被每一个容器所使用的；Docker 中的每一个镜像都是由一系列只读的层组成的，Dockerfile 中的每一个命令都会在已有的只读层上创建一个新的层： FROM ubuntu:15.04 COPY . /app RUN make /app CMD python /app/app.py 当镜像被 docker run 命令创建时就会在镜像的最上层添加一个可写的层，也就是容器层，所有对于运行时容器的修改其实都是对这个容器读写层的修改。 容器和镜像的区别就在于，所有的镜像都是只读的，而每一个容器其实等于镜像加上一个可读写的层，也就是同一个镜像可以对应多个容器。 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:6","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#存储驱动"},{"categories":["docker"],"content":" AUFS AUFS 即 Advanced UnionFS，作为联合文件系统，它能够将不同文件夹中的层联合（Union）到了同一个文件夹中，这些文件夹在 AUFS 中称作分支，整个『联合』的过程被称为联合挂载（Union Mount）。 每一个镜像层或者容器层都是 /var/lib/docker/ 目录下的一个子文件夹；在 Docker 中，所有镜像层和容器层的内容都存储在 /var/lib/docker/aufs/diff/ 目录中。 而 /var/lib/docker/aufs/layers/ 中存储着镜像层的元数据，每一个文件都保存着镜像层的元数据，最后的 /var/lib/docker/aufs/mnt/ 包含镜像或者容器层的挂载点，最终会被 Docker 通过联合的方式进行组装。 上面的这张图片非常好的展示了组装的过程，每一个镜像层都是建立在另一个镜像层之上的，同时所有的镜像层都是只读的，只有每个容器最顶层的容器层才可以被用户直接读写，所有的容器都建立在一些底层服务（Kernel）上，包括命名空间、控制组、rootfs 等等，这种容器的组装方式提供了非常大的灵活性，只读的镜像层通过共享也能够减少磁盘的占用。 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:7","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#aufs"},{"categories":["docker"],"content":" docker架构 docker daemon就是docker的守护进程即server端，可以是远程的，也可以是本地的，这个不是C/S架构吗，客户端Docker client 是通过rest api进行通信 docker cli 用来管理容器和镜像，客户端提供一个只读镜像，然后通过镜像可以创建多个容器，这些容器可以只是一个RFS（Root file system根文件系统），也可以ishi一个包含了用户应用的RFS，容器再docker client中只是要给进程，两个进程之间互不可见 用户不能与server直接交互，但可以通过与容器这个桥梁来交互，由于是操作系统级别的虚拟技术，中间的损耗几乎可以不计 以 docker pull为例讲解，docker 的工作：docker client 组织配置和参数，把 pull 指令发送给 docker server，server 端接收到指令之后会交给对应的 handler。handler 会新开一个 CmdPull job 运行，这个 job 在 docker daemon 启动的时候被注册进来，所以控制权就到了 docker daemon 这边。docker daemon 是怎么根据传过来的 registry 地址、repo 名、image 名和tag 找到要下载的镜像呢？具体流程如下： 获取 repo 下面所有的镜像 id：GET /repositories/{repo}/images 获取 repo 下面所有 tag 的信息: GET /repositories/{repo}/tags 根据 tag 找到对应的镜像 uuid，并下载该镜像 获取该镜像的 history 信息，并依次下载这些镜像层: GET /images/{image_id}/ancestry 如果这些镜像层已经存在，就 skip，不存在的话就继续 获取镜像层的 json 信息：GET /images/{image_id}/json 下载镜像内容： GET /images/{image_id}/layer 下载完成后，把下载的内容存放到本地的 UnionFS 系统 在 TagStore 添加刚下载的镜像信息 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:3:0","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#docker架构"},{"categories":["docker"],"content":" docker使用","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:0","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#docker使用"},{"categories":["docker"],"content":" docker配置vim /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target Wants=docker-storage-setup.service Requires=docker-cleanup.timer [Service] Type=notify NotifyAccess=main EnvironmentFile=-/run/containers/registries.conf EnvironmentFile=-/etc/sysconfig/docker EnvironmentFile=-/etc/sysconfig/docker-storage EnvironmentFile=-/etc/sysconfig/docker-network Environment=GOTRACEBACK=crash Environment=DOCKER_HTTP_HOST_COMPAT=1 Environment=PATH=/usr/libexec/docker:/usr/bin:/usr/sbin ExecStart=/usr/bin/dockerd-current --registry-mirror=https://rfcod7oz.mirror.aliyuncs.com \\ --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current \\ --default-runtime=docker-runc \\ --exec-opt native.cgroupdriver=systemd \\ --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \\ --init-path=/usr/libexec/docker/docker-init-current \\ --seccomp-profile=/etc/docker/seccomp.json \\ $OPTIONS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $ADD_REGISTRY \\ $BLOCK_REGISTRY \\ $INSECURE_REGISTRY \\ $REGISTRIES ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=0 Restart=on-abnormal KillMode=process [Install] WantedBy=multi-user.target 如果更改存储目录就添加--graph=/opt/docker,如果要更改DNS--dns=xxxx的方式指定 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:1","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#docker配置"},{"categories":["docker"],"content":" 简单入门命令 docker images # 查看已经下载的镜像 docker save nginx \u003e/tmp/nginx.tar.gz # 导出镜像 docker rmi -f nginx # 删除镜像 docker load \u003c/tmp/nginx.tar.gz # 导入镜像 docker ps # 查看容器 docker exec -it mynginx sh # 交互式进入容器 docker ps -a #-a :显示所有的容器，包括未运行的 docker inspect mynginx # 查看容器详细信息 docker logs -f mynginx # -f 挂起这个终端，动态查看日志 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:2","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#简单入门命令"},{"categories":["docker"],"content":" docker image 命令 docker images ：列出 docker host 机器上的镜像，可以使用 -f 进行过滤 docker build：从 Dockerfile 中构建出一个镜像 docker history：列出某个镜像的历史 docker import：从 tarball 中创建一个新的文件系统镜像 docker pull：从 docker registry 拉去镜像 docker push：把本地镜像推送到 registry docker rmi： 删除镜像 docker save：把镜像保存为 tar 文件 docker search：在 docker hub 上搜索镜像 docker tag：为镜像打上 tag 标记 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:3","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#docker-image-命令"},{"categories":["docker"],"content":" 仓库相关 docker search $KEY_WORD # 查找镜像 docker pull $REGISTRY:$TAG # 获取镜像 docker push $IMAGE_NAME:$IMAGE_TAG # 推送镜像到仓库，需要先登录 docker login $REGISTRY_URL # 登录仓库 docker logout $REGISTRY_URL # 退出仓库 docker info # 显示Docker详细的系统信息，可查看仓库地址 docker --help # 显示Docker的帮助信息 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:4","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#仓库相关"},{"categories":["docker"],"content":" 容器相关 docker attach $CONTAINER_ID # 启动一个已存在的docker容器 docker stop $CONTAINER_ID # 停止docker容器 docker start $CONTAINER_ID # 启动docker容器 docker restart $CONTAINER_ID # 重启docker容器 docker kill $CONTAINER_ID # 强制关闭docker容器 docker pause $CONTAINER_ID # 暂停容器 docker unpause $CONTAINER_ID # 恢复暂停的容器 docker rename $CONTAINER_ID # 重新命名docker容器 docker rm $CONTAINER_ID # 删除容器 docker exec $CONTAINER_ID # 运行已启动的容器 可以使用该命令配合-it进入容器交互执行 docker logs $CONTAINER_ID # 查看docker容器运行日志，确保正常运行 docker inspect $CONTAINER_ID # 查看container的容器属性，比如ip等等 docker port $CONTAINER_ID # 查看container的端口映射 docker top $CONTAINER_ID # 查看容器中正在运行的进程 docker commit $CONTAINER_ID $NEW_IMAGE_NAME:$NEW_IMAGE_TAG # 将容器保存为镜像 docker ps -a # 查看所有容器 docker stats # 查看容器的资源使用情况 Ctrl+P+Q进行退出容器，正常退出不关闭容器，如果使用exit退出，那么在退出之后会关闭容器 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:5","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#容器相关"},{"categories":["docker"],"content":" 镜像相关命令 docker images # 查看本地镜像 docker rmi $IMAGE_ID # 删除本地镜像 docker inspect $IMAGE_ID # 查看镜像详情 docker save $IMAGE_ID \u003e 文件路径 # 保存镜像为离线文件 docker save -o 文件路径 $IMAGE_ID # 保存镜像为离线文件 docker load \u003c 文件路径 # 加载文件为docker镜像 docker load -i 文件路径 # 加载文件为docker镜像 docker tag $IMAGE_ID $NEW_IMAGE_NAME:$NEW_IMAGE_TAG # 修改镜像TAG docker run 参数 $IMAGE_ID $CMD # 运行一个镜像 docker history $IMAGE_ID # 显示镜像每层的变更内容 ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:6","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#镜像相关命令"},{"categories":["docker"],"content":" docker run时参数 # -d，后台运行容器, 并返回容器ID；不指定时, 启动后开始打印日志, Ctrl+C退出命令同时会关闭容器 # -i，以交互模式运行容器, 通常与-t同时使用 # -t，为容器重新分配一个伪输入终端, 通常与-i同时使用 # --name container_name，设置容器名称, 不指定时随机生成 # -h container_hostname，设置容器的主机名, 默认随机生成 # --dns 8.8.8.8，指定容器使用的DNS服务器, 默认和宿主机一致 # -e docker_host=172.17.0.1，设置环境变量 # --cpuset=\"0-2\" or --cpuset=\"0,1,2\"，绑定容器到指定CPU运行 # -m 100M，设置容器使用内存最大值 # --net bridge，指定容器的网络连接类型, 支持bridge/host/none/container四种类型 # --ip 172.18.0.13，为容器指定固定IP（需要使用自定义网络none） # --expose 8081 --expose 8082，开放一个端口或一组端口，会覆盖镜像设置中开放的端口 # -p [宿主机端口]:[容器内端口]，宿主机到容器的端口映射，可指定宿主机的要监听的IP，默认为0.0.0.0 # -P，注意是大写的, 宿主机随机指定一组可用的端口映射容器expose的所有端口 # -v [宿主机目录路径]:[容器内目录路径]，挂载宿主机的指定目录（或文件）到容器内的指定目录（或文件） # --add-host [主机名]:[IP]，为容器hosts文件追加host, 默认会在hosts文件最后追加[主机名]:[容器IP] # --volumes-from [其他容器名]，将其他容器的数据卷添加到此容器 # --link [其他容器名]:[在该容器中的别名]，添加链接到另一个容器，在本容器hosts文件中加入关联容器的记录，效果类似于--add-host ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:7","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#docker-run时参数"},{"categories":["docker"],"content":" Dockerfile FROM , 从一个基础镜像构建新的镜像 FROM ubuntu MAINTAINER , 维护者信息 MAINTAINER William \u003cwlj@nicescale.com\u003e ENV , 设置环境变量 ENV TEST 1 RUN , 非交互式运行 shell 命令 RUN apt-get -y update RUN apt-get -y install nginx ADD , 将外部文件拷贝到镜像里 ,src 可以为 url ADD http://nicescale.com/ /data/nicescale.tgz WORKDIR /path/to/workdir, 设置工作目录 WORKDIR /var/www USER , 设置用户 ID USER nginx VULUME \u003c#dir\u003e, 设置 volume VOLUME [‘/data’] EXPOSE , 暴露哪些端口 EXPOSE 80 443 ENTRYPOINT [‘executable’, ‘param1’,’param2’] 执行命令 ENTRYPOINT [\"/usr/sbin/nginx\"] CMD [“param1”,”param2”] CMD [\"start\"] ","date":"2021-01-23","objectID":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:8","series":null,"tags":["linux","docker"],"title":"Docker学习笔记","uri":"/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#dockerfile"},{"categories":["vim"],"content":" 目前公司常用的环境还是linux的服务器，经常需要使用vim进行写代码，经过探索Neovim + Coc.nvim非常适合。以下全部采用源码安装（相比之下，直接用各个发行版的命令安装会更加简单）。参考博客 ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:0","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#"},{"categories":["vim"],"content":" Neovim 和插件安装neovim：下载地址 wget https://github.com/neovim/neovim/releases/download/v0.4.3/nvim-linux64.tar.gz tar -zxvf nvim-linux64.tar.gz 然后把neovim路径下的bin加入到~/.bashrc，然后在source一下就算是成功了。 Neovim的基本配置 filetype plugin on \" 设置为双字宽显示，否则无法完整显示如:☆ set ambiwidth=double set t_ut= \" 防止vim背景颜色错误 set showmatch \" 高亮匹配括号 set matchtime=1 set report=0 set ignorecase set nocompatible set noeb set softtabstop=4 set shiftwidth=4 set nobackup set autoread set nocompatible set nu \"设置显示行号 set backspace=2 \"能使用backspace回删 syntax on \"语法检测 set ruler \"显示最后一行的状态 set laststatus=2 \"两行状态行+一行命令行 set ts=4 set expandtab set autoindent \"设置c语言自动对齐 set t_Co=256 \"指定配色方案为256 \" set mouse=a \"设置可以在VIM使用鼠标 set selection=exclusive \" set selectmode=mouse,key set tabstop=4 \"设置TAB宽度 set history=1000 \"设置历史记录条数 \" 配色方案 \" let g:seoul256_background = 234 colo monokai set background=dark set shortmess=atl \" colorscheme desert \"共享剪切板 set clipboard+=unnamed set cmdheight=3 if version \u003e= 603 set helplang=cn set encoding=utf-8 endif set fencs=utf-8,ucs-bom,shift-jis,gb18030,gbk,gb2312,cp936 set termencoding=utf-8 set encoding=utf-8 set fileencodings=ucs-bom,utf-8,cp936 set fileencoding=utf-8 set updatetime=300 set shortmess+=c set signcolumn=yes \" autocmd FileType json syntax match Comment +\\/\\/.\\+$+ set foldmethod=indent \" 设置默认折叠方式为缩进 set foldlevelstart=99 \" 每次打开文件时关闭折叠 \" hi Normal ctermfg=252 ctermbg=none \"背景透明 \" au FileType gitcommit,gitrebase let g:gutentags_enabled=0 if has(\"autocmd\") au BufReadPost * if line(\"'\\\"\") \u003e 1 \u0026\u0026 line(\"'\\\"\") \u003c= line(\"$\") | exe \"normal! g'\\\"\" | endif endif inoremap jj \u003cEsc\u003e \"将jj映射到Esc ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:1","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#neovim-和插件安装"},{"categories":["vim"],"content":" Neovim 和插件安装neovim：下载地址 wget https://github.com/neovim/neovim/releases/download/v0.4.3/nvim-linux64.tar.gz tar -zxvf nvim-linux64.tar.gz 然后把neovim路径下的bin加入到~/.bashrc，然后在source一下就算是成功了。 Neovim的基本配置 filetype plugin on \" 设置为双字宽显示，否则无法完整显示如:☆ set ambiwidth=double set t_ut= \" 防止vim背景颜色错误 set showmatch \" 高亮匹配括号 set matchtime=1 set report=0 set ignorecase set nocompatible set noeb set softtabstop=4 set shiftwidth=4 set nobackup set autoread set nocompatible set nu \"设置显示行号 set backspace=2 \"能使用backspace回删 syntax on \"语法检测 set ruler \"显示最后一行的状态 set laststatus=2 \"两行状态行+一行命令行 set ts=4 set expandtab set autoindent \"设置c语言自动对齐 set t_Co=256 \"指定配色方案为256 \" set mouse=a \"设置可以在VIM使用鼠标 set selection=exclusive \" set selectmode=mouse,key set tabstop=4 \"设置TAB宽度 set history=1000 \"设置历史记录条数 \" 配色方案 \" let g:seoul256_background = 234 colo monokai set background=dark set shortmess=atl \" colorscheme desert \"共享剪切板 set clipboard+=unnamed set cmdheight=3 if version \u003e= 603 set helplang=cn set encoding=utf-8 endif set fencs=utf-8,ucs-bom,shift-jis,gb18030,gbk,gb2312,cp936 set termencoding=utf-8 set encoding=utf-8 set fileencodings=ucs-bom,utf-8,cp936 set fileencoding=utf-8 set updatetime=300 set shortmess+=c set signcolumn=yes \" autocmd FileType json syntax match Comment +\\/\\/.\\+$+ set foldmethod=indent \" 设置默认折叠方式为缩进 set foldlevelstart=99 \" 每次打开文件时关闭折叠 \" hi Normal ctermfg=252 ctermbg=none \"背景透明 \" au FileType gitcommit,gitrebase let g:gutentags_enabled=0 if has(\"autocmd\") au BufReadPost * if line(\"'\\\"\") \u003e 1 \u0026\u0026 line(\"'\\\"\") \u003c= line(\"$\") | exe \"normal! g'\\\"\" | endif endif inoremap jj \"将jj映射到Esc ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:1","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#neovim的基本配置"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + \u003ci\u003e 切换到第i个 tab nmap \u003cleader\u003e1 \u003cPlug\u003eAirlineSelectTab1 nmap \u003cleader\u003e2 \u003cPlug\u003eAirlineSelectTab2 nmap \u003cleader\u003e3 \u003cPlug\u003eAirlineSelectTab3 nmap \u003cleader\u003e4 \u003cPlug\u003eAirlineSelectTab4 nmap \u003cleader\u003e5 \u003cPlug\u003eAirlineSelectTab5 nmap \u003cleader\u003e6 \u003cPlug\u003eAirlineSelectTab6 nmap \u003cleader\u003e7 \u003cPlug\u003eAirlineSelectTab7 nmap \u003cleader\u003e8 \u003cPlug\u003eAirlineSelectTab8 nmap \u003cleader\u003e9 \u003cPlug\u003eAirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap \u003cleader\u003e- \u003cPlug\u003eAirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap \u003cleader\u003e+ \u003cPlug\u003eAirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003cq\u003e 退出当前的 tab nmap \u003cleader\u003eq :bp\u003ccr\u003e:bd #\u003ccr\u003e \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#插件安装"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#vim-plug"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#indentline"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#vim-monokai"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#vim-airline"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#rainbow"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#nerdtree"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#vim-cpp-enhanced-highlight"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#vim-snippets"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#cocnvim"},{"categories":["vim"],"content":" 插件安装 Vim-Plug目前由于vim插件越来越多，由此也出现了插件管理工具，目前用得比较广泛的有：Vundle，vim-plug和dein,推荐使用vim-plug(对Vundle更好的替换)，关于dein，引用某巨佬的原话：“过度设计”。 sh -c 'curl -fLo \"${XDG_DATA_HOME:-$HOME/.local/share}\"/nvim/site/autoload/plug.vim --create-dirs https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim' 这样，这个~/.local/share/nvim/site/autoload/plug.vim就会在你的目录下，并且vim会被调用。 创建nvim的配置文件（这个配置文件和vim的.vimrc）一样： mkdir ~/.config/nvim/ nvim ~/.config/nvim/init.vim 然后把 call plug#begin('~/.vim/plugged') call plug#end() 加入到init.vim中，这样以后在call begin和call end之间加上插件就可以使用了。 之后的每个插件在init.vim文件中配置好后，要进行保存退出，再次进入nvim,使用命令 :PlugInstall安装 indentLineindentLine此插件提供的一个可视化的缩进，把Plug 'Yggdroot/indentLine'，放到init.vim的call begin和call end之间，同时加入一些简单的配置： let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 vim-monokaivim-monokai，这个插件是nvim的一个主题，monokai这个配色是比较经典好看的主题。 在call begin 和 call end之间加上Plug 'crusoexia/vim-monokai'，然后把 colo monokai 加到init.vim后面。 vim-airlinevim-airline给nvim提供一个强大的状态栏和标签栏，当打开多个文本时，可以用它进行快速的切换，是一个很强大的工具。 在call begin 和 call end之间加上： Plug 'vim-airline/vim-airline' Plug 'vim-airline/vim-airline-themes' \"airline 的主题 然后在init.vim里加上一些个性的配置： \" 设置状态栏 let g:airline#extensions#tabline#enabled = 1 let g:airline#extensions#tabline#left_alt_sep = '|' let g:airline#extensions#tabline#buffer_nr_show = 0 let g:airline#extensions#tabline#formatter = 'default' let g:airline_theme = 'desertink' \" 主题 let g:airline#extensions#keymap#enabled = 1 let g:airline#extensions#tabline#buffer_idx_mode = 1 let g:airline#extensions#tabline#buffer_idx_format = { \\ '0': '0 ', \\ '1': '1 ', \\ '2': '2 ', \\ '3': '3 ', \\ '4': '4 ', \\ '5': '5 ', \\ '6': '6 ', \\ '7': '7 ', \\ '8': '8 ', \\ '9': '9 ' \\} \" 设置切换tab的快捷键 \u003c\\\u003e + 切换到第i个 tab nmap 1 AirlineSelectTab1 nmap 2 AirlineSelectTab2 nmap 3 AirlineSelectTab3 nmap 4 AirlineSelectTab4 nmap 5 AirlineSelectTab5 nmap 6 AirlineSelectTab6 nmap 7 AirlineSelectTab7 nmap 8 AirlineSelectTab8 nmap 9 AirlineSelectTab9 \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c-\u003e 切换到前一个 tab nmap - AirlineSelectPrevTab \" 设置切换tab的快捷键 \u003c\\\u003e + \u003c+\u003e 切换到后一个 tab nmap + AirlineSelectNextTab \" 设置切换tab的快捷键 \u003c\\\u003e + 退出当前的 tab nmap q :bp:bd # \" 修改了一些个人不喜欢的字符 if !exists('g:airline_symbols') let g:airline_symbols = {} endif let g:airline_symbols.linenr = \"CL\" \" current line let g:airline_symbols.whitespace = '|' let g:airline_symbols.maxlinenr = 'Ml' \"maxline let g:airline_symbols.branch = 'BR' let g:airline_symbols.readonly = \"RO\" let g:airline_symbols.dirty = \"DT\" let g:airline_symbols.crypt = \"CR\" rainbowrainbow是一个提供嵌套括号高亮的一个工具。 在call begin 和 call end之间加上Plug 'luochen1990/rainbow' 简单的修复相应的配置： let g:rainbow_active = 1 let g:rainbow_conf = { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ 'ctermfgs': ['lightyellow', 'lightcyan','lightblue', 'lightmagenta'], \\ 'operators': '_,_', \\ 'parentheses': ['start=/(/ end=/)/ fold', 'start=/\\[/ end=/\\]/ fold', 'start=/{/ end=/}/ fold'], \\ 'separately': { \\ '*': {}, \\ 'tex': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/'], \\ }, \\ 'lisp': { \\ 'guifgs': ['darkorange3', 'seagreen3', 'royalblue3', 'firebrick'], \\ }, \\ 'vim': { \\ 'parentheses': ['start=/(/ end=/)/', 'start=/\\[/ end=/\\]/', 'start=/{/ end=/}/ fold', 'start=/(/ end=/)/ containedin=vimFuncBody', 'start=/\\[/ end=/\\]/ containedin=vimFuncBody', 'start=/{/ end=/}/ fold containedin=vimFuncBody'], \\ }, \\ 'html': { \\ 'parentheses': ['start=/\\v\\\u003c((area|base|br|col|embed|hr|img|input|keygen|link|menuitem|meta|param|source|track|wbr)[ \u003e])@!\\z([-_:a-zA-Z0-9]+)(\\s+[-_:a-zA-Z0-9]+(\\=(\"[^\"]*\"|'.\"'\".'[^'.\"'\".']*'.\"'\".'|[^ '.\"'\".'\"\u003e\u003c=`]*))?)*\\\u003e/ end=#\u003c/\\z1\u003e# fold'], \\ }, \\ 'css': 0, \\ } \\} nerdtreenerdtree是一个树形的目录管理插件，可以方便在nvim中进行当前文件夹中的文件切换 在call begin 和 call end之间加上 Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' 进行以下的简单配置 \" autocmd vimenter * NERDTree \"自动开启Nerdtree ","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:2","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#cocvim"},{"categories":["vim"],"content":" 附录–个人配置 call plug#begin('~/.vim/plugged') \" 记得先创建该目录 Plug 'honza/vim-snippets' Plug 'sainnhe/sonokai' Plug 'vim-python/python-syntax' \" Plug 'vim-scripts/ctags.vim' Plug 'crusoexia/vim-monokai' Plug 'tmux-plugins/vim-tmux-focus-events' Plug 'vim-airline/vim-airline' Plug 'scrooloose/nerdcommenter' Plug 'vim-airline/vim-airline-themes' Plug 'luochen1990/rainbow' Plug 'octol/vim-cpp-enhanced-highlight' Plug 'Yggdroot/indentLine' Plug 'preservim/nerdtree' Plug 'Xuyuanp/nerdtree-git-plugin' Plug 'neoclide/coc.nvim', {'branch': 'release'} \" Plug 'majutsushi/tagbar' Plug 'lifepillar/vim-solarized8' call plug#end() let mapleader = \",\" \" set map leader filetype plugin on \" 设置为双字宽显示，否则无法完整显示如:☆ set ambiwidth=double set t_ut= \" 防止vim背景颜色错误 set showmatch \" 高亮匹配括号 set matchtime=1 set noshowmode \" block mode display set novisualbell noerrorbells set report=0 set ignorecase set cursorline \"highlight current line set noeb set softtabstop=2 set shiftwidth=2 set nobackup set autoread set nocompatible set nu \"设置显示行号 set backspace=2 \"能使用backspace回删 syntax on \"语法检测 set ruler \"显示最后一行的状态 set laststatus=2 \"两行状态行+一行命令行 \" set ts=4 set expandtab set autoindent \"设置c语言自动对齐 set t_Co=256 \"指定配色方案为256 \" set mouse=a \"设置可以在VIM使用鼠标 set selection=exclusive \" set selectmode=mouse,key set tabstop=2 \"设置TAB宽度 set history=1000 \"设置历史记录条数 set shortmess=atl set clipboard+=unnamed set cmdheight=1 if version \u003e= 603 set helplang=cn set encoding=utf-8 endif set fencs=utf-8,ucs-bom,shift-jis,gb18030,gbk,gb2312,cp936 set termencoding=utf-8 set encoding=utf-8 set fileencodings=ucs-bom,utf-8,cp936 \" set updatetime=300 set shortmess+=c set signcolumn=yes \" reset cursor when vim exits au VimLeave * set guicursor=a:ver25-blinkon0 \" +================================ 可视化缩进 =====================================+ \" \" set notermguicolors \" let g:sonokai_style = 'shusia' \" let g:sonokai_enable_italic = 1 \" let g:sonokai_disable_italic_comment = 1 \" colo sonokai colo monokai set background=dark hi CursorLine ctermfg=NONE ctermbg=NONE cterm=NONE hi Normal ctermfg=NONE ctermbg=234 guifg=NONE hi LineNr ctermfg=yellow ctermbg=236 cterm=NONE \" hi Terminal ctermfg=NONE ctermfg=NONE cterm=NONE hi EndOfBuffer ctermbg=NONE \" visual mode bg set foldmethod=indent \" 设置默认折叠方式为缩进 set foldlevelstart=99 \" 每次打开文件时关闭折叠 \" hi Normal ctermfg=252 ctermbg=none \"背景透明 \" au FileType gitcommit,gitrebase let g:gutentags_enabled=0 if has(\"autocmd\") au BufReadPost * if line(\"'\\\"\") \u003e 1 \u0026\u0026 line(\"'\\\"\") \u003c= line(\"$\") | exe \"normal! g'\\\"\" | endif endif \" neovim python set let g:python3_host_prog = \"/Users/gsscsd/miniconda3/bin/python\" let g:python_highlight_all = 1 \" +================================ 可视化缩进 =====================================+ \" let g:indent_guides_enable_on_vim_startup = 0 \" 默认关闭 let g:indent_guides_guide_size = 1 \" 指定对齐线的尺寸 let g:indent_guides_start_level = 2 \" 从第二层开始可视化显示缩进 \" +================================== NERDTree =======================================+ \" \" autocmd vimenter * NERDTree \"自动开启Nerdtree let g:NERDTreeWinSize = 25 \"设定 NERDTree 视窗大小 let NERDTreeShowBookmarks=1 \" 开启Nerdtree时自动显示Bookmarks \"打开vim时如果没有文件自动打开NERDTree \" autocmd vimenter * if !argc()|NERDTree|endif \"当NERDTree为剩下的唯一窗口时自动关闭 autocmd bufenter * if (winnr(\"$\") == 1 \u0026\u0026 exists(\"b:NERDTree\") \u0026\u0026 b:NERDTree.isTabTree()) | q | endif \" 设置树的显示图标 let g:NERDTreeDirArrowExpandable = '+' let g:NERDTreeDirArrowCollapsible = '-' let g:NERDTreeShowLineNumbers=0 \" 是否显示行号 let g:NERDTreeHidden=0 \"不显示隐藏文件 \"\"Making it prettier let NERDTreeMinimalUI = 1 let NERDTreeDirArrows = 1 \" autocmd vimenter * NERDTreeToggle let NERDTreeIgnore = ['\\.pyc$', '__pycache__'] map \u003cleader\u003enn :NERDTreeToggle\u003ccr\u003e map \u003cleader\u003enb :NERDTreeFromBookmark\u003cSpace\u003e map \u003cleader\u003enf :NERDTreeFind\u003ccr\u003e \" set key map nnoremap \u003csilent\u003e \u003cS-n\u003e :set nu!\u003cCR\u003e \" 开关行号 \" +================================== 按键映射 =======================================+ \" \" self key map: \" \u003cleader\u003es : open key \" \u003cleader\u003ed : close key \" \u003cleader\u003ee : norm key inoremap \u003csilent\u003e jj \u003cEsc\u003e \"将jj映射到Esc nmap \u003cleader\u003esp :","date":"2021-01-13","objectID":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/:0:3","series":null,"tags":["cocvim","vim"],"title":"Neovim+Coc.nvim配置 终端代码编辑环境","uri":"/neovim-coc_nvim%E9%85%8D%E7%BD%AE-%E7%BB%88%E7%AB%AF%E4%BB%A3%E7%A0%81%E7%BC%96%E8%BE%91%E7%8E%AF%E5%A2%83/#附录--个人配置"},{"categories":["剑指Offer"],"content":" 题目描述： 请实现一个函数用来匹配包括’.‘和’‘的正则表达式。模式中的字符’.‘表示任意一个字符，而’‘表示它前面的字符可以出现任意次（包含0次）。 在本题中，匹配是指字符串的所有字符匹配整个模式。例如，字符串\"aaa\"与模式\"a.a\"和\"abaca\"匹配，但是与\"aa.a\"和\"ab*a\"均不匹配 ","date":"2019-04-18","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之正则表达式匹配","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 首先，考虑特殊情况： 1\u003e两个字符串都为空，返回true 2\u003e当第一个字符串不空，而第二个字符串空了，返回false（因为这样，就无法匹配成功了,而如果第一个字符串空了，第二个字符串非空，还是可能匹配成功的，比如第二个字符串是“aaaa”,由于‘’之前的元素可以出现0次，所以有可能匹配成功）。 之后就开始匹配第一个字符，这里有两种可能：匹配成功或匹配失败。 考虑到pattern下一个字符可能是‘’， 这里我们分两种情况讨论：pattern下一个字符为‘’或不为： 1\u003epattern下一个字符不为‘’：这种情况比较简单，直接匹配当前字符。如果匹配成功，继续匹配下一个；如果匹配失败，直接返回false。注意这里的“匹配成功”，除了两个字符相同的情况外，还有一种情况，就是pattern的 当前字符为‘.’,同时str的当前字符不为‘\\0’。 2\u003epattern下一个字符为‘’时，稍微复杂一些，因为‘*’可以代表0个或多个。 这里把这些情况都考虑到： a\u003e当‘’匹配0个字符时，str当前字符不变，pattern当前字符后移两位，跳过这个‘’符号； b\u003e当‘*’匹配1个或多个时，str当前字符移向下一个，pattern当前字符不变。（这里匹配1个或多个可以看成一种情况，因为：当匹配一个时，由于str移到了下一个字符，而pattern字符不变，就回到了上边的情况a；当匹配多于一个字符时，相当于从str的下一个字符继续开始匹配），之后再写代码就很简单了。 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: bool match(char* str, char* pattern) { // 两个字符串都为空，返回true if (*str == '\\0' \u0026\u0026 *pattern == '\\0') return true; // 当第一个字符串不空，而第二个字符串空了，返回false if (*str != '\\0' \u0026\u0026 *pattern == '\\0') return false; // 如果pattern下一个字符为* if(*(pattern + 1) == '*') { // 当‘*’匹配1个或多个时，str当前字符移向下一个，pattern当前字符不变。 // 当‘*’匹配0个字符时，str当前字符不变，pattern当前字符后移两位，跳过这个‘*’符号 if(*pattern == '.' \u0026\u0026*str!='\\0' || *str == *pattern ) { return match(str + 1,pattern) || match(str,pattern + 2); } // 不过不相同，pattern当前字符后移两位，跳过这个‘*’符号 else return match(str,pattern + 2); } // 如果pattern下一个字符不为* else { // 如果匹配成功，继续匹配下一个 // 如果pattern的当前字符为‘.’,同时str的当前字符不为‘\\0’，继续匹配下一个 if((*str != '\\0' \u0026\u0026 *pattern == '.')|| *str == *pattern) return match(str+1,pattern+1); else return false; } } }; ","date":"2019-04-18","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D/:0:2","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之正则表达式匹配","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述: 输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 则依次打印出数字1,2,3,4,8,12,16,15,14,13,9,5,6,7,11,10. ","date":"2019-04-18","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之顺时针打印矩阵","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 顺时针打印就是按圈数循环打印，一圈包含两行或者两列， 在打印的时候会出现某一圈中只包含一行， 要判断从左向右打印和从右向左打印的时候是否会出现重复打印， 同样只包含一列时，要判断从上向下打印和从下向上打印的时候是否会出现重复打印的情况 时间复杂度$O(n^2)$, 空间复杂度$O(n)$. class Solution { public: vector\u003cint\u003e printMatrix(vector\u003cvector\u003cint\u003e \u003e matrix) { vector\u003cint\u003eres; int row=matrix.size();//行数 int collor=matrix[0].size();//列数 //计算打印的圈数 int circle=((row\u003ccollor?row:collor)-1)/2+1;//圈数 for(int i=0;i\u003ccircle;i++) { //从左向右打印 for(int j=i;j\u003ccollor-i;j++) res.push_back(matrix[i][j]); //从上往下的每一列数据 for(int k=i+1;k\u003crow-i;k++) res.push_back(matrix[k][collor-1-i]); //判断是否会重复打印(从右向左的每行数据) for(int m=collor-i-2;(m\u003e=i)\u0026\u0026(row-i-1!=i);m--) res.push_back(matrix[row-i-1][m]); //判断是否会重复打印(从下往上的每一列数据) for(int n=row-i-2;(n\u003ei)\u0026\u0026(collor-i-1!=i);n--) res.push_back(matrix[n][i]); } return res; } }; ","date":"2019-04-18","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之顺时针打印矩阵","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 用左上和右下的坐标定位出一次要旋转打印的数据，一次旋转打印结束后，往对角分别前进和后退一个单位。 提交代码时，主要的问题出在没有控制好后两个for循环，需要加入条件判断，防止出现单行或者单列的情况。 时间复杂度$O(n^2)$, 空间复杂度$O(n)$. class Solution { public: vector\u003cint\u003e printMatrix(vector\u003cvector\u003cint\u003e \u003e matrix) { int row = matrix.size(); int col = matrix[0].size(); vector\u003cint\u003e res; // 输入的二维数组非法，返回空的数组 if (row == 0 || col == 0) return res; // 定义四个关键变量，表示左上和右下的打印范围 int left = 0, top = 0, right = col - 1, bottom = row - 1; while (left \u003c= right \u0026\u0026 top \u003c= bottom) { // left to right for (int i = left; i \u003c= right; ++i) res.push_back(matrix[top][i]); // top to bottom for (int i = top + 1; i \u003c= bottom; ++i) res.push_back(matrix[i][right]); // right to left if (top != bottom) for (int i = right - 1; i \u003e= left; --i) res.push_back(matrix[bottom][i]); // bottom to top if (left != right) for (int i = bottom - 1; i \u003e top; --i) res.push_back(matrix[i][left]); left++,top++,right--,bottom--; } return res; } }; ","date":"2019-04-18","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5/:0:3","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之顺时针打印矩阵","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%A1%BA%E6%97%B6%E9%92%88%E6%89%93%E5%8D%B0%E7%9F%A9%E9%98%B5/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 牛客最近来了一个新员工Fish，每天早晨总是会拿着一本英文杂志，写些句子在本子上。同事Cat对Fish写的内容颇感兴趣，有一天他向Fish借来翻看，但却读不懂它的意思。例如，“student. a am I”。后来才意识到，这家伙原来把句子单词的顺序翻转了，正确的句子应该是“I am a student.”。Cat对一一的翻转这些单词顺序可不在行，你能帮助他么？ ","date":"2019-04-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%BF%BB%E8%BD%AC%E5%8D%95%E8%AF%8D%E9%A1%BA%E5%BA%8F%E5%88%97/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之翻转单词顺序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%BF%BB%E8%BD%AC%E5%8D%95%E8%AF%8D%E9%A1%BA%E5%BA%8F%E5%88%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { public: string ReverseSentence(string str) { string s = \"\"; stack\u003cstring\u003e st; if(str.size() == 0 ) return s; string temp = \"\"; for(int i = 0; i \u003c= str.length(); i++) { if(str[i] == ' ' || str[i] == '\\0') { st.push(temp); temp = \"\"; } else temp += str[i]; } while(!st.empty()) { s += st.top(); s += ' '; st.pop(); } s.pop_back(); return s; } }; ","date":"2019-04-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%BF%BB%E8%BD%AC%E5%8D%95%E8%AF%8D%E9%A1%BA%E5%BA%8F%E5%88%97/:0:2","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之翻转单词顺序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%BF%BB%E8%BD%AC%E5%8D%95%E8%AF%8D%E9%A1%BA%E5%BA%8F%E5%88%97/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: string ReverseSentence(string str) { string res = \"\", tmp = \"\"; for(unsigned int i = 0; i \u003c str.size(); ++i){ if(str[i] == ' ') res = \" \" + tmp + res, tmp = \"\"; else tmp += str[i]; } if(tmp.size()) res = tmp + res; return res; } }; ","date":"2019-04-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%BF%BB%E8%BD%AC%E5%8D%95%E8%AF%8D%E9%A1%BA%E5%BA%8F%E5%88%97/:0:3","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之翻转单词顺序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%BF%BB%E8%BD%AC%E5%8D%95%E8%AF%8D%E9%A1%BA%E5%BA%8F%E5%88%97/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述: 输入一个字符串,按字典序打印出该字符串中字符的所有排列。例如输入字符串abc,则打印出由字符a,b,c所能排列出来的所有字符串abc,acb,bac,bca,cab和cba。 输入一个字符串,长度不超过9(可能有字符重复),字符只包括大小写字母。 ","date":"2019-04-16","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之数组中的逆序对","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(nlogn)$, 空间复杂度：$O(n)$. class Solution { public: vector\u003cstring\u003e Permutation(string str) { vector\u003cstring\u003e res; if(str.length() == 0) return res; // 先排序 sort(str.begin(),str.end(),[](char a,char b){return a \u003c b;}); // 将第一个排列加入到数组中 res.push_back(str); // 下一个全排列 // next_permutation 获取下一个全排列 while(next_permutation(str.begin(),str.end())) {i res.push_back(str); } return res; } }; ","date":"2019-04-16","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之数组中的逆序对","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 思想： 1，3， 5， 7， 6， 4， 2， 1 第一步： 从后往前找到第一个比后面数小的,记做sw1. 1, 2, sw1, 7, 6, 4, 2, 1 第二步： 从后往前找到第一个比sw1数大的,记做sw2. 1, 2, sw1 5, 7, sw2 6, 4, 2, 1 第三步：交换sw1和 sw2 1, 2, sw1 6, 7, sw2 5 4, 2, 1 第四步：sw1之后的数进行排序 1, 2, sw1 6,1, 2, 4, 5, 7 时间复杂度：$O(nlogn)$, 空间复杂度：$O(n)$. class Solution { public: vector\u003cstring\u003e Permutation(string str) { vector\u003cstring\u003e res; if(str.length() == 0) return res; // 第三个是一个lambda表达式 sort(str.begin(),str.end(),[](char a,char b){return a \u003c b;}); res.push_back(str); while(findALL(str)) { res.push_back(str); } return res; } // 自定义的下一个全排列 bool findALL(string \u0026str) { // 定义两个位置指针 int sw1 = -1,sw2 = -1; // 从后向前寻找到第一个：str[i] \u003e str[i + 1] 位置指针 for(int i = str.length() - 2;i \u003e= 0;i--) { if(str[i ] \u003c str[i + 1]) { sw1 = i ; break; } } // 如果没有找到，说明没有下一个全排列 if(sw1 == -1) return false; // 从后向前寻找，找到第一个大于str[sw1]的位置指针 for(int j = str.length() - 1;j \u003e= sw1;j--) { if(str[j] \u003e str[sw1]) { sw2 = j; break; } } // 交换str的两个位置指针 swap(str[sw1],str[sw2]); // 将sw1后面的数全排列 sort(str.begin() + sw1 + 1,str.end()); return true; } }; ","date":"2019-04-16","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/:0:3","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之数组中的逆序对","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E6%8E%92%E5%88%97/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述：在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 输入描述:题目保证输入的数组中没有的相同的数字数据范围： 对于%50的数据,size\u003c=10^4对于%75的数据,size\u003c=10^5 对于%100的数据,size\u003c=2*10^5 示例1 输入 1,2,3,4,5,6,7,0 输出 7 ","date":"2019-04-16","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中的逆序对","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/#题目描述"},{"categories":["剑指Offer"],"content":" 题目描述：在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 输入描述:题目保证输入的数组中没有的相同的数字数据范围： 对于%50的数据,size\u003c=10^4对于%75的数据,size\u003c=10^5 对于%100的数据,size\u003c=2*10^5 示例1 输入 1,2,3,4,5,6,7,0 输出 7 ","date":"2019-04-16","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中的逆序对","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/#输入描述"},{"categories":["剑指Offer"],"content":" 题目描述：在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 输入描述:题目保证输入的数组中没有的相同的数字数据范围： 对于%50的数据,size\u003c=10^4对于%75的数据,size\u003c=10^5 对于%100的数据,size\u003c=2*10^5 示例1 输入 1,2,3,4,5,6,7,0 输出 7 ","date":"2019-04-16","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中的逆序对","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/#输入"},{"categories":["剑指Offer"],"content":" 题目描述：在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。输入一个数组,求出这个数组中的逆序对的总数P。并将P对1000000007取模的结果输出。 即输出P%1000000007 输入描述:题目保证输入的数组中没有的相同的数字数据范围： 对于%50的数据,size\u003c=10^4对于%75的数据,size\u003c=10^5 对于%100的数据,size\u003c=2*10^5 示例1 输入 1,2,3,4,5,6,7,0 输出 7 ","date":"2019-04-16","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中的逆序对","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/#输出"},{"categories":["剑指Offer"],"content":" 解题思路： 先把数组分割成子数组，先统计出子数组内部的逆序对的数目，然后再统计出两个相邻子数组之间的逆序对的数目。在统计逆序对的过程中，还需要对数组进行排序。 时间复杂度：$$O(nlogn)$$,空间复杂度：$$O(n)$$. class Solution { public: long countRes ; int InversePairs(vector\u003cint\u003e data) { countRes = 0; if(data.size() == 0) return 0; // 调用归并排序 MergeSort(data,0,data.size()-1); return countRes%1000000007 ; } // 归并排序 void MergeSort(vector\u003cint\u003e\u0026 data,int first,int end){ if(first \u003c end){ int mid = (first + end)/2; MergeSort(data,first,mid); MergeSort(data,mid+1,end); vector\u003cint\u003e tmp; MergeArray(data,first,mid,end,tmp); } } void MergeArray(vector\u003cint\u003e\u0026 data,int first,int mid,int end,vector\u003cint\u003e tmp){ int i = first;int m = mid; int j = mid + 1;int n = end; while(i\u003c=m \u0026\u0026 j\u003c=n){ if(data[i] \u003e data[j]){ tmp.push_back(data[i++]); countRes += n - j + 1; // ***** } else{ tmp.push_back(data[j++]); } } while(i\u003c=m) tmp.push_back(data[i++]); while (j\u003c=n) tmp.push_back(data[j++]); //更新data数组 int k = 0; for (int i = first; i \u003c= end \u0026\u0026k\u003ctmp.size(); i++) data[i] = tmp[k++]; } }; ","date":"2019-04-16","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中的逆序对","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E9%80%86%E5%BA%8F%E5%AF%B9/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个链表，输出该链表中倒数第k个结点。 ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/:0:1","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之链表中倒数第k个结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) { ListNode *p = pListHead; ListNode *q = pListHead; int i = 0; for( ;p != NULL;i++) { if(i \u003e= k) q = q -\u003e next; p = p -\u003e next; } return i \u003c k?NULL:q; } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/:0:2","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之链表中倒数第k个结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/#解题思路"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) { ListNode *p = pListHead; ListNode *q = pListHead; while(p \u0026\u0026 k \u003e 0 ) { p = p -\u003e next; k--; } while(p) { p = p -\u003e next; q = q -\u003e next; } return k \u003e 0 ? NULL:q; } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/:0:3","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之链表中倒数第k个结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 在一个排序的链表中，存在重复的结点，请删除该链表中重复的结点，重复的结点不保留，返回链表头指针。 例如，链表1-\u003e2-\u003e3-\u003e3-\u003e4-\u003e4-\u003e5 处理后为 1-\u003e2-\u003e5 ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9/:0:1","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之删除链表中重复的结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一: 非递归 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } }; */ class Solution { public: ListNode *deleteDuplication(ListNode *pHead) { if (!pHead) return NULL; if (!pHead-\u003enext) return pHead; ListNode *pre = new ListNode(0); pre-\u003enext = pHead; ListNode *p = pre; ListNode *q = pHead; while (q) { // 如果 q 和 q-\u003enext相同，那个把那个值记录下来，和后面的比较 while (q != NULL \u0026\u0026 q-\u003enext != NULL \u0026\u0026 q-\u003enext-\u003eval == q-\u003eval) { int tmp = q-\u003eval; // 凡是相等的直接往后移 while (q != NULL \u0026\u0026 q-\u003eval == tmp) q = q-\u003enext; } // 直到找到不相同的，把q 给 p的next p-\u003enext = q; // p 移向下一个 p = p-\u003enext; if (q) q = q-\u003enext; } return pre-\u003enext; } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9/:0:2","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之删除链表中重复的结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 递归 时间复杂度:$O(n)$,空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } }; */ class Solution { public: ListNode* deleteDuplication(ListNode* pHead) { if (pHead==NULL) return NULL; if (pHead!=NULL \u0026\u0026 pHead-\u003enext==NULL) return pHead; ListNode* current; if ( pHead-\u003enext-\u003eval==pHead-\u003eval){ current=pHead-\u003enext-\u003enext; while (current != NULL \u0026\u0026 current-\u003eval==pHead-\u003eval) current=current-\u003enext; return deleteDuplication(current); } else { current=pHead-\u003enext; pHead-\u003enext=deleteDuplication(current); return pHead; } } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9/:0:3","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之删除链表中重复的结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%88%A0%E9%99%A4%E9%93%BE%E8%A1%A8%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E7%BB%93%E7%82%B9/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6/:0:1","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之复杂链表的复制","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 1、遍历链表，复制每个结点，如复制结点A得到A1，将结点A1插到结点A后面； 2、重新遍历链表，复制老结点的随机指针给新结点，如A1.random = A.random.next; 3、拆分链表，将链表拆分为原链表和复制后的链表 时间复杂度：$O(n)$, 空间复杂度：$O(n)$. /* struct RandomListNode { int label; struct RandomListNode *next, *random; RandomListNode(int x) : label(x), next(NULL), random(NULL) { } }; */ /* *解题思路： *1、遍历链表，复制每个结点，如复制结点A得到A1，将结点A1插到结点A后面； *2、重新遍历链表，复制老结点的随机指针给新结点，如A1.random = A.random.next; *3、拆分链表，将链表拆分为原链表和复制后的链表 */ class Solution { public: RandomListNode* Clone(RandomListNode* pHead) { if(!pHead) return NULL; RandomListNode *currNode = pHead; // 复制next 如原来是A-\u003eB-\u003eC 变成A-\u003eA'-\u003eB-\u003eB'-\u003eC-\u003eC' while(currNode){ RandomListNode *node = new RandomListNode(currNode-\u003elabel); node-\u003enext = currNode-\u003enext; currNode-\u003enext = node; currNode = node-\u003enext; } currNode = pHead; // 复制random // 如果原始链表上的节点N的random指向S，则对应的复制节点N'的random指向S的下一个节点S' while(currNode){ RandomListNode *node = currNode-\u003enext; if(currNode-\u003erandom) { node-\u003erandom = currNode-\u003erandom-\u003enext; } currNode = node-\u003enext; } //拆分 RandomListNode *pCloneHead = pHead-\u003enext; RandomListNode *tmp; currNode = pHead; while(currNode-\u003enext){ tmp = currNode-\u003enext; currNode-\u003enext =tmp-\u003enext; currNode = tmp; } return pCloneHead; } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6/:0:2","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之复杂链表的复制","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 首先遍历一遍原链表，创建新链表（赋值label和next），用map关联对应结点；再遍历一遍，更新新链表的random指针。（注意map中应有NULL —-\u003e NULL的映射） 时间复杂度：$O(n)$, 空间复杂度：$O(n)$. /* struct RandomListNode { int label; struct RandomListNode *next, *random; RandomListNode(int x) : label(x), next(NULL), random(NULL) { } }; */ class Solution { public: RandomListNode* Clone(RandomListNode* pHead) { if(pHead==NULL) return NULL; map\u003cRandomListNode*,RandomListNode*\u003e m; RandomListNode* pHead1 = pHead; RandomListNode* pHead2 = new RandomListNode(pHead1-\u003elabel); RandomListNode* newHead = pHead2; m[pHead1] = pHead2; while(pHead1){ if(pHead1-\u003enext) pHead2-\u003enext = new RandomListNode(pHead1-\u003enext-\u003elabel); else pHead2-\u003enext = NULL; pHead1 = pHead1-\u003enext; pHead2 = pHead2-\u003enext; m[pHead1] = pHead2; } pHead1 = pHead; pHead2 = newHead; while(pHead1){ pHead2-\u003erandom = m[pHead1-\u003erandom]; pHead1 = pHead1-\u003enext; pHead2 = pHead2-\u003enext; } return newHead; } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6/:0:3","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之复杂链表的复制","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 把只包含质因子2、3和5的数称作丑数（Ugly Number）。例如6、8都是丑数，但14不是，因为它包含质因子7。 习惯上我们把1当做是第一个丑数。求按从小到大的顺序的第N个丑数。 ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%91%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之丑数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%91%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度：$O(n)$, 空间复杂度：$O(n)$. class Solution { public: int GetUglyNumber_Solution(int index) { // 0-6的丑数分别为0-6 if (index \u003c 7)return index; vector\u003cint\u003e res(index); res[0] = 1; // t2，t3，t5分别为三个队列的指针 int t2 = 0, t3 = 0, t5 = 0, i; for (i = 1; i \u003c index; ++i) { // 选出三个队列头最小的数 res[i] = min(res[t2] * 2, min(res[t3] * 3, res[t5] * 5)); // 这三个if有可能进入一个或者多个，进入多个是三个队列头最小的数有多个的情况 if (res[i] == res[t2] * 2)t2++; if (res[i] == res[t3] * 3)t3++; if (res[i] == res[t5] * 5)t5++; } return res[index - 1]; } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%91%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之丑数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%91%E6%95%B0/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 请实现两个函数，分别用来序列化和反序列化二叉树 ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E4%BA%8C%E5%8F%89%E6%A0%91/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之序列化二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E4%BA%8C%E5%8F%89%E6%A0%91/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 对于序列化：使用前序遍历，递归的将二叉树的值转化为字符，并且在每次二叉树的结点不为空时，在转化val所得的字符之后添加一个’ ， ‘作为分割。对于空节点则以 ‘#’ 代替。 对于反序列化：按照前序顺序，递归的使用字符串中的字符创建一个二叉树(特别注意： 在递归时，递归函数的参数一定要是char ** ，这样才能保证每次递归后指向字符串的指针会 随着递归的进行而移动！！！) /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } }; */ class Solution { public: char* Serialize(TreeNode *root) { if(root == NULL) return NULL; string str; Serialize(root, str); char *ret = new char[str.length() + 1]; int i; for(i = 0; i \u003c str.length(); i++){ ret[i] = str[i]; } ret[i] = '\\0'; return ret; } void Serialize(TreeNode *root, string\u0026 str){ if(root == NULL){ str += '#'; return ; } string r = to_string(root-\u003eval); str += r; str += ','; Serialize(root-\u003eleft, str); Serialize(root-\u003eright, str); } TreeNode* Deserialize(char *str) { if(str == NULL) return NULL; TreeNode *ret = Deserialize(\u0026str); return ret; } TreeNode* Deserialize(char **str){//由于递归时，会不断的向后读取字符串 if(**str == '#'){ //所以一定要用**str, ++(*str); //以保证得到递归后指针str指向未被读取的字符 return NULL; } int num = 0; while(**str != '\\0' \u0026\u0026 **str != ','){ num = num*10 + ((**str) - '0'); ++(*str); } TreeNode *root = new TreeNode(num); if(**str == '\\0') return root; else (*str)++; root-\u003eleft = Deserialize(str); root-\u003eright = Deserialize(str); return root; } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E4%BA%8C%E5%8F%89%E6%A0%91/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之序列化二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%BA%8F%E5%88%97%E5%8C%96%E4%BA%8C%E5%8F%89%E6%A0%91/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 请设计一个函数，用来判断在一个矩阵中是否存在一条包含某字符串所有字符的路径。路径可以从矩阵中的任意一个格子开始，每一步可以在矩阵中向左，向右，向上，向下移动一个格子。如果一条路径经过了矩阵中的某一个格子，则之后不能再次进入这个格子。 例如 a b c e s f c s a d e e 这样的3 X 4 矩阵中包含一条字符串\"bcced\"的路径，但是矩阵中不包含\"abcb\"路径，因为字符串的第一个字符b占据了矩阵中的第一行第二个格子之后，路径不能再次进入该格子。 ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%9F%A9%E9%98%B5%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之矩阵中的路径","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%9F%A9%E9%98%B5%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 回溯算法： 这是一个可以用回朔法解决的典型题。首先，在矩阵中任选一个格子作为路径的起点。如果路径上的第i个字符不是ch，那么这个格子不可能处在路径上的 第i个位置。如果路径上的第i个字符正好是ch，那么往相邻的格子寻找路径上的第i+1个字符。除在矩阵边界上的格子之外，其他格子都有4个相邻的格子。 重复这个过程直到路径上的所有字符都在矩阵中找到相应的位置。 由于回朔法的递归特性，路径可以被开成一个栈。当在矩阵中定位了路径中前n个字符的位置之后，在与第n个字符对应的格子的周围都没有找到第n+1个 字符，这个时候只要在路径上回到第n-1个字符，重新定位第n个字符。 由于路径不能重复进入矩阵的格子，还需要定义和字符矩阵大小一样的布尔值矩阵，用来标识路径是否已经进入每个格子。 当矩阵中坐标为（row,col）的 格子和路径字符串中相应的字符一样时，从4个相邻的格子(row,col-1),(row-1,col),(row,col+1)以及(row+1,col)中去定位路径字符串中下一个字符 如果4个相邻的格子都没有匹配字符串中下一个的字符，表明当前路径字符串中字符在矩阵中的定位不正确，我们需要回到前一个，然后重新定位。 一直重复这个过程，直到路径字符串上所有字符都在矩阵中找到合适的位置 时间复杂度：$O(n^2)$, 空间复杂度：$O(n)$. class Solution { public: bool hasPath(char* matrix, int rows, int cols, char* str) { if(str==NULL||rows\u003c=0||cols\u003c=0) return false; bool *isOk=new bool[rows*cols](); for(int i=0;i\u003crows;i++) { for(int j=0;j\u003ccols;j++) if(isHsaPath(matrix,rows,cols,str,isOk,i,j)) return true; } return false; } bool isHsaPath(char *matrix,int rows,int cols,char *str,bool *isOk,int curx,int cury) { if(*str=='\\0') return true; if(cury==cols) { curx++; cury=0; } if(cury==-1) { curx--; cury=cols-1; } if(curx\u003c0||curx\u003e=rows) return false; if(isOk[curx*cols+cury]||*str!=matrix[curx*cols+cury]) return false; isOk[curx*cols+cury]=true; bool sign=isHsaPath(matrix,rows,cols,str+1,isOk,curx-1,cury) ||isHsaPath(matrix,rows,cols,str+1,isOk,curx+1,cury) ||isHsaPath(matrix,rows,cols,str+1,isOk,curx,cury-1) ||isHsaPath(matrix,rows,cols,str+1,isOk,curx,cury+1); isOk[curx*cols+cury]=false; return sign; } }; ","date":"2019-04-15","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%9F%A9%E9%98%B5%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之矩阵中的路径","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%9F%A9%E9%98%B5%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 地上有一个m行和n列的方格。一个机器人从坐标0,0的格子开始移动，每一次只能向左，右，上，下四个方向移动一格，但是不能进入行坐标和列坐标的数位之和大于k的格子。 例如，当k为18时，机器人能够进入方格（35,37），因为3+5+3+7 = 18。但是，它不能进入方格（35,38），因为3+5+3+8 = 19。请问该机器人能够达到多少个格子？ ","date":"2019-04-12","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%BF%90%E5%8A%A8%E8%8C%83%E5%9B%B4/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之机器人的运动范围","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%BF%90%E5%8A%A8%E8%8C%83%E5%9B%B4/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 核心思路： 1.从(0,0)开始走，每成功走一步标记当前位置为true,然后从当前位置往四个方向探索， 返回1 + 4 个方向的探索值之和。 2.探索时，判断当前节点是否可达的标准为： 1）当前节点在矩阵内； 2）当前节点未被访问过； 3）当前节点满足limit限制。 // 注意他的分析思路 比如说，节点是否可以访问，用函数来处理，位置数值的分解用函数来处理 class Solution { public: int movingCount(int threshold, int rows, int cols) { int count = 0; bool *flag = new bool[rows * cols](); count = move(threshold,rows,cols,0,0,flag); return count; } // 判断是否能移动 bool isMove(int rows,int cols,int i,int j,int threshold,bool *flag) { if(i \u003e= 0 \u0026\u0026 j \u003e= 0\u0026\u0026 i \u003c rows \u0026\u0026 j \u003c cols \u0026\u0026 !flag[i * cols + j] \u0026\u0026 getNum(i) + getNum(j) \u003c= threshold) return true; return false; } int getNum(int k) { int res = 0; while(k \u003e 0 ) { res += k % 10; k /= 10; } return res; } // 递归的移动函数 int move(int threshold,int rows,int cols,int i,int j,bool *flag) { int count = 0; if(isMove(rows,cols,i,j,threshold,flag)) { flag[i * cols + j] = true; count = 1 + move(threshold,rows,cols,i-1,j,flag) + move(threshold,rows,cols,i+1,j,flag) + move(threshold,rows,cols,i,j - 1,flag) + move(threshold,rows,cols,i,j + 1,flag); } return count; } }; ","date":"2019-04-12","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%BF%90%E5%8A%A8%E8%8C%83%E5%9B%B4/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之机器人的运动范围","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%BA%E5%99%A8%E4%BA%BA%E7%9A%84%E8%BF%90%E5%8A%A8%E8%8C%83%E5%9B%B4/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述: 输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 ","date":"2019-04-12","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之重建二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(n)$, 空间复杂度：$O(n)$. /** * Definition for binary tree * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: TreeNode* reConstructBinaryTree(vector\u003cint\u003e pre,vector\u003cint\u003e vin) { int inlen=vin.size(); if(inlen==0) return NULL; vector\u003cint\u003e left_pre,right_pre,left_in,right_in; //创建根节点，根节点肯定是前序遍历的第一个数 TreeNode* head=new TreeNode(pre[0]); //找到中序遍历根节点所在位置,存放于变量gen中 int gen=0; for(int i=0;i\u003cinlen;i++) { if (vin[i]==pre[0]) { gen=i; break; } } //对于中序遍历，根节点左边的节点位于二叉树的左边，根节点右边的节点位于二叉树的右边 //利用上述这点，对二叉树节点进行归并 for(int i=0;i\u003cgen;i++) { left_in.push_back(vin[i]); left_pre.push_back(pre[i+1]);//前序第一个为根节点 } for(int i=gen+1;i\u003cinlen;i++) { right_in.push_back(vin[i]); right_pre.push_back(pre[i]); } //和shell排序的思想类似，取出前序和中序遍历根节点左边和右边的子树 //递归，再对其进行上述所有步骤，即再区分子树的左、右子子数，直到叶节点 head-\u003eleft=reConstructBinaryTree(left_pre,left_in); head-\u003eright=reConstructBinaryTree(right_pre,right_in); return head; } }; ","date":"2019-04-12","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之重建二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. /** * Definition for binary tree * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: TreeNode* reConstructBinaryTree(vector\u003cint\u003e pre,vector\u003cint\u003e vin) { TreeNode *root = isreConstructBinaryTree(pre,0,pre.size()-1,vin,0,vin.size()-1); return root; } TreeNode* isreConstructBinaryTree(vector\u003cint\u003e pre,int startPre,int endPre,vector\u003cint\u003e in,int startIn,int endIn) { if(startPre\u003eendPre||startIn\u003eendIn) return NULL; TreeNode *root=new TreeNode(pre[startPre]); for(int i = startIn;i \u003c= endIn;i++) { if(in[i] == pre[startPre]) { root-\u003eleft = isreConstructBinaryTree(pre,startPre+1,startPre+i- startIn,in,startIn,i-1); root-\u003eright = isreConstructBinaryTree(pre,i-startIn+startPre+1,endPre,in,i+1,endIn); break; } } return root; } }; ","date":"2019-04-12","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/:0:3","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之重建二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%87%8D%E5%BB%BA%E4%BA%8C%E5%8F%89%E6%A0%91/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） ","date":"2019-04-12","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之树的子结构","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$， 空间复杂度: $O(1)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } };*/ class Solution { public: bool HasSubtree(TreeNode* pRoot1, TreeNode* pRoot2) { bool flag = false; if(pRoot1 \u0026\u0026 pRoot2) { //如果找到了对应pRoot2的根节点的点 if(pRoot1-\u003eval == pRoot2-\u003eval) //以这个根节点为为起点判断是否包含pRoot2 flag = isHasSubtree(pRoot1,pRoot2); //如果找不到，那么就再去pRoot1的左儿子当作起点，去判断是否包含pRoot2 if(!flag) flag = HasSubtree(pRoot1-\u003eleft,pRoot2); //如果找不到，那么就再去pRoot1的右儿子当作起点，去判断是否包含pRoot2 if(!flag) flag = HasSubtree(pRoot1-\u003eright,pRoot2); } return flag; } bool isHasSubtree(TreeNode* pRoot1, TreeNode* pRoot2) { if(pRoot2 == NULL) return true; if(pRoot1 == NULL) return false; if(pRoot1-\u003eval != pRoot2-\u003eval) return false; return isHasSubtree(pRoot1-\u003eleft, pRoot2-\u003eleft) \u0026\u0026 isHasSubtree(pRoot1-\u003eright, pRoot2-\u003eright); } }; ","date":"2019-04-12","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之树的子结构","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 请实现一个函数按照之字形打印二叉树，即第一行按照从左到右的顺序打印，第二层按照从右至左的顺序打印，第三行按照从左到右的顺序打印，其他行以此类推。 ","date":"2019-04-11","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8C%89%E4%B9%8B%E5%AD%97%E5%BD%A2%E9%A1%BA%E5%BA%8F%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之按之字形顺序打印二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8C%89%E4%B9%8B%E5%AD%97%E5%BD%A2%E9%A1%BA%E5%BA%8F%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 使用两个栈进行转化 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } }; */ class Solution { public: vector\u003cvector\u003cint\u003e \u003e Print(TreeNode* pRoot) { // 奇数层 stack\u003cTreeNode *\u003e s1; // 偶数层 stack\u003cTreeNode *\u003e s2; vector\u003cvector\u003cint\u003e \u003e vec; if(!pRoot) return vec; s1.push(pRoot); // 记录放在了那个栈里面 int flag = 0; while(!s1.empty() || !s2.empty()) { int ssize1 = s1.size(); vector\u003cint\u003e temp; for(int i = 0; i \u003c ssize1;i++) { TreeNode* t1 = s1.top(); if(t1 -\u003e left)s2.push(t1-\u003eleft); if(t1 -\u003e right)s2.push(t1-\u003eright); temp.push_back(t1 -\u003e val); s1.pop(); flag = 1; } // 只有走s1时才放入到vec中 if(flag == 1) { vec.push_back(temp); temp.clear(); } int ssize2 = s2.size(); for(int j = 0; j \u003c ssize2;j++) { TreeNode* t2 = s2.top(); if(t2 -\u003e right)s1.push(t2-\u003eright); if(t2 -\u003e left)s1.push(t2-\u003eleft); temp.push_back(t2 -\u003e val); s2.pop(); flag = 2; } // 只有走s2时才放入到vec中 if(flag == 2) { vec.push_back(temp); temp.clear(); } } return vec; } }; ","date":"2019-04-11","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8C%89%E4%B9%8B%E5%AD%97%E5%BD%A2%E9%A1%BA%E5%BA%8F%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之按之字形顺序打印二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8C%89%E4%B9%8B%E5%AD%97%E5%BD%A2%E9%A1%BA%E5%BA%8F%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述: 输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 ","date":"2019-04-11","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%E5%BA%8F%E5%88%97/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉搜索树的后序遍历序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%E5%BA%8F%E5%88%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 非递归也是一个基于递归的思想： 左子树一定比右子树小，因此去掉根后，数字分为left，right两部分， right部分的最后一个数字是右子树的根他也比左子树所有值大，因此我们可以每次只看有子树是否符合条件 即可，即使到达了左子树，左子树也可以看出由左右子树组成的树还想右子树那样处理 对于左子树回到了原问题，对于右子树，左子树的所有值都比右子树的根小可以暂时把他看出右子树的左子树 只需看看右子树的右子树是否符合要求即可 时间复杂度：$O(n^2)$, 空间复杂度$O(1)$. class Solution { public: bool VerifySquenceOfBST(vector\u003cint\u003e sequence) { int s = sequence.size(); if(s == 0) return false; int i = 0; while(--s) { while(sequence[i]\u003csequence[s]) i++; while(sequence[i]\u003esequence[s]) i++; if(i \u003c s)return false; i=0; } return true; } }; ","date":"2019-04-11","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%E5%BA%8F%E5%88%97/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉搜索树的后序遍历序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E5%90%8E%E5%BA%8F%E9%81%8D%E5%8E%86%E5%BA%8F%E5%88%97/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 给定一个数组和滑动窗口的大小，找出所有滑动窗口里数值的最大值。例如，如果输入数组{2,3,4,2,6,2,5,1}及滑动窗口的大小3，那么一共存在6个滑动窗口，他们的最大值分别为{4,4,6,6,6,5}； 针对数组{2,3,4,2,6,2,5,1}的滑动窗口有以下6个： {[2,3,4],2,6,2,5,1}， {2,[3,4,2],6,2,5,1}， {2,3,[4,2,6],2,5,1}， {2,3,4,[2,6,2],5,1}， {2,3,4,2,[6,2,5],1}， {2,3,4,2,6,[2,5,1]}。 ","date":"2019-04-11","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之滑动窗口的最大值","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n^2)$, 空间复杂度: $O(n)$. class Solution { public: vector\u003cint\u003e maxInWindows(const vector\u003cint\u003e\u0026 num, unsigned int size) { vector\u003cint\u003e vec; if(size == 0 || size \u003e num.size()) return vec; int s = num.size(); int m = s - size + 1; for(int i = 0; i \u003c num.size(); i++) { int temp = num[i]; int j = i; while( (j \u003c size + i) \u0026\u0026 m) { if(temp \u003c num[j]) temp = num[j]; j++; } m--; vec.push_back(temp); if(!m) break; } return vec; } }; ","date":"2019-04-11","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之滑动窗口的最大值","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E7%9A%84%E6%9C%80%E5%A4%A7%E5%80%BC/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 给定一棵二叉搜索树，请找出其中的第k小的结点。例如, （5，3，7，2，4，6，8） 中，按结点数值大小顺序第三小结点的值为4。 ","date":"2019-04-11","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉搜索树的第k个结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 按照树的中序遍历，然后找到第k小的结点 时间复杂度：$O(n)$, 空间复杂度$O(1)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } }; */ class Solution { public: int count = 0; TreeNode* KthNode(TreeNode* pRoot, int k) { if(!pRoot) return NULL; if(pRoot) { TreeNode *p = KthNode(pRoot-\u003eleft,k); if(p) return p; count++; if(count == k) return pRoot; p = KthNode(pRoot-\u003eright,k); if(p) return p; } return NULL; } }; ","date":"2019-04-11","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉搜索树的第k个结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E7%9A%84%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述: LL今天心情特别好,因为他去买了一副扑克牌,发现里面居然有2个大王,2个小王(一副牌原本是54张^_^)…他随机从中抽出了5张牌,想测测自己的手气,看看能不能抽到顺子,如果抽到的话,他决定去买体育彩票,嘿嘿！！“红心A,黑桃3,小王,大王,方片5”,“Oh My God!”不是顺子…..LL不高兴了,他想了想,决定大\\小 王可以看成任何数字,并且A看作1,J为11,Q为12,K为13。上面的5张牌就可以变成“1,2,3,4,5”(大小王分别看作2和4),“So Lucky!”。LL决定去买体育彩票啦。 现在,要求你使用这幅牌模拟上面的过程,然后告诉我们LL的运气如何， 如果牌能组成顺子就输出true，否则就输出false。为了方便起见,你可以认为大小王是0。 ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%89%91%E5%85%8B%E7%89%8C%E9%A1%BA%E5%AD%90/:0:1","series":null,"tags":["剑指Offer","there"],"title":"剑指Offer之扑克牌顺子","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%89%91%E5%85%8B%E7%89%8C%E9%A1%BA%E5%AD%90/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 先统计王的数量，再把牌排序，如果后面一个数比前面一个数大于1以上，那么中间的差值就必须用王来补了。看王的数量够不够，如果够就返回true，否则返回false。 时间复杂度：$O(nlogn)$, 空间复杂度:$O(1)$. class Solution { public: bool IsContinuous( vector\u003cint\u003e numbers ) { if(numbers.size() == 0 || numbers.size() \u003c 5) return false; sort(numbers.begin(), numbers.end()); int z = 0; int count = 0; for(int i = 0; i \u003c 5; i++) { if(numbers[i] == 0) z++; else { if(numbers[i + 1] - numbers[i] == 0) return false; // 统计之前的差值，相差几个数，就要填几个0， if(numbers[i + 1] - numbers[i] \u003e 1) count += numbers[i + 1] - numbers[i] - 1; } } if(count \u003e z) return false; return true; } }; ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%89%91%E5%85%8B%E7%89%8C%E9%A1%BA%E5%AD%90/:0:2","series":null,"tags":["剑指Offer","there"],"title":"剑指Offer之扑克牌顺子","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%89%91%E5%85%8B%E7%89%8C%E9%A1%BA%E5%AD%90/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之调整数组顺序使奇数位于偶数前面","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { public: void reOrderArray(vector\u003cint\u003e \u0026array) { vector\u003cint\u003e vec1; vector\u003cint\u003e vec2; for(int i = 0; i \u003c array.size(); i++) { if(array[i] % 2 == 0) vec2.push_back(array[i]); else vec1.push_back(array[i]); } for(int j = 0; j \u003c vec1.size(); j++) array[j] = vec1[j]; for(int z = 0; z \u003c vec2.size(); z++) array[ vec1.size()+ z] = vec2[z]; } }; ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之调整数组顺序使奇数位于偶数前面","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { public: void reOrderArray(vector\u003cint\u003e \u0026array) { vector\u003cint\u003e vec; for(int i = 0; i \u003c array.size(); i++) { if(array[i] % 2 != 0) vec.push_back(array[i]); } for(int j = 0; j \u003c array.size(); j++) { if(array[j] % 2 == 0) vec.push_back(array[j]); } array = vec; } }; ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2/:0:3","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之调整数组顺序使奇数位于偶数前面","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B0%83%E6%95%B4%E6%95%B0%E7%BB%84%E9%A1%BA%E5%BA%8F%E4%BD%BF%E5%A5%87%E6%95%B0%E4%BD%8D%E4%BA%8E%E5%81%B6%E6%95%B0%E5%89%8D%E9%9D%A2/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 如何得到一个数据流中的中位数？如果从数据流中读出奇数个数值，那么中位数就是所有数值排序之后位于中间的数值。如果从数据流中读出偶数个数值，那么中位数就是所有数值排序之后中间两个数的平均值。我们使用Insert()方法读取数据流，使用GetMedian()方法获取当前读取数据的中位数。 ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之数据流中的中位数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： class Solution { public: vector\u003cint\u003e vec; void Insert(int num) { vec.push_back(num); sort(vec.begin(), vec.end()); } double GetMedian() { int s = vec.size(); double d = 0; if(s % 2 == 0) d = (vec[s / 2 - 1] + vec[s / 2]) / 2.0; else d = vec[s / 2]; return d; } }; ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之数据流中的中位数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%AD%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 在一个字符串(0\u003c=字符串长度\u003c=10000，全部由字母组成)中找到第一个只出现一次的字符,并返回它的位置, 如果没有则返回 -1（需要区分大小写）. ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之第一个只出现一次的字符","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { public: int FirstNotRepeatingChar(string str) { map\u003cchar, int\u003e map; for(int i = 0; i \u003c str.length(); i++) { if(!map[str[i]]) map[str[i]] = 1; else map[str[i]]++; } for(int i = 0; i \u003c str.length(); i++) { if(map[str[i]] == 1) return i; } return -1; } }; ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6/:0:2","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之第一个只出现一次的字符","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： find()的应用 （rfind() 类似，只是从反向查找） 找到 – 返回 第一个字符的索引 没找到–返回 string::npos 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: int FirstNotRepeatingChar(string str) { for(size_t i = 0; i \u003c str.length(); i++) { if(str.find(str[i]) == str.rfind(str[i])) return i; } return -1; } }; ","date":"2019-04-10","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6/:0:3","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之第一个只出现一次的字符","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E5%AD%97%E7%AC%A6/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一颗二叉树的跟节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%92%8C%E4%B8%BA%E6%9F%90%E4%B8%80%E5%80%BC%E7%9A%84%E8%B7%AF%E5%BE%84/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树中和为某一值的路径","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%92%8C%E4%B8%BA%E6%9F%90%E4%B8%80%E5%80%BC%E7%9A%84%E8%B7%AF%E5%BE%84/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度：$O(n)$, 空间复杂度$O(n)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } };*/ class Solution { public: // 定义全局变量 vector\u003cvector\u003cint\u003e \u003e vec; vector\u003cint\u003e temp; vector\u003cvector\u003cint\u003e \u003e FindPath(TreeNode* root,int expectNumber) { // 如果root为NULL if(!root) return vec; // 将root的数据放入到temp里面 temp.push_back(root -\u003e val); // target减去root的值 expectNumber -= root -\u003e val; // 如果减去的数为0并且是叶子节点，那么将temp放入发到vec if(expectNumber == 0 \u0026\u0026 root-\u003eleft == NULL \u0026\u0026 root-\u003eright == NULL) { vec.push_back(temp); } // 递归计算左子树 FindPath(root-\u003eleft,expectNumber); // 递归计算右子树 FindPath(root-\u003eright,expectNumber); // 回溯，弹出本层的数据 temp.pop_back(); return vec; } }; ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%92%8C%E4%B8%BA%E6%9F%90%E4%B8%80%E5%80%BC%E7%9A%84%E8%B7%AF%E5%BE%84/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树中和为某一值的路径","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%AD%E5%92%8C%E4%B8%BA%E6%9F%90%E4%B8%80%E5%80%BC%E7%9A%84%E8%B7%AF%E5%BE%84/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述：将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 输入描述:输入一个字符串,包括数字字母符号,可以为空 输出描述: 如果是合法的数值表达则返回该数字，否则返回0 示例1 输入 +2147483647 1a33 输出 2147483647 0 ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之把字符串转换成整数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 题目描述：将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 输入描述:输入一个字符串,包括数字字母符号,可以为空 输出描述: 如果是合法的数值表达则返回该数字，否则返回0 示例1 输入 +2147483647 1a33 输出 2147483647 0 ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之把字符串转换成整数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/#输入描述"},{"categories":["剑指Offer"],"content":" 题目描述：将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 输入描述:输入一个字符串,包括数字字母符号,可以为空 输出描述: 如果是合法的数值表达则返回该数字，否则返回0 示例1 输入 +2147483647 1a33 输出 2147483647 0 ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之把字符串转换成整数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/#输出描述"},{"categories":["剑指Offer"],"content":" 题目描述：将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 输入描述:输入一个字符串,包括数字字母符号,可以为空 输出描述: 如果是合法的数值表达则返回该数字，否则返回0 示例1 输入 +2147483647 1a33 输出 2147483647 0 ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之把字符串转换成整数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/#输入"},{"categories":["剑指Offer"],"content":" 题目描述：将一个字符串转换成一个整数(实现Integer.valueOf(string)的功能，但是string不符合数字要求时返回0)，要求不能使用字符串转换整数的库函数。 数值为0或者字符串不是一个合法的数值则返回0。 输入描述:输入一个字符串,包括数字字母符号,可以为空 输出描述: 如果是合法的数值表达则返回该数字，否则返回0 示例1 输入 +2147483647 1a33 输出 2147483647 0 ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之把字符串转换成整数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/#输出"},{"categories":["剑指Offer"],"content":" 解题思路：时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: int StrToInt(string str) { if(str.length() == 0) return 0; int temp = 1; int n = 0; // 处理正负号 if(str[0] == '+') n = 1; if(str[0] == '-') { temp = -1; n = 1; } // 按照每取一位相乘 int sum = 0; for(int i = n; i \u003c str.length(); i++) { if( str[i] \u003c '0' || str[i] '9') return 0; sum = sum * 10 + str[i] - '0'; } return temp * sum; } }; ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之把字符串转换成整数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%88%90%E6%95%B4%E6%95%B0/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 请实现一个函数用来判断字符串是否表示数值（包括整数和小数）。例如，字符串\"+100\",“5e2”,\"-123\",“3.1416\"和”-1E-16\"都表示数值。 但是\"12e\",“1a3.14”,“1.2.3”,\"+-5\"和\"12e+4.3\"都不是。 ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之表示数值的字符串","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 几个关键点： 1.基本边界 string == NULL || *string == ‘\\0’ 2.检测是否有符号位(检测第一位) 3.检测除符号位外的第一个有效位，有效位只能是数字或者小数点. 4.检测是否有E或者e，且不能重复出现 5.小数点不能重复出现 ，E或者e后面不能出现小数点 6.中间的符号位必须出现在E或者e后面 7.数字的合法性，不能是其他字母如‘a’等 设置参数hasPoint、hasEe来判断E、e、小数点是否重复出现 时间复杂度：$O(n), $空间复杂度：$O(1)$. class Solution { public: bool isNumeric(char* string) { bool hasPoint = false; bool hasEe = false; // 基本边界 if(string == NULL || *string == '\\0') return false; // 检测是否有符号位 bool isMinus = false; if(*string == '-') { isMinus = true; string++; } else if(*string == '+') { isMinus = false; string++; } // 检测第一个数字或者小数点是否存在 if((*string \u003e= '0' \u0026\u0026 *string \u003c= '9')) string++; if(*string == '.') { hasPoint = true; string++; } while(*string != '\\0') { // 是否为E或者e if(*string == 'E' || *string == 'e') { if(hasEe == true) { return false; } else { hasEe = true; string++; } } // 是否为小数点. if(*string == '.') { if(hasPoint == true) { return false; } else { if(hasEe == true) return false; hasPoint = true; string++; } } // 是否为符号 if(*string == '-' || *string == '+') { if(hasEe == true) string++; else return false; } // 是否为合法数字 else if(*string \u003e= '0' \u0026\u0026 *string \u003c= '9') string++; else return false; } // 如果不是所有不合法，则返回true return true; } }; ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/:0:2","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之表示数值的字符串","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 注意表示数值的字符串遵循的规则； 在数值之前可能有一个“+”或“-”，接下来是0到9的数位表示数值的整数部分，如果数值是一个小数，那么小数点后面可能会有若干个0到9的数位 表示数值的小数部分。如果用科学计数法表示，接下来是一个‘e’或者‘E’，以及紧跟着一个整数（可以有正负号）表示指数。 时间复杂度：$O(n)$,空间复杂度：$O(1)$. bool isNumeric(char* string) { if(string==NULL) return false; if(*string=='+'||*string=='-') string++; if(*string=='\\0') return false; int dot=0,num=0,nume=0;//分别用来标记小数点、整数部分和e指数是否存在 while(*string!='\\0'){ if(*string\u003e='0'\u0026\u0026*string\u003c='9') { string++; num=1; } else if(*string=='.'){ if(dot\u003e0||nume\u003e0) return false; string++; dot=1; } else if(*string=='e'||*string=='E') { if(num==0||nume\u003e0) return false; string++; nume++; if(*string=='+'||*string=='-') string++; if(*string=='\\0') return false; } else return false; } return true; ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/:0:3","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之表示数值的字符串","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%A1%A8%E7%A4%BA%E6%95%B0%E5%80%BC%E7%9A%84%E5%AD%97%E7%AC%A6%E4%B8%B2/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 从上往下打印出二叉树的每个节点，同层节点从左至右打印。 ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之从上往下打印二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 二叉树的层次遍历么，借助一个队列。 时间复杂度：$O(n)$, 空间复杂度：$O(n)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } };*/ class Solution { public: vector\u003cint\u003e PrintFromTopToBottom(TreeNode* root) { vector\u003cint\u003e vec; queue\u003cTreeNode*\u003e q; if(!root) return vec; q.push(root); while(!q.empty()) { vec.push_back(q.front()-\u003eval); if(q.front()-\u003eleft) q.push(q.front()-\u003eleft); if(q.front()-\u003eright) q.push(q.front()-\u003eright); q.pop(); } return vec; } }; ","date":"2019-04-09","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之从上往下打印二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个正整数数组，把数组里所有数字拼接起来排成一个数，打印能拼接出的所有数字中最小的一个。例如输入数组{3，32，321}，则打印出这三个数字能排成的最小数字为321323。 ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E6%95%B0%E7%BB%84%E6%8E%92%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之把数组排成最小的数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E6%95%B0%E7%BB%84%E6%8E%92%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(nlogn)$, 空间复杂度: $O(n)$. class Solution { public: static bool com(string a,string b) { return (a+b) \u003c (b+a); } string PrintMinNumber(vector\u003cint\u003e numbers) { string s = \"\"; vector\u003cstring\u003e vec; for(int i = 0; i \u003c numbers.size(); i++) { vec.push_back(to_string(numbers[i])); } sort(vec.begin(),vec.end(),com); for(auto str : vec) { s += str; } return s; } }; ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E6%95%B0%E7%BB%84%E6%8E%92%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之把数组排成最小的数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E6%95%B0%E7%BB%84%E6%8E%92%E6%88%90%E6%9C%80%E5%B0%8F%E7%9A%84%E6%95%B0/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。例如输入一个长度为9的数组{1,2,3,2,2,2,5,4,2}。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。如果不存在则输出0。 ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中出现次数超过一半的数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(n^2)$,空间复杂度：$O(1)$. class Solution { public: int MoreThanHalfNum_Solution(vector\u003cint\u003e numbers) { int temp = numbers.size() / 2; if(numbers.size() == 1) return numbers[0]; for(int i = 0; i \u003c numbers.size(); i++) { int count = 0; for(int j = i + 1; j \u003c numbers.size(); j++) { if(numbers[i] == numbers[j]) { count++; if(count \u003e= temp) return numbers[i]; } } } return 0; } }; ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中出现次数超过一半的数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度：$O(n)$,空间复杂度：$O(1)$. class Solution { public: int MoreThanHalfNum_Solution(vector\u003cint\u003e numbers) { int temp = numbers.size() / 2 ; if(numbers.size() == 1) return numbers[0]; map\u003cint,int\u003e map; for(int i = 0; i \u003c numbers.size(); i++) { if(!map[numbers[i]]) map[numbers[i]]=1; else map[numbers[i]]++; if( map[numbers[i]] \u003e temp) return numbers[i]; } return 0; } }; ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97/:0:3","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中出现次数超过一半的数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E6%AC%A1%E6%95%B0%E8%B6%85%E8%BF%87%E4%B8%80%E5%8D%8A%E7%9A%84%E6%95%B0%E5%AD%97/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/:0:1","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之合并两个排序的链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度：$O(n)$,空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: // 合并有序链表 ListNode* Merge(ListNode* pHead1, ListNode* pHead2) { // 如果1为NULL，返回2 if(!pHead1) return pHead2; // 如果2为NULL，返回1 if(!pHead2) return pHead1; ListNode *p = pHead1; ListNode *q = pHead2; // 新建一个头节点 ListNode *temp = new ListNode(0); // 设置新的节点去遍历 ListNode *pN = temp; // 循环遍历两个链表 while(p \u0026\u0026 q) { // 比较大小，小的加载到新链表的后面 if(p-\u003eval \u003c= q-\u003eval) { pN -\u003e next = p; p = p-\u003enext; } else { pN -\u003e next = q; q = q-\u003enext; } pN = pN -\u003e next; } // 如果p不为NULL，将所有的p加载到新链表 if(p) { pN -\u003e next = p; } // 如果q不为NULL，将所有的q加载到新链表 if(q) { pN -\u003e next = q; } return temp -\u003e next; } }; ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/:0:2","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之合并两个排序的链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的双向链表。要求不能创建任何新的结点，只能调整树中结点指针的指向。 ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉搜索树与双向链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 1 .将左子树构造成双链表，并返回链表头节点。 2 .定位至左子树双链表最后一个节点。 3 .如果左子树链表不为空的话，将当前root追加到左子树链表。 4 .将右子树构造成双链表，并返回链表头节点。 5 .如果右子树链表不为空的话，将该链表追加到root节点之后。 6 .根据左子树链表是否为空确定返回的节点。 时间复杂度：$O(n)$, 空间复杂度$O(1)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } };*/ class Solution { public: TreeNode* Convert(TreeNode* pRootOfTree) { //if(!pRootOfTree || !(pRootOfTree-\u003eleft \u0026\u0026 pRootOfTree-\u003eright)) return pRootOfTree; if(!pRootOfTree) return NULL; if(!pRootOfTree-\u003eleft \u0026\u0026 !pRootOfTree-\u003eright) return pRootOfTree; TreeNode *left = NULL; TreeNode *p = NULL; TreeNode *right = NULL; // 1.将左子树构造成双链表，并返回链表头节点 left = Convert(pRootOfTree-\u003eleft); p = left; // 2.定位至左子树双链表最后一个节点 while(p \u0026\u0026 p-\u003eright) { p=p-\u003eright; } // 3.如果左子树链表不为空的话，将当前root追加到左子树链表 if(left) { p-\u003eright = pRootOfTree; pRootOfTree-\u003eleft = p; } // 4.将右子树构造成双链表，并返回链表头节点 right = Convert(pRootOfTree-\u003eright); // 5.如果右子树链表不为空的话，将该链表追加到root节点之后 if(right) { right-\u003eleft = pRootOfTree; pRootOfTree-\u003eright = right; } return left != NULL? left:pRootOfTree; } }; ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉搜索树与双向链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 思路与方法一中的递归版一致，仅对第2点中的定位作了修改，新增一个全局变量记录左子树的最后一个节点。 时间复杂度：$O(n)$, 空间复杂度$O(1)$. // 记录子树链表的最后一个节点，终结点只可能为只含左子树的非叶节点与叶节点 class Solution { protected: TreeNode *leftLast = NULL; public: TreeNode* Convert(TreeNode* pRootOfTree) { if(pRootOfTree==NULL) return NULL; if(pRootOfTree-\u003eleft==NULL\u0026\u0026pRootOfTree-\u003eright==NULL){ leftLast = pRootOfTree;// 最后的一个节点可能为最右侧的叶节点 return pRootOfTree; } // 1.将左子树构造成双链表，并返回链表头节点 TreeNode *left = Convert(pRootOfTree-\u003eleft); // 3.如果左子树链表不为空的话，将当前root追加到左子树链表 if(left!=null){ leftLast-\u003eright = pRootOfTree; pRootOfTree-\u003eleft = leftLast; } leftLast = pRootOfTree;// 当根节点只含左子树时，则该根节点为最后一个节点 // 4.将右子树构造成双链表，并返回链表头节点 TreeNode *right = Convert(pRootOfTree-\u003eright); // 5.如果右子树链表不为空的话，将该链表追加到root节点之后 if(right!=null){ right-\u003eleft = pRootOfTree; pRootOfTree-\u003eright = right; } return left!=null?left:pRootOfTree; } ","date":"2019-04-08","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/:0:3","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉搜索树与双向链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述: 在一个长度为n的数组里的所有数字都在0到n-1的范围内。 数组中某些数字是重复的，但不知道有几个数字是重复的。也不知道每个数字重复几次。请找出数组中任意一个重复的数字。 例如，如果输入长度为7的数组{2,3,1,0,2,5,3}，那么对应的输出是第一个重复的数字2。 ","date":"2019-04-05","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中重复的数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(n^2)$,空间复杂度：$O(1)$. class Solution { public: // Parameters: // numbers: an array of integers // length: the length of array numbers // duplication: (Output) the duplicated number in the array number // Return value: true if the input is valid, and there are some duplications in the array number, otherwise false bool duplicate(int numbers[], int length, int* duplication) { for(int i = 0; i \u003c length; i++) { for(int j = i+1; j \u003c length; j++) { if(numbers[i]==numbers[j]) { duplication[0]=numbers[i]; return true; } } } return false; } }; ","date":"2019-04-05","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中重复的数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： **最简单的方法：**我最直接的想法就是构造一个容量为N的辅助数组B，原数组A中每个数对应B中下标，首次命中，B中对应元素+1。如果某次命中时，B中对应的不为0，说明，前边已经有一样数字了，那它就是重复的了。 举例：A{1,2,3,3,4,5}，刚开始B是**{0,0,0,0,0,0}**，开始扫描A。 A[0] = 1 {0,1,0,0,0,0} A[1] = 2 {0,1,1,0,0,0} A[2] = 3 {0,1,1,1,0,0} A[3] = 3 {0,1,1,2,0,0}，到这一步，就已经找到了重复数字。 A[4] = 4 {0,1,1,2,1,0} A[5] = 5 {0,1,1,2,1,1} 时间复杂度O（n），空间复杂度O（n） class Solution { public: // Parameters: // numbers: an array of integers // length: the length of array numbers // duplication: (Output) the duplicated number in the array number // Return value: true if the input is valid, and there are some duplications in the array number // otherwise false bool duplicate(int numbers[], int length, int* duplication) { if(numbers==NULL||length==0) return 0; int hashTable[255]={0}; for(int i=0;i\u003clength;i++) hashTable[numbers[i]]++; int count=0; for(int i=0;i\u003clength;i++) { if(hashTable[numbers[i]]\u003e1) { duplication[count++]=numbers[i]; //break; return true; } } return false; } }; ","date":"2019-04-05","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/:0:3","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中重复的数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E9%87%8D%E5%A4%8D%E7%9A%84%E6%95%B0%E5%AD%97/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） ","date":"2019-04-04","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%A0%88%E7%9A%84%E5%8E%8B%E5%85%A5%E5%BC%B9%E5%87%BA%E5%BA%8F%E5%88%97/:0:1","series":null,"tags":["剑指Offer","stack"],"title":"剑指Offer之栈的压入、弹出序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%A0%88%E7%9A%84%E5%8E%8B%E5%85%A5%E5%BC%B9%E5%87%BA%E5%BA%8F%E5%88%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 借用一个辅助的栈，遍历压栈顺序，先讲第一个放入栈中，这里是1，然后判断栈顶元素是不是出栈顺序的第一个元素，这里是4，很显然1≠4，所以我们继续压栈，直到相等以后开始出栈，出栈一个元素，则将出栈顺序向后移动一位，直到不相等，这样循环等压栈顺序遍历完成，如果辅助栈还不为空，说明弹出序列不是该栈的弹出顺序。 举例： 入栈1,2,3,4,5 出栈4,5,3,2,1 首先1入辅助栈，此时栈顶1≠4，继续入栈2 此时栈顶2≠4，继续入栈3 此时栈顶3≠4，继续入栈4 此时栈顶4＝4，出栈4，弹出序列向后一位，此时为5，,辅助栈里面是1,2,3 此时栈顶3≠5，继续入栈5 此时栈顶5=5，出栈5,弹出序列向后一位，此时为3，,辅助栈里面是1,2,3 …. 依次执行，最后辅助栈为空。如果不为空说明弹出序列不是该栈的弹出顺序。 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { public: bool IsPopOrder(vector\u003cint\u003e pushV,vector\u003cint\u003e popV) { if(pushV.size() == 0 || popV.size() == 0) return false; stack\u003cint\u003e s; int i = 0; // pushV int j = 0; // popV while(i \u003c pushV.size()) { s.push(pushV[i++]); while(!s.empty() \u0026\u0026 s.top() == popV[j]) { j++; s.pop(); } } return s.empty(); } }; ","date":"2019-04-04","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%A0%88%E7%9A%84%E5%8E%8B%E5%85%A5%E5%BC%B9%E5%87%BA%E5%BA%8F%E5%88%97/:0:2","series":null,"tags":["剑指Offer","stack"],"title":"剑指Offer之栈的压入、弹出序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%A0%88%E7%9A%84%E5%8E%8B%E5%85%A5%E5%BC%B9%E5%87%BA%E5%BA%8F%E5%88%97/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个链表，反转链表后，输出新链表的表头。 ","date":"2019-04-04","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/:0:1","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之反转链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(n)$,空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* ReverseList(ListNode* pHead) { if(!pHead || !pHead-\u003enext) return pHead; ListNode *p = pHead; pHead = pHead-\u003enext; p-\u003enext = NULL; while(pHead-\u003enext) { ListNode *q = pHead; pHead = pHead-\u003enext; q-\u003enext = p; p = q; } pHead-\u003enext = p; return pHead; } }; ","date":"2019-04-04","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/:0:2","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之反转链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 递归方法: 递归的方法其实是非常巧的，它利用递归走到链表的末端，然后再更新每一个node的next 值 ，实现链表的反转。而newhead 的值没有发生改变，为该链表的最后一个结点，所以，反转后，我们可以得到新链表的head。 注意关于链表问题的常见注意点的思考： 1、如果输入的头结点是 NULL，或者整个链表只有一个结点的时候 2、链表断裂的考虑 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* ReverseList(ListNode* pHead) { //如果链表为空或者链表中只有一个元素 if(pHead==NULL||pHead-\u003enext==NULL) return pHead; //先反转后面的链表，走到链表的末端结点 ListNode* pReverseNode=ReverseList(pHead-\u003enext); //再将当前节点设置为后面节点的后续节点 pHead-\u003enext-\u003enext=pHead; pHead-\u003enext=NULL; return pReverseNode; } }; ","date":"2019-04-04","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/:0:3","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之反转链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 一个整型数组里除了两个数字之外，其他的数字都出现了两次。请写程序找出这两个只出现一次的数字。 ","date":"2019-04-03","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中只出现一次的数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度：$O(n)$,空间复杂度：$O(1)$. class Solution { public: void FindNumsAppearOnce(vector\u003cint\u003e data,int* num1,int *num2) { sort(data.begin(), data.end()); int flag = 0; for(int i = 0; i \u003c data.size(); ) { if(data[i] != data[i + 1]) { if(flag == 0) { *num1 = data[i]; flag = 1; i++; continue; } if(flag == 1) { *num2 = data[i]; break; } } else i = i + 2; } } }; ","date":"2019-04-03","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数组中只出现一次的数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E7%BB%84%E4%B8%AD%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%E7%9A%84%E6%95%B0%E5%AD%97/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 给定一个二叉树和其中的一个结点，请找出中序遍历顺序的下一个结点并且返回。注意，树中的结点不仅包含左右子结点，同时包含指向父结点的指针。 ","date":"2019-04-03","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E4%B8%8B%E4%B8%80%E4%B8%AA%E7%BB%93%E7%82%B9/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树的下一个结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E4%B8%8B%E4%B8%80%E4%B8%AA%E7%BB%93%E7%82%B9/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 1.二叉树为空，则返回空； 2.节点右孩子存在，则设置一个指针从该节点的右孩子出发，一直沿着指向左子结点的指针找到的叶子节点即为下一个节点； 3.节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点；否则继续向上遍历其父节点的父节点，重复之前的判断，返回结果。 时间复杂度：$O(n)$, 空间复杂度$O(1)$. /* struct TreeLinkNode { int val; struct TreeLinkNode *left; struct TreeLinkNode *right; struct TreeLinkNode *next; TreeLinkNode(int x) :val(x), left(NULL), right(NULL), next(NULL) { } }; */ class Solution { public: TreeLinkNode* GetNext(TreeLinkNode* pNode) { // 如果pNode为空，直接返回NULL if(!pNode) { return NULL; } // 2.节点右孩子存在，则设置一个指针从该节点的右孩子出发，一直沿着指向左子结点的指针找到的叶子节点即为下一个节点 if(pNode -\u003e right) { pNode = pNode -\u003e right; while(pNode -\u003e left) { pNode = pNode -\u003e left; } return pNode; } // 3.节点不是根节点。如果该节点是其父节点的左孩子，则返回父节点；否则继续向上遍历其父节点的父节点，重复之前的判断，返回结果。 while(pNode -\u003e next) { TreeLinkNode *proot = pNode -\u003e next; if(proot -\u003e left == pNode) { return proot; } pNode = pNode -\u003e next; } return NULL; } }; ","date":"2019-04-03","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E4%B8%8B%E4%B8%80%E4%B8%AA%E7%BB%93%E7%82%B9/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树的下一个结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E4%B8%8B%E4%B8%80%E4%B8%AA%E7%BB%93%E7%82%B9/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。 n\u003c=39 ","date":"2019-04-03","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之斐波那契数列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 斐波那契数列（Fibonacci sequence），又称黄金分割数列、因数学家列昂纳多·斐波那契（Leonardoda Fibonacci）以兔子繁殖为例子而引入，故又称为“兔子数列”，指的是这样一个数列：1、1、2、3、5、8、13、21、34、……在数学上，斐波纳契数列以如下被以递推的方法定义：F(1)=1，F(2)=1, F(n)=F(n-1)+F(n-2)（n\u003e=3，n∈N*）, 这个数列从第3项开始，每一项都等于前两项之和。 时间复杂度：$O(n)$, 空间复杂度：$O(n)$. class Solution { public: int Fibonacci(int n) { vector\u003cint\u003e vec(n+1); vec[0] = 0; vec[1] = 1; vec[2] = 1; for(int i = 3; i \u003c= n; i++) vec[i] = vec[i - 1] + vec[i - 2]; return vec[n]; } }; ","date":"2019-04-03","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之斐波那契数列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. class Solution { public: int Fibonacci(int n) { int first = 0; int second = 1; int result = n; for(int i = 2; i\u003c=n; i++){ result = first + second; first = second; second = result; } return result; } }; ","date":"2019-04-03","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/:0:3","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之斐波那契数列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 每年六一儿童节,牛客都会准备一些小礼物去看望孤儿院的小朋友,今年亦是如此。HF作为牛客的资深元老,自然也准备了一些小游戏。其中,有个游戏是这样的:首先,让小朋友们围成一个大圈。然后,他随机指定一个数m,让编号为0的小朋友开始报数。每次喊到m-1的那个小朋友要出列唱首歌,然后可以在礼品箱中任意的挑选礼物,并且不再回到圈中,从他的下一个小朋友开始,继续0…m-1报数….这样下去….直到剩下最后一个小朋友,可以不用表演,并且拿到牛客名贵的“名侦探柯南”典藏版(名额有限哦!!^_^)。请你试着想下,哪个小朋友会得到这份礼品呢？(注：小朋友的编号是从0到n-1) ","date":"2019-04-02","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F-%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之孩子们的游戏(圆圈中最后剩下的数)","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F-%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(n)$,空间复杂度：$O(n)$. class Solution { public: int LastRemaining_Solution(int n, int m) { /* *这道题我用数组来模拟环，思路还是比较简单，但是各种下标要理清 */ if(n\u003c1||m\u003c1) return -1; vector\u003cint\u003e vec(n); int i = -1,step = 0, count = n; while(count\u003e0){ //跳出循环时将最后一个元素也设置为了-1 i++; //指向上一个被删除对象的下一个元素。 if(i\u003e=n) i=0; //模拟环。 if(array[i] == -1) continue; //跳过被删除的对象。 step++; //记录已走过的。 if(step==m) { //找到待删除的对象。 vec[i]=-1; step = 0; count--; } } return i;//返回跳出循环时的i,即最后一个被设置为-1的元素 } }; ","date":"2019-04-02","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F-%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之孩子们的游戏(圆圈中最后剩下的数)","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F-%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 利用数学分析得到的解法 时间复杂度：$O(n)$,空间复杂度：$O(1)$. class Solution { public: int LastRemaining_Solution(int n, int m) { //f(n,m)=[f(n-1,m)+m]%n，其中f(n,m)为长度为n的删除第m个节点，最后剩下的数字 if(n\u003c=0||m\u003c=0) return -1; int last=0; for(int i=2;i\u003c=n;i++) last=(last+m)%i; return last; } }; ","date":"2019-04-02","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F-%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0/:0:3","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之孩子们的游戏(圆圈中最后剩下的数)","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%A9%E5%AD%90%E4%BB%AC%E7%9A%84%E6%B8%B8%E6%88%8F-%E5%9C%86%E5%9C%88%E4%B8%AD%E6%9C%80%E5%90%8E%E5%89%A9%E4%B8%8B%E7%9A%84%E6%95%B0/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 统计一个数字在排序数组中出现的次数。 ","date":"2019-04-02","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数字在排序数组中出现的次数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度:$O(n)$, 空间复杂度：$O(1)$. class Solution { public: int GetNumberOfK(vector\u003cint\u003e data ,int k) { int temp = 0; if(data.size() == 0 || k \u003c data[0] || k \u003e data[data.size() - 1]) return 0; for(int i = 0; i \u003c data.size(); i++) { if(k == data[i]) temp++; } return temp; } }; ","date":"2019-04-02","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数字在排序数组中出现的次数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 观察数组本身的特性可以发现，排序数组这样做没有充分利用数组的特性，可以使用二分查找，找出数据，然后进行左右进行统计 具体算法设计： 二分查找找到k的所在位置,在原数组里面分别左右对k的出现次数进行统计 class Solution { public: int BinarySearch(vector\u003cint\u003e data, int low, int high, int k) { while (low\u003c=high) { int m = (high + low) / 2; if (data[m] == k)return m; else if (data[m] \u003c k) low = m+ 1; else high = m - 1; } return -1; } int GetNumberOfK(vector\u003cint\u003e data ,int k) { if(data.size()==0)return 0; int len=data.size(); int KeyIndex=0; KeyIndex=BinarySearch(data,0,len-1,k); if(KeyIndex==-1) return 0; int sumber=1; int m=KeyIndex-1; int n=KeyIndex+1; while(m\u003e=0\u0026\u0026data[m]==k) { m--; sumber++; } while(n\u003clen\u0026\u0026data[n]==k) { n++; sumber++; } return sumber; } }; ","date":"2019-04-02","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/:0:3","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之数字在排序数组中出现的次数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%AD%97%E5%9C%A8%E6%8E%92%E5%BA%8F%E6%95%B0%E7%BB%84%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 从上到下按层打印二叉树，同一层结点从左至右输出。每一层输出一行。 ","date":"2019-04-01","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0%E6%88%90%E5%A4%9A%E8%A1%8C/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之把二叉树打印成多行","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0%E6%88%90%E5%A4%9A%E8%A1%8C/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 使用队列 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } }; */ class Solution { public: vector\u003cvector\u003cint\u003e \u003e Print(TreeNode* pRoot) { vector\u003cvector\u003cint\u003e \u003e vec; queue\u003cTreeNode *\u003e q; if(!pRoot) return vec; q.push(pRoot); while(!q.empty()) { //这里是重点，要计算队列的长队，好能一行一行的放进去 int qsize = q.size(); vector\u003cint\u003e temp; for(int i = 0; i \u003c qsize;i++) { TreeNode* t = q.front(); if(t -\u003e left)q.pu.sh(t-\u003eleft); if(t -\u003e right)q.push(t-\u003eright); temp.push_back(t -\u003e val); q.pop(); } vec.push_back(temp); } return vec; } }; ","date":"2019-04-01","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0%E6%88%90%E5%A4%9A%E8%A1%8C/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之把二叉树打印成多行","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%8A%8A%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0%E6%88%90%E5%A4%9A%E8%A1%8C/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 请实现一个函数，用来判断一颗二叉树是不是对称的。注意，如果一个二叉树同此二叉树的镜像是同样的，定义其为对称的。 ","date":"2019-04-01","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AF%B9%E7%A7%B0%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之对称的二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AF%B9%E7%A7%B0%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } }; */ class Solution { public: // 递归的判断是否是对成树 bool isSymmetrical(TreeNode* pRoot) { //如果为空，返回true if(pRoot == NULL) return true; // 递归判断子树 return isbt(pRoot-\u003eleft,pRoot-\u003eright); } // 递归判断对称树 bool isbt(TreeNode* left, TreeNode* right) { // 如果左右子树都为NULL，则返回true if(!left \u0026\u0026 !right) return true; // 如果左右子树有一个为NULL，返回false if(!left || !right) return false; // 如果左子树==右子树 if(left-\u003eval == right-\u003eval) // 递归的判断左子树的左子树与右子树的右子树 // 以及左子树的右子树和右子树的左子树是否对称 return isbt(left-\u003eleft, right-\u003eright) \u0026\u0026 isbt(left-\u003eright, right-\u003eleft); return false; } }; ","date":"2019-04-01","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AF%B9%E7%A7%B0%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之对称的二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AF%B9%E7%A7%B0%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91/#解题思路"},{"categories":["机器学习"],"content":"感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。因此，本文作为复习笔记，记录感知机的理论知识。 感知机模型感知机的思想很简单，对于二元分类问题，感知机的模型就是尝试找到一条直线，能够把两个分类隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果能够找到分界线或者是分界面，那么称数据是线性可分的，如果找不到分界面或者分界线，那么称数据是线性不可分的。也就意味着感知机模型不适合你的数据的分类。使用感知机一个最大的前提，就是数据是线性可分的，这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。 用数学公式来表达，假设我们有m个样本，每个样本都是n维，这些样本属于二元分类： $$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m) \\tag{1}$$ 我们的目标是学习一个映射函数（模型）： $$f(x) = sign(w \\cdot x +b) \\tag{2}$$ 这个函数就叫做感知机，其中，w和b是感知机模型的参数，w叫做权重，b叫做偏置。w·x 表示权重与样本特征的内积，sign是符号函数：$$sign(x)= \\begin{cases} -1\u0026 {x\u003c0}\\ 1\u0026 {x\\geq 0} \\end{cases}$$ 损失函数上面已经定义好了感知机模型，只要确定好了模型的参数w和b,那么模型也就确定了，确定模型参数w和b，需要确定一个学习策略，即定义损失函数并且根据优化方法来最小化损失函数。 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数，对于参数w和b而言，不是连续可导的函数，不易优化，因此，损失函数的另一个选择是误分类点到分界面的总距离： $$L(w,b) = -\\frac{1}{||w||} \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) = \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) \\tag{3}$$ 分子和分母都含有w，当分子的w扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，忽略分母。 优化方法损失函数的优化方法有很多，比如：牛顿法、拟牛顿法、梯度下降法等等。感知机模型损失函数的优化方法采用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 损失函数对于w向量的的偏导数为：\\frac{\\partial}{\\partial w}J(w) = - \\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 损失函数对于w向量的梯度更新公式为：w = w + \\alpha\\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的w向量的梯度下降迭代公式为：$w = w + \\alpha y^{(i)} x^{(i)}$ 其中α为步长也可以称之为学习率，$y^{(i)}$为样本输出1或者-1，$x^{(i)}$为(n+1)x1的向量。 算法流程假设我们有如下的m个样本，$$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m)$$ 每个样本有n个特征，每个样本对应的输出值为：-1或1. 算法的执行步骤如下： 定义所有$x_0$为1。选择θ向量的初值和 步长α的初值。可以将θ向量置为0向量，步长设置为1。要注意的是，由于感知机的解不唯一，使用的这两个初值会影响θ向量的最终迭代结果。 在训练集里面选择一个误分类的点$(x_1^{(i)}, x_2^{(i)}, …x_n^{(i)}, y_i)$, 用向量表示即$(x^{(i)}, y^{(i)})$,这个点应该满足：$y^{(i)}\\theta \\bullet x^{(i)} \\leq 0$ 对θ向量进行一次随机梯度下降的迭代：$w = w + \\alpha y^{(i)} x^{(i)}$ 检查训练集里是否还有误分类的点，如果没有，算法结束，此时的θ向量即为最终结果。如果有，继续第2步。 算法对偶形式 参考知乎 算法实现 # 数据线性可分，二分类数据 # 此处为一元一次线性方程 class myModel: def __init__(self,lr = 0.001): self.lr = lr def sign(self, x, w, b): y = np.dot(x, w) + b return y # 随机梯度下降法 def fit(self, X_train, y_train): X_train = np.mat(X_train) m,n = np.shape(X_train) self.w = np.ones((n,1)) self.b = np.zeros(1) is_wrong = False while not is_wrong: wrong_count = 0 for d in range(m): X = X_train[d] y = y_train[d] if y * self.sign(X, self.w, self.b) \u003c= 0: # 注意shape要匹配 self.w = self.w + self.lr * np.dot(X.T,y) self.b = self.b + self.lr*y wrong_count += 1 if wrong_count == 0: is_wrong = True return 'Perceptron Model!' def predict(self): pass def score(self): pass perceptron = myModel(0.01) perceptron.fit(X, y) perceptron.w,perceptron.b # (matrix([[ 0.237], # [-0.249]]), array([-0.26])) 算法小结感知机算法是一个简单易懂的算法，自己编程实现也不太难。它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。 ","date":"2019-04-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之感知机","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/#"},{"categories":["机器学习"],"content":"感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。因此，本文作为复习笔记，记录感知机的理论知识。 感知机模型感知机的思想很简单，对于二元分类问题，感知机的模型就是尝试找到一条直线，能够把两个分类隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果能够找到分界线或者是分界面，那么称数据是线性可分的，如果找不到分界面或者分界线，那么称数据是线性不可分的。也就意味着感知机模型不适合你的数据的分类。使用感知机一个最大的前提，就是数据是线性可分的，这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。 用数学公式来表达，假设我们有m个样本，每个样本都是n维，这些样本属于二元分类： $$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m) \\tag{1}$$ 我们的目标是学习一个映射函数（模型）： $$f(x) = sign(w \\cdot x +b) \\tag{2}$$ 这个函数就叫做感知机，其中，w和b是感知机模型的参数，w叫做权重，b叫做偏置。w·x 表示权重与样本特征的内积，sign是符号函数：$$sign(x)= \\begin{cases} -1\u0026 {x\u003c0}\\ 1\u0026 {x\\geq 0} \\end{cases}$$ 损失函数上面已经定义好了感知机模型，只要确定好了模型的参数w和b,那么模型也就确定了，确定模型参数w和b，需要确定一个学习策略，即定义损失函数并且根据优化方法来最小化损失函数。 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数，对于参数w和b而言，不是连续可导的函数，不易优化，因此，损失函数的另一个选择是误分类点到分界面的总距离： $$L(w,b) = -\\frac{1}{||w||} \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) = \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) \\tag{3}$$ 分子和分母都含有w，当分子的w扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，忽略分母。 优化方法损失函数的优化方法有很多，比如：牛顿法、拟牛顿法、梯度下降法等等。感知机模型损失函数的优化方法采用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 损失函数对于w向量的的偏导数为：\\frac{\\partial}{\\partial w}J(w) = - \\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 损失函数对于w向量的梯度更新公式为：w = w + \\alpha\\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的w向量的梯度下降迭代公式为：$w = w + \\alpha y^{(i)} x^{(i)}$ 其中α为步长也可以称之为学习率，$y^{(i)}$为样本输出1或者-1，$x^{(i)}$为(n+1)x1的向量。 算法流程假设我们有如下的m个样本，$$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m)$$ 每个样本有n个特征，每个样本对应的输出值为：-1或1. 算法的执行步骤如下： 定义所有$x_0$为1。选择θ向量的初值和 步长α的初值。可以将θ向量置为0向量，步长设置为1。要注意的是，由于感知机的解不唯一，使用的这两个初值会影响θ向量的最终迭代结果。 在训练集里面选择一个误分类的点$(x_1^{(i)}, x_2^{(i)}, …x_n^{(i)}, y_i)$, 用向量表示即$(x^{(i)}, y^{(i)})$,这个点应该满足：$y^{(i)}\\theta \\bullet x^{(i)} \\leq 0$ 对θ向量进行一次随机梯度下降的迭代：$w = w + \\alpha y^{(i)} x^{(i)}$ 检查训练集里是否还有误分类的点，如果没有，算法结束，此时的θ向量即为最终结果。如果有，继续第2步。 算法对偶形式 参考知乎 算法实现 # 数据线性可分，二分类数据 # 此处为一元一次线性方程 class myModel: def __init__(self,lr = 0.001): self.lr = lr def sign(self, x, w, b): y = np.dot(x, w) + b return y # 随机梯度下降法 def fit(self, X_train, y_train): X_train = np.mat(X_train) m,n = np.shape(X_train) self.w = np.ones((n,1)) self.b = np.zeros(1) is_wrong = False while not is_wrong: wrong_count = 0 for d in range(m): X = X_train[d] y = y_train[d] if y * self.sign(X, self.w, self.b) \u003c= 0: # 注意shape要匹配 self.w = self.w + self.lr * np.dot(X.T,y) self.b = self.b + self.lr*y wrong_count += 1 if wrong_count == 0: is_wrong = True return 'Perceptron Model!' def predict(self): pass def score(self): pass perceptron = myModel(0.01) perceptron.fit(X, y) perceptron.w,perceptron.b # (matrix([[ 0.237], # [-0.249]]), array([-0.26])) 算法小结感知机算法是一个简单易懂的算法，自己编程实现也不太难。它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。 ","date":"2019-04-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之感知机","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/#感知机模型"},{"categories":["机器学习"],"content":"感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。因此，本文作为复习笔记，记录感知机的理论知识。 感知机模型感知机的思想很简单，对于二元分类问题，感知机的模型就是尝试找到一条直线，能够把两个分类隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果能够找到分界线或者是分界面，那么称数据是线性可分的，如果找不到分界面或者分界线，那么称数据是线性不可分的。也就意味着感知机模型不适合你的数据的分类。使用感知机一个最大的前提，就是数据是线性可分的，这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。 用数学公式来表达，假设我们有m个样本，每个样本都是n维，这些样本属于二元分类： $$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m) \\tag{1}$$ 我们的目标是学习一个映射函数（模型）： $$f(x) = sign(w \\cdot x +b) \\tag{2}$$ 这个函数就叫做感知机，其中，w和b是感知机模型的参数，w叫做权重，b叫做偏置。w·x 表示权重与样本特征的内积，sign是符号函数：$$sign(x)= \\begin{cases} -1\u0026 {x\u003c0}\\ 1\u0026 {x\\geq 0} \\end{cases}$$ 损失函数上面已经定义好了感知机模型，只要确定好了模型的参数w和b,那么模型也就确定了，确定模型参数w和b，需要确定一个学习策略，即定义损失函数并且根据优化方法来最小化损失函数。 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数，对于参数w和b而言，不是连续可导的函数，不易优化，因此，损失函数的另一个选择是误分类点到分界面的总距离： $$L(w,b) = -\\frac{1}{||w||} \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) = \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) \\tag{3}$$ 分子和分母都含有w，当分子的w扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，忽略分母。 优化方法损失函数的优化方法有很多，比如：牛顿法、拟牛顿法、梯度下降法等等。感知机模型损失函数的优化方法采用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 损失函数对于w向量的的偏导数为：\\frac{\\partial}{\\partial w}J(w) = - \\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 损失函数对于w向量的梯度更新公式为：w = w + \\alpha\\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的w向量的梯度下降迭代公式为：$w = w + \\alpha y^{(i)} x^{(i)}$ 其中α为步长也可以称之为学习率，$y^{(i)}$为样本输出1或者-1，$x^{(i)}$为(n+1)x1的向量。 算法流程假设我们有如下的m个样本，$$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m)$$ 每个样本有n个特征，每个样本对应的输出值为：-1或1. 算法的执行步骤如下： 定义所有$x_0$为1。选择θ向量的初值和 步长α的初值。可以将θ向量置为0向量，步长设置为1。要注意的是，由于感知机的解不唯一，使用的这两个初值会影响θ向量的最终迭代结果。 在训练集里面选择一个误分类的点$(x_1^{(i)}, x_2^{(i)}, …x_n^{(i)}, y_i)$, 用向量表示即$(x^{(i)}, y^{(i)})$,这个点应该满足：$y^{(i)}\\theta \\bullet x^{(i)} \\leq 0$ 对θ向量进行一次随机梯度下降的迭代：$w = w + \\alpha y^{(i)} x^{(i)}$ 检查训练集里是否还有误分类的点，如果没有，算法结束，此时的θ向量即为最终结果。如果有，继续第2步。 算法对偶形式 参考知乎 算法实现 # 数据线性可分，二分类数据 # 此处为一元一次线性方程 class myModel: def __init__(self,lr = 0.001): self.lr = lr def sign(self, x, w, b): y = np.dot(x, w) + b return y # 随机梯度下降法 def fit(self, X_train, y_train): X_train = np.mat(X_train) m,n = np.shape(X_train) self.w = np.ones((n,1)) self.b = np.zeros(1) is_wrong = False while not is_wrong: wrong_count = 0 for d in range(m): X = X_train[d] y = y_train[d] if y * self.sign(X, self.w, self.b) \u003c= 0: # 注意shape要匹配 self.w = self.w + self.lr * np.dot(X.T,y) self.b = self.b + self.lr*y wrong_count += 1 if wrong_count == 0: is_wrong = True return 'Perceptron Model!' def predict(self): pass def score(self): pass perceptron = myModel(0.01) perceptron.fit(X, y) perceptron.w,perceptron.b # (matrix([[ 0.237], # [-0.249]]), array([-0.26])) 算法小结感知机算法是一个简单易懂的算法，自己编程实现也不太难。它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。 ","date":"2019-04-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之感知机","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/#损失函数"},{"categories":["机器学习"],"content":"感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。因此，本文作为复习笔记，记录感知机的理论知识。 感知机模型感知机的思想很简单，对于二元分类问题，感知机的模型就是尝试找到一条直线，能够把两个分类隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果能够找到分界线或者是分界面，那么称数据是线性可分的，如果找不到分界面或者分界线，那么称数据是线性不可分的。也就意味着感知机模型不适合你的数据的分类。使用感知机一个最大的前提，就是数据是线性可分的，这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。 用数学公式来表达，假设我们有m个样本，每个样本都是n维，这些样本属于二元分类： $$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m) \\tag{1}$$ 我们的目标是学习一个映射函数（模型）： $$f(x) = sign(w \\cdot x +b) \\tag{2}$$ 这个函数就叫做感知机，其中，w和b是感知机模型的参数，w叫做权重，b叫做偏置。w·x 表示权重与样本特征的内积，sign是符号函数：$$sign(x)= \\begin{cases} -1\u0026 {x\u003c0}\\ 1\u0026 {x\\geq 0} \\end{cases}$$ 损失函数上面已经定义好了感知机模型，只要确定好了模型的参数w和b,那么模型也就确定了，确定模型参数w和b，需要确定一个学习策略，即定义损失函数并且根据优化方法来最小化损失函数。 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数，对于参数w和b而言，不是连续可导的函数，不易优化，因此，损失函数的另一个选择是误分类点到分界面的总距离： $$L(w,b) = -\\frac{1}{||w||} \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) = \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) \\tag{3}$$ 分子和分母都含有w，当分子的w扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，忽略分母。 优化方法损失函数的优化方法有很多，比如：牛顿法、拟牛顿法、梯度下降法等等。感知机模型损失函数的优化方法采用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 损失函数对于w向量的的偏导数为：\\frac{\\partial}{\\partial w}J(w) = - \\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 损失函数对于w向量的梯度更新公式为：w = w + \\alpha\\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的w向量的梯度下降迭代公式为：$w = w + \\alpha y^{(i)} x^{(i)}$ 其中α为步长也可以称之为学习率，$y^{(i)}$为样本输出1或者-1，$x^{(i)}$为(n+1)x1的向量。 算法流程假设我们有如下的m个样本，$$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m)$$ 每个样本有n个特征，每个样本对应的输出值为：-1或1. 算法的执行步骤如下： 定义所有$x_0$为1。选择θ向量的初值和 步长α的初值。可以将θ向量置为0向量，步长设置为1。要注意的是，由于感知机的解不唯一，使用的这两个初值会影响θ向量的最终迭代结果。 在训练集里面选择一个误分类的点$(x_1^{(i)}, x_2^{(i)}, …x_n^{(i)}, y_i)$, 用向量表示即$(x^{(i)}, y^{(i)})$,这个点应该满足：$y^{(i)}\\theta \\bullet x^{(i)} \\leq 0$ 对θ向量进行一次随机梯度下降的迭代：$w = w + \\alpha y^{(i)} x^{(i)}$ 检查训练集里是否还有误分类的点，如果没有，算法结束，此时的θ向量即为最终结果。如果有，继续第2步。 算法对偶形式 参考知乎 算法实现 # 数据线性可分，二分类数据 # 此处为一元一次线性方程 class myModel: def __init__(self,lr = 0.001): self.lr = lr def sign(self, x, w, b): y = np.dot(x, w) + b return y # 随机梯度下降法 def fit(self, X_train, y_train): X_train = np.mat(X_train) m,n = np.shape(X_train) self.w = np.ones((n,1)) self.b = np.zeros(1) is_wrong = False while not is_wrong: wrong_count = 0 for d in range(m): X = X_train[d] y = y_train[d] if y * self.sign(X, self.w, self.b) \u003c= 0: # 注意shape要匹配 self.w = self.w + self.lr * np.dot(X.T,y) self.b = self.b + self.lr*y wrong_count += 1 if wrong_count == 0: is_wrong = True return 'Perceptron Model!' def predict(self): pass def score(self): pass perceptron = myModel(0.01) perceptron.fit(X, y) perceptron.w,perceptron.b # (matrix([[ 0.237], # [-0.249]]), array([-0.26])) 算法小结感知机算法是一个简单易懂的算法，自己编程实现也不太难。它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。 ","date":"2019-04-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之感知机","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/#优化方法"},{"categories":["机器学习"],"content":"感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。因此，本文作为复习笔记，记录感知机的理论知识。 感知机模型感知机的思想很简单，对于二元分类问题，感知机的模型就是尝试找到一条直线，能够把两个分类隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果能够找到分界线或者是分界面，那么称数据是线性可分的，如果找不到分界面或者分界线，那么称数据是线性不可分的。也就意味着感知机模型不适合你的数据的分类。使用感知机一个最大的前提，就是数据是线性可分的，这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。 用数学公式来表达，假设我们有m个样本，每个样本都是n维，这些样本属于二元分类： $$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m) \\tag{1}$$ 我们的目标是学习一个映射函数（模型）： $$f(x) = sign(w \\cdot x +b) \\tag{2}$$ 这个函数就叫做感知机，其中，w和b是感知机模型的参数，w叫做权重，b叫做偏置。w·x 表示权重与样本特征的内积，sign是符号函数：$$sign(x)= \\begin{cases} -1\u0026 {x\u003c0}\\ 1\u0026 {x\\geq 0} \\end{cases}$$ 损失函数上面已经定义好了感知机模型，只要确定好了模型的参数w和b,那么模型也就确定了，确定模型参数w和b，需要确定一个学习策略，即定义损失函数并且根据优化方法来最小化损失函数。 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数，对于参数w和b而言，不是连续可导的函数，不易优化，因此，损失函数的另一个选择是误分类点到分界面的总距离： $$L(w,b) = -\\frac{1}{||w||} \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) = \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) \\tag{3}$$ 分子和分母都含有w，当分子的w扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，忽略分母。 优化方法损失函数的优化方法有很多，比如：牛顿法、拟牛顿法、梯度下降法等等。感知机模型损失函数的优化方法采用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 损失函数对于w向量的的偏导数为：\\frac{\\partial}{\\partial w}J(w) = - \\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 损失函数对于w向量的梯度更新公式为：w = w + \\alpha\\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的w向量的梯度下降迭代公式为：$w = w + \\alpha y^{(i)} x^{(i)}$ 其中α为步长也可以称之为学习率，$y^{(i)}$为样本输出1或者-1，$x^{(i)}$为(n+1)x1的向量。 算法流程假设我们有如下的m个样本，$$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m)$$ 每个样本有n个特征，每个样本对应的输出值为：-1或1. 算法的执行步骤如下： 定义所有$x_0$为1。选择θ向量的初值和 步长α的初值。可以将θ向量置为0向量，步长设置为1。要注意的是，由于感知机的解不唯一，使用的这两个初值会影响θ向量的最终迭代结果。 在训练集里面选择一个误分类的点$(x_1^{(i)}, x_2^{(i)}, …x_n^{(i)}, y_i)$, 用向量表示即$(x^{(i)}, y^{(i)})$,这个点应该满足：$y^{(i)}\\theta \\bullet x^{(i)} \\leq 0$ 对θ向量进行一次随机梯度下降的迭代：$w = w + \\alpha y^{(i)} x^{(i)}$ 检查训练集里是否还有误分类的点，如果没有，算法结束，此时的θ向量即为最终结果。如果有，继续第2步。 算法对偶形式 参考知乎 算法实现 # 数据线性可分，二分类数据 # 此处为一元一次线性方程 class myModel: def __init__(self,lr = 0.001): self.lr = lr def sign(self, x, w, b): y = np.dot(x, w) + b return y # 随机梯度下降法 def fit(self, X_train, y_train): X_train = np.mat(X_train) m,n = np.shape(X_train) self.w = np.ones((n,1)) self.b = np.zeros(1) is_wrong = False while not is_wrong: wrong_count = 0 for d in range(m): X = X_train[d] y = y_train[d] if y * self.sign(X, self.w, self.b) \u003c= 0: # 注意shape要匹配 self.w = self.w + self.lr * np.dot(X.T,y) self.b = self.b + self.lr*y wrong_count += 1 if wrong_count == 0: is_wrong = True return 'Perceptron Model!' def predict(self): pass def score(self): pass perceptron = myModel(0.01) perceptron.fit(X, y) perceptron.w,perceptron.b # (matrix([[ 0.237], # [-0.249]]), array([-0.26])) 算法小结感知机算法是一个简单易懂的算法，自己编程实现也不太难。它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。 ","date":"2019-04-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之感知机","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/#算法流程"},{"categories":["机器学习"],"content":"感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。因此，本文作为复习笔记，记录感知机的理论知识。 感知机模型感知机的思想很简单，对于二元分类问题，感知机的模型就是尝试找到一条直线，能够把两个分类隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果能够找到分界线或者是分界面，那么称数据是线性可分的，如果找不到分界面或者分界线，那么称数据是线性不可分的。也就意味着感知机模型不适合你的数据的分类。使用感知机一个最大的前提，就是数据是线性可分的，这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。 用数学公式来表达，假设我们有m个样本，每个样本都是n维，这些样本属于二元分类： $$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m) \\tag{1}$$ 我们的目标是学习一个映射函数（模型）： $$f(x) = sign(w \\cdot x +b) \\tag{2}$$ 这个函数就叫做感知机，其中，w和b是感知机模型的参数，w叫做权重，b叫做偏置。w·x 表示权重与样本特征的内积，sign是符号函数：$$sign(x)= \\begin{cases} -1\u0026 {x\u003c0}\\ 1\u0026 {x\\geq 0} \\end{cases}$$ 损失函数上面已经定义好了感知机模型，只要确定好了模型的参数w和b,那么模型也就确定了，确定模型参数w和b，需要确定一个学习策略，即定义损失函数并且根据优化方法来最小化损失函数。 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数，对于参数w和b而言，不是连续可导的函数，不易优化，因此，损失函数的另一个选择是误分类点到分界面的总距离： $$L(w,b) = -\\frac{1}{||w||} \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) = \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) \\tag{3}$$ 分子和分母都含有w，当分子的w扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，忽略分母。 优化方法损失函数的优化方法有很多，比如：牛顿法、拟牛顿法、梯度下降法等等。感知机模型损失函数的优化方法采用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 损失函数对于w向量的的偏导数为：\\frac{\\partial}{\\partial w}J(w) = - \\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 损失函数对于w向量的梯度更新公式为：w = w + \\alpha\\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的w向量的梯度下降迭代公式为：$w = w + \\alpha y^{(i)} x^{(i)}$ 其中α为步长也可以称之为学习率，$y^{(i)}$为样本输出1或者-1，$x^{(i)}$为(n+1)x1的向量。 算法流程假设我们有如下的m个样本，$$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m)$$ 每个样本有n个特征，每个样本对应的输出值为：-1或1. 算法的执行步骤如下： 定义所有$x_0$为1。选择θ向量的初值和 步长α的初值。可以将θ向量置为0向量，步长设置为1。要注意的是，由于感知机的解不唯一，使用的这两个初值会影响θ向量的最终迭代结果。 在训练集里面选择一个误分类的点$(x_1^{(i)}, x_2^{(i)}, …x_n^{(i)}, y_i)$, 用向量表示即$(x^{(i)}, y^{(i)})$,这个点应该满足：$y^{(i)}\\theta \\bullet x^{(i)} \\leq 0$ 对θ向量进行一次随机梯度下降的迭代：$w = w + \\alpha y^{(i)} x^{(i)}$ 检查训练集里是否还有误分类的点，如果没有，算法结束，此时的θ向量即为最终结果。如果有，继续第2步。 算法对偶形式 参考知乎 算法实现 # 数据线性可分，二分类数据 # 此处为一元一次线性方程 class myModel: def __init__(self,lr = 0.001): self.lr = lr def sign(self, x, w, b): y = np.dot(x, w) + b return y # 随机梯度下降法 def fit(self, X_train, y_train): X_train = np.mat(X_train) m,n = np.shape(X_train) self.w = np.ones((n,1)) self.b = np.zeros(1) is_wrong = False while not is_wrong: wrong_count = 0 for d in range(m): X = X_train[d] y = y_train[d] if y * self.sign(X, self.w, self.b) \u003c= 0: # 注意shape要匹配 self.w = self.w + self.lr * np.dot(X.T,y) self.b = self.b + self.lr*y wrong_count += 1 if wrong_count == 0: is_wrong = True return 'Perceptron Model!' def predict(self): pass def score(self): pass perceptron = myModel(0.01) perceptron.fit(X, y) perceptron.w,perceptron.b # (matrix([[ 0.237], # [-0.249]]), array([-0.26])) 算法小结感知机算法是一个简单易懂的算法，自己编程实现也不太难。它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。 ","date":"2019-04-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之感知机","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/#算法对偶形式"},{"categories":["机器学习"],"content":"感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。因此，本文作为复习笔记，记录感知机的理论知识。 感知机模型感知机的思想很简单，对于二元分类问题，感知机的模型就是尝试找到一条直线，能够把两个分类隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果能够找到分界线或者是分界面，那么称数据是线性可分的，如果找不到分界面或者分界线，那么称数据是线性不可分的。也就意味着感知机模型不适合你的数据的分类。使用感知机一个最大的前提，就是数据是线性可分的，这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。 用数学公式来表达，假设我们有m个样本，每个样本都是n维，这些样本属于二元分类： $$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m) \\tag{1}$$ 我们的目标是学习一个映射函数（模型）： $$f(x) = sign(w \\cdot x +b) \\tag{2}$$ 这个函数就叫做感知机，其中，w和b是感知机模型的参数，w叫做权重，b叫做偏置。w·x 表示权重与样本特征的内积，sign是符号函数：$$sign(x)= \\begin{cases} -1\u0026 {x\u003c0}\\ 1\u0026 {x\\geq 0} \\end{cases}$$ 损失函数上面已经定义好了感知机模型，只要确定好了模型的参数w和b,那么模型也就确定了，确定模型参数w和b，需要确定一个学习策略，即定义损失函数并且根据优化方法来最小化损失函数。 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数，对于参数w和b而言，不是连续可导的函数，不易优化，因此，损失函数的另一个选择是误分类点到分界面的总距离： $$L(w,b) = -\\frac{1}{||w||} \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) = \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) \\tag{3}$$ 分子和分母都含有w，当分子的w扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，忽略分母。 优化方法损失函数的优化方法有很多，比如：牛顿法、拟牛顿法、梯度下降法等等。感知机模型损失函数的优化方法采用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 损失函数对于w向量的的偏导数为：\\frac{\\partial}{\\partial w}J(w) = - \\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 损失函数对于w向量的梯度更新公式为：w = w + \\alpha\\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的w向量的梯度下降迭代公式为：$w = w + \\alpha y^{(i)} x^{(i)}$ 其中α为步长也可以称之为学习率，$y^{(i)}$为样本输出1或者-1，$x^{(i)}$为(n+1)x1的向量。 算法流程假设我们有如下的m个样本，$$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m)$$ 每个样本有n个特征，每个样本对应的输出值为：-1或1. 算法的执行步骤如下： 定义所有$x_0$为1。选择θ向量的初值和 步长α的初值。可以将θ向量置为0向量，步长设置为1。要注意的是，由于感知机的解不唯一，使用的这两个初值会影响θ向量的最终迭代结果。 在训练集里面选择一个误分类的点$(x_1^{(i)}, x_2^{(i)}, …x_n^{(i)}, y_i)$, 用向量表示即$(x^{(i)}, y^{(i)})$,这个点应该满足：$y^{(i)}\\theta \\bullet x^{(i)} \\leq 0$ 对θ向量进行一次随机梯度下降的迭代：$w = w + \\alpha y^{(i)} x^{(i)}$ 检查训练集里是否还有误分类的点，如果没有，算法结束，此时的θ向量即为最终结果。如果有，继续第2步。 算法对偶形式 参考知乎 算法实现 # 数据线性可分，二分类数据 # 此处为一元一次线性方程 class myModel: def __init__(self,lr = 0.001): self.lr = lr def sign(self, x, w, b): y = np.dot(x, w) + b return y # 随机梯度下降法 def fit(self, X_train, y_train): X_train = np.mat(X_train) m,n = np.shape(X_train) self.w = np.ones((n,1)) self.b = np.zeros(1) is_wrong = False while not is_wrong: wrong_count = 0 for d in range(m): X = X_train[d] y = y_train[d] if y * self.sign(X, self.w, self.b) \u003c= 0: # 注意shape要匹配 self.w = self.w + self.lr * np.dot(X.T,y) self.b = self.b + self.lr*y wrong_count += 1 if wrong_count == 0: is_wrong = True return 'Perceptron Model!' def predict(self): pass def score(self): pass perceptron = myModel(0.01) perceptron.fit(X, y) perceptron.w,perceptron.b # (matrix([[ 0.237], # [-0.249]]), array([-0.26])) 算法小结感知机算法是一个简单易懂的算法，自己编程实现也不太难。它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。 ","date":"2019-04-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之感知机","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/#算法实现"},{"categories":["机器学习"],"content":"感知机可以说是最古老的分类方法之一了，在1957年就已经提出。今天看来它的分类模型在大多数时候泛化能力不强，但是它的原理却值得好好研究。因为研究透了感知机模型，学习支持向量机的话会降低不少难度。同时如果研究透了感知机模型，再学习神经网络，深度学习，也是一个很好的起点。因此，本文作为复习笔记，记录感知机的理论知识。 感知机模型感知机的思想很简单，对于二元分类问题，感知机的模型就是尝试找到一条直线，能够把两个分类隔离开。放到三维空间或者更高维的空间，感知机的模型就是尝试找到一个超平面，能够把所有的二元类别隔离开。如果能够找到分界线或者是分界面，那么称数据是线性可分的，如果找不到分界面或者分界线，那么称数据是线性不可分的。也就意味着感知机模型不适合你的数据的分类。使用感知机一个最大的前提，就是数据是线性可分的，这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分。 用数学公式来表达，假设我们有m个样本，每个样本都是n维，这些样本属于二元分类： $$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m) \\tag{1}$$ 我们的目标是学习一个映射函数（模型）： $$f(x) = sign(w \\cdot x +b) \\tag{2}$$ 这个函数就叫做感知机，其中，w和b是感知机模型的参数，w叫做权重，b叫做偏置。w·x 表示权重与样本特征的内积，sign是符号函数：$$sign(x)= \\begin{cases} -1\u0026 {x\u003c0}\\ 1\u0026 {x\\geq 0} \\end{cases}$$ 损失函数上面已经定义好了感知机模型，只要确定好了模型的参数w和b,那么模型也就确定了，确定模型参数w和b，需要确定一个学习策略，即定义损失函数并且根据优化方法来最小化损失函数。 损失函数的一个自然选择是误分类点的总数，但是，这样的损失函数，对于参数w和b而言，不是连续可导的函数，不易优化，因此，损失函数的另一个选择是误分类点到分界面的总距离： $$L(w,b) = -\\frac{1}{||w||} \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) = \\sum_{i = 1} ^ny_i(w\\cdot x_i+b) \\tag{3}$$ 分子和分母都含有w，当分子的w扩大N倍时，分母的L2范数也会扩大N倍。也就是说，分子和分母有固定的倍数关系。那么我们可以固定分子或者分母为1，然后求另一个即分子自己或者分母的倒数的最小化作为损失函数，这样可以简化我们的损失函数。在感知机模型中，我们采用的是保留分子，忽略分母。 优化方法损失函数的优化方法有很多，比如：牛顿法、拟牛顿法、梯度下降法等等。感知机模型损失函数的优化方法采用的是梯度下降法。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）或者小批量梯度下降（MBGD）。 损失函数对于w向量的的偏导数为：\\frac{\\partial}{\\partial w}J(w) = - \\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 损失函数对于w向量的梯度更新公式为：w = w + \\alpha\\sum\\limits_{x_i \\in M}y^{(i)}x^{(i)} 由于我们采用随机梯度下降，所以每次仅仅采用一个误分类的样本来计算梯度，假设采用第i个样本来更新梯度，则简化后的w向量的梯度下降迭代公式为：$w = w + \\alpha y^{(i)} x^{(i)}$ 其中α为步长也可以称之为学习率，$y^{(i)}$为样本输出1或者-1，$x^{(i)}$为(n+1)x1的向量。 算法流程假设我们有如下的m个样本，$$(x_1^{(0)}, x_2^{(0)}, …x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, …x_n^{(1)},y_1), … (x_1^{(m)}, x_2^{(m)}, …x_n^{(m)}, y_m)$$ 每个样本有n个特征，每个样本对应的输出值为：-1或1. 算法的执行步骤如下： 定义所有$x_0$为1。选择θ向量的初值和 步长α的初值。可以将θ向量置为0向量，步长设置为1。要注意的是，由于感知机的解不唯一，使用的这两个初值会影响θ向量的最终迭代结果。 在训练集里面选择一个误分类的点$(x_1^{(i)}, x_2^{(i)}, …x_n^{(i)}, y_i)$, 用向量表示即$(x^{(i)}, y^{(i)})$,这个点应该满足：$y^{(i)}\\theta \\bullet x^{(i)} \\leq 0$ 对θ向量进行一次随机梯度下降的迭代：$w = w + \\alpha y^{(i)} x^{(i)}$ 检查训练集里是否还有误分类的点，如果没有，算法结束，此时的θ向量即为最终结果。如果有，继续第2步。 算法对偶形式 参考知乎 算法实现 # 数据线性可分，二分类数据 # 此处为一元一次线性方程 class myModel: def __init__(self,lr = 0.001): self.lr = lr def sign(self, x, w, b): y = np.dot(x, w) + b return y # 随机梯度下降法 def fit(self, X_train, y_train): X_train = np.mat(X_train) m,n = np.shape(X_train) self.w = np.ones((n,1)) self.b = np.zeros(1) is_wrong = False while not is_wrong: wrong_count = 0 for d in range(m): X = X_train[d] y = y_train[d] if y * self.sign(X, self.w, self.b) \u003c= 0: # 注意shape要匹配 self.w = self.w + self.lr * np.dot(X.T,y) self.b = self.b + self.lr*y wrong_count += 1 if wrong_count == 0: is_wrong = True return 'Perceptron Model!' def predict(self): pass def score(self): pass perceptron = myModel(0.01) perceptron.fit(X, y) perceptron.w,perceptron.b # (matrix([[ 0.237], # [-0.249]]), array([-0.26])) 算法小结感知机算法是一个简单易懂的算法，自己编程实现也不太难。它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。 ","date":"2019-04-01","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之感知机","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA/#算法小结"},{"categories":["剑指Offer"],"content":" 题目描述： 汇编语言中有一种移位指令叫做循环左移（ROL），现在有个简单的任务，就是用字符串模拟这个指令的运算结果。对于一个给定的字符序列S，请你把其循环左移K位后的序列输出。例如，字符序列S=”abcXYZdef”,要求输出循环左移3位后的结果，即“XYZdefabc”。是不是很简单？OK，搞定它！ ","date":"2019-03-30","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B7%A6%E6%97%8B%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之左旋转字符串","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B7%A6%E6%97%8B%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(n)$,空间复杂度：$O(n)$. class Solution { public: string LeftRotateString(string str, int n) { string s1 = \"\"; string s2 = \"\"; for(int i = 0; i \u003c n; i++) { s1 += str[i]; } for(int j = n; j \u003c str.length(); j++) { s2 += str[j]; } return s2 + s1; } }; ","date":"2019-03-30","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B7%A6%E6%97%8B%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2/:0:2","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之左旋转字符串","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B7%A6%E6%97%8B%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 原理：YX = (XTY T)T 时间复杂度：$O(n)$,空间复杂度：$O(1)$. class Solution { public: string LeftRotateString(string str, int n) { int nLength = str.size(); if(!str.empty() \u0026\u0026 n \u003c= nLength) { if(n \u003e= 0 \u0026\u0026 n \u003c= nLength) { int pFirstStart = 0; int pFirstEnd = n - 1; int pSecondStart = n; int pSecondEnd = nLength - 1; // 翻转字符串的前面n个字符 reverse(str, pFirstStart, pFirstEnd); // 翻转字符串的后面部分 reverse(str, pSecondStart, pSecondEnd); // 翻转整个字符串 reverse(str, pFirstStart, pSecondEnd); } } return str; } void reverse(string \u0026str, int begin, int end) { while(begin \u003c end) { char tmp = str[begin]; str[begin] = str[end]; str[end] = tmp; begin++; end--; } } }; ","date":"2019-03-30","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B7%A6%E6%97%8B%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2/:0:3","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之左旋转字符串","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B7%A6%E6%97%8B%E8%BD%AC%E5%AD%97%E7%AC%A6%E4%B8%B2/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 给一个链表，若其中包含环，请找出该链表的环的入口结点，否则，输出null。 ","date":"2019-03-30","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%8E%AF%E7%9A%84%E5%85%A5%E5%8F%A3%E7%BB%93%E7%82%B9/:0:1","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之链表中环的入口结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%8E%AF%E7%9A%84%E5%85%A5%E5%8F%A3%E7%BB%93%E7%82%B9/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 两个指针一个fast、一个slow同时从一个链表的头部出发, fast一次走2步，slow一次走一步，如果该链表有环，两个指针必然在环内相遇,此时只需要把其中的一个指针重新指向链表头部，另一个不变（还在环内），这次两个指针一次走一步，相遇的地方就是入口节点。 时间复杂度：$O(n^2)$, 空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } }; */ class Solution { public: ListNode* EntryNodeOfLoop(ListNode* pHead) { ListNode* p = pHead; ListNode* q = pHead; if(pHead == NULL) return NULL; while(q-\u003enext != NULL \u0026\u0026 q-\u003enext-\u003enext != NULL){ p = p-\u003enext; q = q-\u003enext-\u003enext; if(p == q) { q = pHead; while(p != q) { p = p-\u003enext; q = q-\u003enext; } return p; } } return NULL; } }; 时间复杂度：$O(n^2)$, 空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } }; */ class Solution { public: ListNode* EntryNodeOfLoop(ListNode* pHead) { if (!pHead || pHead-\u003enext == NULL) return NULL; ListNode *h1 = pHead; ListNode *h2 = pHead; ListNode *h3 = pHead; int flag = 0; while(h2) { h1 = h1-\u003enext; h2 = h2-\u003enext-\u003enext; if(h1 == h2) { flag = 1; break; } } while(flag) { if(h1 == h3) return h1; h1 = h1-\u003enext; h3 = h3-\u003enext; } return NULL; } }; ","date":"2019-03-30","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%8E%AF%E7%9A%84%E5%85%A5%E5%8F%A3%E7%BB%93%E7%82%B9/:0:2","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之链表中环的入口结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E9%93%BE%E8%A1%A8%E4%B8%AD%E7%8E%AF%E7%9A%84%E5%85%A5%E5%8F%A3%E7%BB%93%E7%82%B9/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述：请实现一个函数用来找出字符流中第一个只出现一次的字符。例如，当从字符流中只读出前两个字符\"go\"时，第一个只出现一次的字符是\"g\"。当从该字符流中读出前六个字符“google\"时，第一个只出现一次的字符是\"l\"。 输出描述： 如果当前字符流没有存在出现一次的字符，返回#字符。 ","date":"2019-03-29","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E6%B5%81%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%97%E7%AC%A6/:0:1","series":null,"tags":["剑指Offer","queue"],"title":"剑指Offer之字符流中第一个不重复的字符","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E6%B5%81%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%97%E7%AC%A6/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路：时间复杂度：$O(n)$, 空间复杂度：$O(n)$. class Solution { private: queue\u003cchar q; unsigned cnt[128] = {0}; // 保存每个字符出现的次数 public: //Insert one char from stringstream void Insert(char ch) { ++cnt[ch - '\\0']; //计算每个字符出现的次数 q.push(ch); //保存每个字符 } //return the first appearence once char in current stringstream char FirstAppearingOnce() { while(!q.empty() \u0026\u0026 cnt[q.front()] = 2) q.pop(); if(q.empty()) return '#'; return q.front(); } }; ","date":"2019-03-29","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E6%B5%81%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%97%E7%AC%A6/:0:2","series":null,"tags":["剑指Offer","queue"],"title":"剑指Offer之字符流中第一个不重复的字符","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%AD%97%E7%AC%A6%E6%B5%81%E4%B8%AD%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%AD%97%E7%AC%A6/#解题思路"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#监督学习"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#回归"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#线性回归"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#分类"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#梯度下降gd"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#批量梯度下降batch-gd"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#随机梯度下降分类器sgd"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#小批量梯度下降mini-batch-gd"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#支持向量机svm"},{"categories":["机器学习"],"content":" 监督学习： 监督学习任务：回归 (用于预测某个值) 和 分类 (用于预测某个分类) 常见模型：K邻近值算法、线性回归、逻辑回归、支持向量机(SVM)、决策树和随机森林、神经网络 回归： 线性回归：线性模型更一般化的描述指通过计算输入变量的加权和，并加上一个常数偏置项（截距项）来得到一个预测值。 逻辑回归： 线性回归： from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.predict(X_new) 分类： 二分类：SVM、线性分类 多分类：随机森林、朴素贝叶斯 梯度下降（GD）： 梯度下降的整体思路是通过的迭代来逐渐调整参数使得损失函数达到最小值。 假设浓雾下，你迷失在了大山中，你只能感受到自己脚下的坡度。为了最快到达山底，一个最好的方法就是沿着坡度最陡的地方下山。这其实就是梯度下降所做的：它计算误差函数关于参数向量 的局部梯度，同时它沿着梯度下降的方向进行下一次迭代。当梯度值为零的时候，就达到了误差函数最小值 。 具体来说，开始时，需要选定一个随机的 （这个值称为随机初始值），然后逐渐去改进它，每一次变化一小步，每一步都试着降低损失函数（例如：均方差损失函数），直到算法收敛到一个最小值。 在梯度下降中一个重要的参数是步长，超参数学习率的值决定了步长的大小。如果学习率太小，必须经过多次迭代，算法才能收敛，这是非常耗时的。 另一方面，如果学习率太大，你将跳过最低点，到达山谷的另一面，可能下一次的值比上一次还要大。这可能使的算法是发散的，函数值变得越来越大，永远不可能找到一个好的答案。 常见模型: 批量梯度下降（Batch GD）、小批量梯度下降（Mini-batch GD）、随机梯度下降（Stochastic GD） 批量梯度下降(Batch GD): 批量梯度下降的最要问题是计算每一步的梯度时都需要使用整个训练集，这导致在规模较大的数据集上，其会变得非常的慢。与其完全相反的随机梯度下降，在每一步的梯度计算上只随机选取训练集中的一个样本。很明显，由于每一次的操作都使用了非常少的数据，这样使得算法变得非常快。由于每一次迭代，只需要在内存中有一个实例，这使随机梯度算法可以在大规模训练集上使用。 随机梯度下降分类器(SGD)： 这个分类器有一个好处是能够高效地处理非常大的数据集。这部分原因在于SGD一次只处理一条数据，这也使得 SGD 适合在线学习（online learning）。 from sklearn.linear_model import SGDClassifier sgd_clf = SGDClassifier(random_state=42) sgd_clf.fit(X_train, y_train_5) 小批量梯度下降(Mini-batch GD): 在迭代的每一步，批量梯度使用整个训练集，随机梯度时候用仅仅一个实例，在小批量梯度下降中，它则使用一个随机的小型实例集。它比随机梯度的主要优点在于你可以通过矩阵运算的硬件优化得到一个较好的训练表现，尤其当你使用 GPU 进行运算的时候。 支持向量机(SVM)： 支持向量机（SVM）是个非常强大并且有多种功能的机器学习模型，能够做线性或者非线性的分类，回归，甚至异常值检测。 SVM 特别适合应用于复杂但中小规模数据集的分类问题。 线性支持向量机： 以下的 Scikit-Learn 代码加载了内置的鸢尾花（Iris）数据集，缩放特征，并训练一个线性 SVM 模型(使用LinearSVM类，超参数 C = 1，hinge 损失函数)来检测 Virginica 鸢尾花。 import numpy as np from sklearn import datasets from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler from sklearn.svm import LinearSVC iris = datasets.load_iris() X = iris[\"data\"][:, (2, 3)] # petal length, petal width y = (iris[\"target\"] == 2).astype(np.float64) # Iris-Virginica svm_clf = Pipeline(( (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), )) svm_clf.fit(X, y) Then, as usual, you can use the model to make predictions: svm_clf.predict([[5.5, 1.7]]) array([ 1.]) ","date":"2019-03-29","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/:0:1","series":null,"tags":["机器学习"],"title":"监督学习","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/#线性支持向量机"},{"categories":["剑指Offer"],"content":" 题目描述： 定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88/:0:1","series":null,"tags":["剑指Offer","stack"],"title":"剑指Offer之包含min函数的栈","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： class Solution { public: void push(int value) { st.push(value); if(smin.empty()) smin.push(value); if(smin.top()\u003evalue) smin.push(value); } void pop() { if(smin.top()==st.top()) smin.pop(); st.pop(); } int top() { return st.top(); } int min() { return smin.top(); } private: stack\u003cint\u003e st; stack\u003cint\u003e smin; }; ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88/:0:2","series":null,"tags":["剑指Offer","stack"],"title":"剑指Offer之包含min函数的栈","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8C%85%E5%90%ABmin%E5%87%BD%E6%95%B0%E7%9A%84%E6%A0%88/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之数值的整数次方","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： class Solution { public: double Power(double base, int exponent) { base = pow(base, exponent); return base; } }; ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之数值的整数次方","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. class Solution { public: double Power(double base, int exponent) { if(exponent == 0) return 1.0; // 判断exponent是正数还是负数 int sign = exponent \u003c 0 ? 1:0; exponent = abs(exponent); int res = base; while(--exponent) { base *= res; } // 如果是负数，要求倒数 if(sign) base = 1 / base; return base; } }; ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9/:0:3","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之数值的整数次方","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B0%E5%80%BC%E7%9A%84%E6%95%B4%E6%95%B0%E6%AC%A1%E6%96%B9/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。 输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。 例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。 NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之旋转数组的最小数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: int minNumberInRotateArray(vector\u003cint\u003e rotateArray) { if(rotateArray.size() == 0) return 0; int temp = rotateArray[0]; for(int i = 0; i \u003c rotateArray.size(); i++) { if(temp \u003e rotateArray[i]) temp = rotateArray[i]; } return temp; } }; ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之旋转数组的最小数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: int minNumberInRotateArray(vector\u003cint\u003e rotateArray) { if (rotateArray.size() == 0) return 0; int temp = rotateArray[0]; int i = 0; int j = rotateArray.size() - 1; while(i \u003c j) { if(rotateArray[i] \u003c= rotateArray[j]) { temp = rotateArray[i]; j--; } else { temp = rotateArray[j]; i++; } } return temp; } }; ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/:0:3","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之旋转数组的最小数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/#解题思路二"},{"categories":["剑指Offer"],"content":" 解题思路三： 采用二分法解答这个问题， mid = low + (high - low)/2 需要考虑三种情况： (1)array[mid] \u003e array[high]: 出现这种情况的array类似[3,4,5,6,0,1,2]，此时最小数字一定在mid的右边。 low = mid + 1 (2)array[mid] == array[high]: 出现这种情况的array类似 [1,0,1,1,1] 或者[1,1,1,0,1]，此时最小数字不好判断在mid左边 还是右边,这时只好一个一个试 ， high = high - 1 (3)array[mid] \u003c array[high]: 出现这种情况的array类似[2,2,3,4,5,6,6],此时最小数字一定就是array[mid]或者在mid的左 边。因为右边必然都是递增的。 high = mid 时间复杂度: $O(logn)$, 空间复杂度: $O(1)$. class Solution { public: int minNumberInRotateArray(vector\u003cint\u003e rotateArray) { int low = 0 ; int high = rotateArray.size() - 1; while(low \u003c high){ int mid = low + (high - low) / 2; if(rotateArray[mid] \u003e rotateArray[high]){ low = mid + 1; }else if(rotateArray[mid] == rotateArray[high]){ high = high - 1; }else{ high = mid; } } return rotateArray[low]; } }; ","date":"2019-03-28","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/:0:4","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之旋转数组的最小数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%97%8B%E8%BD%AC%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E6%95%B0%E5%AD%97/#解题思路三"},{"categories":["OpenCV"],"content":" Chapter_01 : 基础知识 Chapter_02 : 操作像素 Chapter_03 : 处理图像的颜色 Chapter_04 : 用直方图统计像素 Chapter_05 : 用形态学运算变换图像 Chapter_06 : 图像滤波 Chapter_07 : 提取直线、轮廓和区域 Chapter_08 : 检测兴趣点 Chapter_09 : 描述和匹配兴趣点 Chapter_10 : 估算图像之间的投影关系 Chapter_11 : 三维重建 Chapter_12 : 处理视频序列 Chapter_13 : 跟踪运动物体 Chapter_14 : 实用案列 ","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#"},{"categories":["OpenCV"],"content":" Chapter_01 : 基础知识 1.图像的水平变换(flip),添加文字（putText）,鼠标的触发事件 2.感兴趣区 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; void onMouse(int event, int x, int y, int flags, void *param) { //reinterpret_cast允许将任何指针转为任何其他指针类型， //也允许将任何整数类型转化为任何指针类型以及反向转换。 Mat *im = reinterpret_cast\u003cMat *\u003e(param); switch (event) { case EVENT_LBUTTONDOWN: cout \u003c\u003c \"at(\" \u003c\u003c x \u003c\u003c \",\" \u003c\u003c y \u003c\u003c \") value is: \" \u003c\u003c static_cast\u003cint\u003e(im-\u003eat\u003cuchar\u003e(Point(x, y))) \u003c\u003c endl; break; default: break; } } int main() { //1.图像的水平变换(flip), 添加文字（putText）, 鼠标的触发事件 /*Mat image; cout \u003c\u003c \"This image is \" \u003c\u003c image.rows \u003c\u003c \"x\" \u003c\u003c image.cols \u003c\u003c \"y\" \u003c\u003c endl; image = imread(\"./images/puppy.bmp\", IMREAD_GRAYSCALE); if (image.empty()) { cout \u003c\u003c \"Error reading image...\" \u003c\u003c endl; return 0; } imshow(\"image\", image); cout \u003c\u003c \"This image is \" \u003c\u003c image.rows \u003c\u003c \"x\" \u003c\u003c image.cols \u003c\u003c \"y\" \u003c\u003c endl; cout \u003c\u003c \"This image has\" \u003c\u003c image.channels() \u003c\u003c \"channel(s)\" \u003c\u003c endl; setMouseCallback(\"image\", onMouse, reinterpret_cast\u003cvoid*\u003e(\u0026image)); Mat result; flip(image, result, 1); imshow(\"flip\", result); circle(image, Point(155, 100), 65, 0, 3); putText(image, \"SHA_CUN\", Point(40, 200), FONT_HERSHEY_PLAIN, 2.0, 255, 2); imshow(\"sha_cun\",image);*/ //2.感兴趣区域 Mat image = imread(\"./images/puppy.bmp\"); Mat logo = imread(\"./images/smalllogo.png\"); imshow(\"image\", image); imshow(\"logo\", logo); cout \u003c\u003c \"logo : \" \u003c\u003c logo.channels() \u003c\u003c endl; Mat imageROI(image, Rect(image.cols - logo.cols, image.rows - logo.rows, logo.cols, logo.rows)); logo.copyTo(imageROI); imshow(\"Image_logo\", image); image = imread(\"./images/puppy.bmp\"); imageROI = image(Rect(image.cols - logo.cols, image.rows - logo.rows, logo.cols, logo.rows)); Mat mask(logo); //必须是灰度图，只复制值不为0的部分（0为黑色， 255为白色） logo.copyTo(imageROI, mask); imshow(\"image_mask\", image); waitKey(0); return 0; } 图像像素的遍历:.at()、迭代器、指针 //1.at()方法实现图像像素的遍历 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; int main() { Mat grayim(600, 800, CV_8UC1); Mat colorim(600, 800, CV_8UC3); for(int i = 0; i \u003c grayim.rows; i++) { for(int j = 0; j \u003c grayim.cols; j++) { grayim.at\u003cuchar\u003e(i,j) = (i + j) % 255; } } for(int i = 0; i \u003c colorim.rows; i++) { for(int j = 0; j \u003c colorim.cols; j++) { Vec3b pixel; pixel[0] = i % 255; //Blue pixel[1] = i % 255; //Green pixel[2] = 0; //Red colorim.at\u003cVec3b\u003e(i, j) = pixel; } } imshow(\"grayim\", grayim); imshow(\"colorim\", colorim); waitKey(0); return 0; } //2.简单的二值化处理代码 //如果，我们只需要图像的轮廓或者边缘的时候，那么其他像素是不是就可以认为噪声， //为了减少数据量和方便计算，此时，我们就可以进行二值化 //而且二值化，还能去用来进行图像分割，前景后景分割 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImage; srcImage = imread(\"4.jpg\"); imshow(\"srcImage1\",srcImage); for(int i = 0; i \u003c srcImage.rows; i++) { for(int j = 0; j \u003c srcImage.cols; j++) { Vec3b value = srcImage.at\u003cVec3b\u003e(i,j); for(int h = 0; h \u003c 3; h++) { if(value[h] \u003e 128) value[h] = 255; else value[h] = 0; } } srcImage.at\u003cVec3b\u003e(i, j) = value; } imshow(\"srcImage2\", srcImage); waitKey(0); return 0; } //3.使用迭代器遍历矩阵 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; int main() { Mat grayim(800, 600, CV_8UC1); Mat colorim(800, 600, CV_8UC3); MatIterator_\u003cuchar\u003e grayit, grayend; for (grayit = grayim.begin\u003cuchar\u003e(), grayend = grayim.end\u003cuchar\u003e(); grayit != grayend; grayit++) *grayit = rand() % 255; MatIterator_\u003cVec3b\u003e colorit, colorend; for (colorit = colorim.begin\u003cVec3b\u003e(), colorend = colorim.end\u003cVec3b\u003e(); colorit != colorend; colorit++) (*colorit)[0] = rand() % 255; //Blue (*colorit)[1] = rand() % 255; //Green (*colorit)[2] = rand() % 255; //Red imshow(\"grayim\", grayim); imshow(\"colorim\", colorim); waitKey(0); return 0; } //4.使用指针来遍历 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; int main() { Mat grayim(800, 600, CV_8UC1); Mat colorim(800, 600, CV_8UC3); for (int i = 0; i \u003c grayim.rows; i++) { uchar *p = grayim.ptr\u003cuchar\u003e(i); for (int j = 0; j \u003c grayim.cols; j++) { p[j] = (i + j) %","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_01--基础知识"},{"categories":["OpenCV"],"content":" Chapter_02 : 操作像素 1.创建椒盐噪声 2.创建波浪影响remap() —- 重映射 3.扫描图像并访问相邻像素（1、代码的运行时间 2、锐化） 4.增加图片 5.减少图片中颜色的数量(没敲，太麻烦 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e //5.减少图片中颜色的数量 #define NTESTS 15 #define NITERATIONS 10 using namespace std; using namespace cv; //2.创建波浪影响remap() void wave( const Mat \u0026image, Mat \u0026result) { //the map functiuons Mat srcX(image.rows, image.cols, CV_32F); Mat srcY(image.rows, image.cols, CV_32F); //creating the mapping for (int i = 0; i \u003c image.rows; i++) { for (int j = 0; j \u003c image.cols; j++) { srcX.at\u003cfloat\u003e(i, j) = j; srcY.at\u003cfloat\u003e(i, j) = i + 3 * sin(j / 6.0); // horizontal flipping // srcX.at\u003cfloat\u003e(i,j)= image.cols-j-1; // srcY.at\u003cfloat\u003e(i,j)= i; } } //applying the mapping remap(image, result, srcX, srcY, INTER_LINEAR); } //3.扫描图像并访问相邻像素 void sharpen(const Mat \u0026image, Mat \u0026result) { result.create(image.size(), image.type()); int nchannels = image.channels(); //处理所有的行，（除了第一行和最后一行） for (int j = 1; j \u003c image.rows - 1; j++) { const uchar *previous = image.ptr\u003cconst uchar\u003e(j - 1); //previous row const uchar *current = image.ptr\u003cconst uchar\u003e(j); //current row const uchar *next = image.ptr\u003cconst uchar\u003e(j + 1); //next row uchar *output = result.ptr\u003cuchar\u003e(j); //output row for (int i = nchannels; i \u003c (image.cols - 1) * nchannels; i++) { //应用锐化算子 *output++ = saturate_cast\u003cuchar\u003e(5 * current[i] - current[i-nchannels] - current[i+nchannels] - previous[i] - next[i]); } } //把未处理的像素设置为0 result.row(0).setTo(Scalar(0)); result.row(result.rows - 1).setTo(Scalar(0)); result.col(0).setTo(Scalar(0)); result.col(result.cols - 1).setTo(Scalar(0)); } //和sharpen一样的功能，只不过用了迭代 void sharpenIterator(const Mat \u0026image, Mat \u0026result) { //must be a gray-level image CV_Assert(image.type() == CV_8UC1); //initialize iterator at row 1 Mat_\u003cuchar\u003e :: const_iterator it = image.begin\u003cuchar\u003e() + image.cols; Mat_\u003cuchar\u003e :: const_iterator itend = image.end\u003cuchar\u003e() - image.cols; Mat_\u003cuchar\u003e :: const_iterator itup = image.begin\u003cuchar\u003e(); Mat_\u003cuchar\u003e ::const_iterator itdown = image.begin\u003cuchar\u003e() + 2 * image.cols; //setup output image and iterator result.create(image.size(), image.type()); //allocate if necessary Mat_\u003cuchar\u003e ::iterator itout = result.begin\u003cuchar\u003e() + result.cols; for (; it != itend; ++it, ++itout, ++itup, ++itdown) { *itout = cv::saturate_cast\u003cuchar\u003e(*it * 5 - *(it - 1) - *(it + 1) - *itup - *itdown); } // Set the unprocessed pixels to 0 result.row(0).setTo(cv::Scalar(0)); result.row(result.rows - 1).setTo(cv::Scalar(0)); result.col(0).setTo(cv::Scalar(0)); result.col(result.cols - 1).setTo(cv::Scalar(0)); } void sharpen2D(const Mat \u0026image, Mat \u0026result) { //构造内核（所有入口都初始化为0） Mat kernel(3, 3, CV_32F, Scalar(0)); //对内核赋值 kernel.at\u003cfloat\u003e(1, 1) = 5.0; kernel.at\u003cfloat\u003e(0, 1) = -1.0; kernel.at\u003cfloat\u003e(2, 1) = -1.0; kernel.at\u003cfloat\u003e(1, 0) = -1.0; kernel.at\u003cfloat\u003e(1, 2) = -1.0; //对图像滤波 filter2D(image, result, image.depth(), kernel); } //5.减少图片中颜色的数量 // 1st version // see recipe Scanning an image with pointers void colorReduce(Mat image, int div = 64) { int nl = image.rows; // number of lines int nc = image.cols * image.channels(); // total number of elements per line for (int j = 0; j\u003cnl; j++) { // get the address of row j uchar* data = image.ptr\u003cuchar\u003e(j); for (int i = 0; i\u003cnc; i++) { // process each pixel --------------------- data[i] = data[i] / div*div + div / 2; // end of pixel processing ---------------- } // end of line } } // version with input/ouput images // see recipe Scanning an image with pointers void colorReduceIO(const cv::Mat \u0026image, // input image cv::Mat \u0026result, // output image int div = 64) { int nl = image.rows; // number of lines int nc = image.cols; // number of columns int nchannels = image.channels(); // number of channels // allocate output image if necessary result.create(image.rows, image.cols, image.type()); for (int j = 0; j\u003cnl; j++) { // get the addresses of input and output row j const uchar* data_in = image.ptr\u003cuchar\u003e(j); uchar* data_out = result.ptr\u003cuchar\u003e(j); for (int i = 0; i\u003cnc*nchannels; i++) { //","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:2","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_02--操作像素"},{"categories":["OpenCV"],"content":" Chapter_03 : 处理图像的颜色 1.用策略设计模式比较颜色 2.用GrabCut算法分割图像 3.用色调、饱和度、和亮度表示颜色 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \"ColorDetector.h\" using namespace std; using namespace cv; //3.用色调、饱和度、和亮度表示颜色 ---- 检测肤色 //输入图像 色调区间 饱和度区间 输出掩码 void detectHScolor(const Mat\u0026 image, double minHue, double maxHue, double minSat, double maxSat, Mat \u0026mask) { //转到HSV空间 Mat hsv; cvtColor(image, hsv, CV_BGR2HSV); //将3通道分割成3幅图像 vector\u003cMat\u003e channels; split(hsv, channels); //色调掩码 Mat mask1; //小于maxHue threshold(channels[0], mask1, maxHue, 255, THRESH_BINARY_INV); Mat mask2; //大于minHue threshold(channels[0], mask2, minHue, 255, THRESH_BINARY); Mat hueMask; //色调掩码 if (minHue \u003c maxHue) hueMask = mask1 \u0026 mask2; else //如果区间穿越0度中轴线 hueMask = mask1 | mask2; //饱和度掩码 //从minSat 到 maxSat //threshold(channels[1], mask1, maxSat, 255, THRESH_BINARY_INV); //threshold(channels[1], mask2, minSat, 255, THRESH_BINARY); //Mat satMask; //satMask = mask1 \u0026 mask2; Mat satMask; inRange(channels[1], minSat, maxSat, satMask); //组合掩码 mask = hueMask \u0026 satMask; } int main() { ////1.用策略设计模式比较颜色 ////创建图像处理器对象 //ColorDetector cdetect; ////读取图像 //Mat image = imread(\"./images/boldt.jpg\"); //if (image.empty()) // return 0; //imshow(\"image\", image); ////设置输入参数 //cdetect.setTargetColor(230, 190, 130); // 这里表示蓝天 ////处理图像并显示结果 //Mat result = cdetect.process(image); //imshow(\"result\", result); //// or using functor //// here distance is measured with the Lab color space //ColorDetector colordetector(230, 190, 130, // color // 45, true); // Lab threshold //namedWindow(\"result (functor)\"); //result = colordetector(image); //imshow(\"result (functor)\", result); //// testing floodfill //floodFill(image, // input/ouput image // Point(100, 50), // seed point // Scalar(255, 255, 255), // repainted color // (Rect*)0, // bounding rectangle of the repainted pixel set // Scalar(35, 35, 35), // low and high difference threshold // Scalar(35, 35, 35), // most of the time will be identical // FLOODFILL_FIXED_RANGE); // pixels are compared to seed color //namedWindow(\"Flood Fill result\"); //result = colordetector(image); //imshow(\"Flood Fill result\", image); //// Creating artificial images to demonstrate color space properties //Mat colors(100, 300, CV_8UC3, Scalar(100, 200, 150)); //Mat range = colors.colRange(0, 100); //range = range + Scalar(10, 10, 10); //range = colors.colRange(200, 300); //range = range + Scalar(-10, -10, 10); //namedWindow(\"3 colors\"); //imshow(\"3 colors\", colors); //Mat labImage(100, 300, CV_8UC3, Scalar(100, 200, 150)); //cvtColor(labImage, labImage, CV_BGR2Lab); //range = colors.colRange(0, 100); //range = range + Scalar(10, 10, 10); //range = colors.colRange(200, 300); //range = range + Scalar(-10, -10, 10); //cvtColor(labImage, labImage, CV_Lab2BGR); //namedWindow(\"3 colors (Lab)\"); //imshow(\"3 colors (Lab)\", colors); //// brightness versus luminance //Mat grayLevels(100, 256, CV_8UC3); //for (int i = 0; i \u003c 256; i++) { // grayLevels.col(i) = Scalar(i, i, i); //} //range = grayLevels.rowRange(50, 100); //Mat channels[3]; //split(range, channels); //channels[1] = 128; //channels[2] = 128; //merge(channels, 3, range); //cvtColor(range, range, CV_Lab2BGR); //namedWindow(\"Luminance vs Brightness\"); //imshow(\"Luminance vs Brightness\", grayLevels); //2.用GrabCut算法分割图像 //Mat image = imread(\"./images/boldt.jpg\"); //if (image.empty()) // return 0; //imshow(\"image\", image); //Rect rectangle(5, 70, 260, 120); //Mat result; //Mat bg, fg; ////GrabCut分割算法 //grabCut(image, result, rectangle, bg, fg, 5, GC_INIT_WITH_RECT); ////取得标记可能为前景的像素 //compare(result, GC_PR_FGD, result, CMP_EQ); ////生成输出图像 //Mat foreground(image.size(), CV_8UC3, Scalar(255, 255, 255)); //image.copyTo(foreground, result); //A.copyTo(B, mask); //imshow(\"foreground\", foreground); //imshow(\"result\", result); //imshow(\"image_copy\", image); //3.用色调、饱和度、和亮度表示颜色 Mat image = imread(\"./images/boldt.jpg\"); if (image.empty()) return 0; imshow(\"image\", image); //转换成HSV Mat hsv; cvtColor(image, hsv, CV_BGR2HSV); imshow(\"hsv\",","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:3","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_03--处理图像的颜色"},{"categories":["OpenCV"],"content":" Chapter_04 : 用直方图统计像素 计算图像直方图 //#include \"Histogram1D.h\" //#include \"ContentFinder.h\" //#include \"ColorHistogram.h\" // // //int main() //{ // //读入图像，并转成灰度图 // Mat image = imread(\"./images/waves.jpg\", 0); // if (!image.data) // return 0; // // imshow(\"image\", image); // // Mat imageROI; // imageROI = image(Rect(216, 33, 24, 30)); //设置感兴趣区域 // // imshow(\"imageROI\", imageROI); // // //直方图对象 // //计算直方图 // Histogram1D h; // //计算直方图 // Mat hist = h.getHistogram(imageROI); // imshow(\"Histogram1D\", h.getHistogramImage(imageROI)); // // //创建内容搜寻器 // ContentFinder finder; // //设置用来反向投影的直方图 // // // set histogram to be back-projected // finder.setHistogram(hist); // finder.setThreshold(-1.0f); // // // Get back-projection // cv::Mat result1; // result1 = finder.find(image); // // // Create negative image and display result // cv::Mat tmp; // result1.convertTo(tmp, CV_8U, -1.0, 255.0); // cv::namedWindow(\"Backprojection result\"); // cv::imshow(\"Backprojection result\", tmp); // // // Get binary back-projection // finder.setThreshold(0.12f); // result1 = finder.find(image); // // // Draw a rectangle around the reference area // cv::rectangle(image, cv::Rect(216, 33, 24, 30), cv::Scalar(0, 0, 0)); // // // Display image // cv::namedWindow(\"Image\"); // cv::imshow(\"Image\", image); // // // Display result // cv::namedWindow(\"Detection Result\"); // cv::imshow(\"Detection Result\", result1); // // // // //装载彩色图片 // ColorHistogram hc; // Mat color = imread(\"./images/waves.jpg\"); // // imshow(\"color\", color); // // //提取ROI // imageROI = color(Rect(0, 0, 100, 45)); //蓝色天空的区域 // //获取3D颜色直方图（每个通道适合8个箱子） // hc.setSize(8); //8 * 8 * 8 // Mat shist = hc.getHistogram(imageROI); // //创建内容搜寻器 // //设置用来反向投影的直方图 // finder.setHistogram(shist); // finder.setThreshold(0.05f); // //取得颜色直方图的反向投影 // result1 = finder.find(color); // // imshow(\"result1\", result1); // // //装载彩色图片 // Mat color2 = imread(\"./images/dog.jpg\"); // imshow(\"color2\", color2); // // Mat result2 = finder.find(color2); // // imshow(\"result2\", result2); // // // // Get ab color histogram // hc.setSize(256); // 256x256 // cv::Mat colorhist = hc.getabHistogram(imageROI); // // // display 2D histogram // colorhist.convertTo(tmp, CV_8U, -1.0, 255.0); // cv::namedWindow(\"ab histogram\"); // cv::imshow(\"ab histogram\", tmp); // // // // set histogram to be back - projected // finder.setHistogram(colorhist); // finder.setThreshold(0.05f); // // // Convert to Lab space // cv::Mat lab; // cv::cvtColor(color, lab, CV_BGR2Lab); // // // Get back-projection of ab histogram // int ch[2] = { 1,2 }; // result1 = finder.find(lab, 0, 256.0f, ch); // // cv::namedWindow(\"Result ab (1)\"); // cv::imshow(\"Result ab (1)\", result1); // // // // // Second colour image // cv::cvtColor(color2, lab, CV_BGR2Lab); // // // Get back-projection of ab histogram // result2 = finder.find(lab, 0, 256.0, ch); // // cv::namedWindow(\"Result ab (2)\"); // cv::imshow(\"Result ab (2)\", result2); // // // Draw a rectangle around the reference sky area // cv::rectangle(color, cv::Rect(0, 0, 100, 45), cv::Scalar(0, 0, 0)); // cv::namedWindow(\"Color Image\"); // cv::imshow(\"Color Image\", color); // // // Get Hue colour histogram // hc.setSize(180); // 180 bins // colorhist = hc.getHueHistogram(imageROI); // // // set histogram to be back-projected // finder.setHistogram(colorhist); // // // // // Convert to HSV space // cv::Mat hsv; // cv::cvtColor(color, hsv, CV_BGR2HSV); // // Get back-projection of hue histogram // ch[0] = 0; // result1 = finder.find(hsv, 0.0f, 180.0f, ch); // // cv::namedWindow(\"Result Hue (1)\"); // cv::imshow(\"Result Hue (1)\", result1); // // //// Second colour image // //color2 = cv::imread(\"./images/dog.jpg\"); // // //// Convert to HSV space // //cv::cvtColor(color2, hsv, CV_BGR2HSV); // // //// Get back-projection of hue histogram // //result2 = finder.find(hsv, 0.0f, 180.0f, ch); // // //cv::namedWindow(\"Result Hue (2)\"); // //cv::imshow(\"Result Hue (2)\", result2); // // // waitKey(0); // return 0; //} #pragma once #if !d","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:4","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_04--用直方图统计像素"},{"categories":["OpenCV"],"content":" Chapter_05 : 用形态学运算变换图像 1.腐蚀、膨胀、开始、闭合等形态学运算 2.MSER算法提取特征区域 3.用分水岭算法实现图像分割 #include \"WatershedSegmenter.h\" int main() { ////1.腐蚀、膨胀、开始、闭合等形态学运算 //Mat image = imread(\"./images/binary.bmp\"); //if (!image.data) // return 0; //imshow(\"image\", image); //Mat eroded; //erode(image, eroded, Mat()); //imshow(\"erode\", eroded); //Mat dilated; //dilate(image, dilated, Mat()); //imshow(\"dilate\", dilated); //Mat element(7, 7, CV_8U, Scalar(1)); //erode(image, eroded, element); //imshow(\"Erode-7*7\", eroded); //erode(image, eroded, Mat(), Point(-1, -1), 3); //imshow(\"erode--3time\", eroded); //Mat element5(5, 5, CV_8U, Scalar(1)); //Mat closed; //morphologyEx(image, closed, MORPH_CLOSE, element5); //imshow(\"close\", closed); //Mat opened; //morphologyEx(image, opened, MORPH_OPEN, element5); //imshow(\"open\", opened); //// explicit closing //// 1. dilate original image //cv::Mat result; //cv::dilate(image, result, element5); //// 2. in-place erosion of the dilated image //cv::erode(result, result, element5); //// Display the closed image //cv::namedWindow(\"Closed Image (2)\"); //cv::imshow(\"Closed Image (2)\", result); //// Close and Open the image //cv::morphologyEx(image, image, cv::MORPH_CLOSE, element5); //cv::morphologyEx(image, image, cv::MORPH_OPEN, element5); //// Display the close/opened image //cv::namedWindow(\"Closed|Opened Image\"); //cv::imshow(\"Closed|Opened Image\", image); //cv::imwrite(\"binaryGroup.bmp\", image); //// Read input image //image = cv::imread(\"./images/binary.bmp\"); //// Open and Close the image //cv::morphologyEx(image, image, cv::MORPH_OPEN, element5); //cv::morphologyEx(image, image, cv::MORPH_CLOSE, element5); //// Display the close/opened image //cv::namedWindow(\"Opened|Closed Image\"); //cv::imshow(\"Opened|Closed Image\", image); //Mat image2 = imread(\"./images/boldt.jpg\", 0); //if (!image2.data) // return 0; //imshow(\"image2\", image2); //morphologyEx(image2, result, MORPH_GRADIENT, Mat()); //imshow(\"Edge_result\", result); //imshow(\"Edge_255-result\", 255 - result); //int threshold(80); //cv::threshold(result, result, threshold, 255, THRESH_BINARY); //imshow(\"threshold_result\", result); //Mat image3 = imread(\"./images/book.jpg\", 0); //if (!image3.data) // return 0; //imshow(\"Image3\", image3); //transpose(image3, image3); //imshow(\"transpose\", image3); //flip(image3, image3, 0); //imshow(\"image3\", image3); //Mat element7(7, 7, CV_8U, Scalar(1)); //morphologyEx(image3, result, MORPH_BLACKHAT, element7); //imshow(\"Blackhat\", result); //threshold = 25; //cv::threshold(result, result, // threshold, 255, cv::THRESH_BINARY); //imshow(\"Thresholded Black Top-hat\", 255 - result); ////2.MSER算法提取特征区域 //Mat image = imread(\"./images/building.jpg\", 0); //if (!image.data) // return 0; //imshow(\"image\", image); ////基本的MSER检测器 //Ptr\u003cMSER\u003e ptrMSER = MSER::create(5, //局部检测时使用的增量值 // 200, //允许的最小面积 // 2000); //允许的最大面积 ////点集的容器 //vector\u003cvector\u003cPoint\u003e \u003e points; ////矩形的容器 //vector\u003cRect\u003e rects; ////检测MSER特征 ----检测的结果放在两个区域，第一个是区域的容器，每个区域用组成它的像素点表示， ////第二个是矩形的容器，每个矩形包围一个区域 //ptrMSER-\u003edetectRegions(image, points, rects); //cout \u003c\u003c points.size() \u003c\u003c \"MSERs detected\" \u003c\u003c endl; ////创建白色区域 //Mat output(image.size(), CV_8UC3); //output = Scalar(255, 255, 255); // //Opencv随机数生成器 //RNG rng; ////针对每个检测到的特征区域， 在彩色区域显示MSER ////反向排序，先显示较大的MSER //for (vector\u003cvector\u003cPoint\u003e \u003e ::reverse_iterator it = points.rbegin(); it != points.rend(); ++it) //{ // //生成随机颜色 // Vec3b c(rng.uniform(0, 254), rng.uniform(0, 254), rng.uniform(0, 254)); // cout \u003c\u003c \"MSER size = \" \u003c\u003c it-\u003esize() \u003c\u003c endl; // //针对MSER集合中的每个点 // for (vector\u003cPoint\u003e ::iterator itPts = it-\u003ebegin(); itPts != it-\u003eend(); ++itPts) // { // //不重复MSER的像素 // if (output.at\u003cVec3b\u003e(*itPts)[0] == 255) // { // output.at\u003cVec3b\u003e(*itPts) = c; // } // } //} //imshow(\"MSER point sets\", output); //imwrite(\"./images/mser.bmp\", output); ////提取并显示矩形的MSER //vector\u003cRect\u003e ::iterator itr = rects.begin(); //vector\u003cvector\u003cPoint\u003e \u003e ::iterator itp = points.begin(); //for (; itr != rects.end(); ++itr, ++itp) //{ // //检查两者比例 // if (stati","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:5","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_05--用形态学运算变换图像"},{"categories":["OpenCV"],"content":" Chapter_06 : 图像滤波 1.方块滤波、高斯滤波、resize(), pyrUp(), pyrDown() 2.Sobel滤波器 3.Laplacian算子 4.高斯差分 //#include \u003ciostream\u003e //#include \u003copencv2/opencv.hpp\u003e // //#include \"LaplacianZC.h\" // //using namespace std; //using namespace cv; // //int main() //{ // ////1.方块滤波、高斯滤波、resize(), pyrUp(), pyrDown() // //Mat image = imread(\"./images/boldt.jpg\", 0); // //if (!image.data) // //{ // // cout \u003c\u003c \"image is error\" \u003c\u003c endl; // // return 0; // //} // //imshow(\"image\", image); // // // ////方块滤波 // //Mat result; // //blur(image, result, Size(5, 5)); // //imshow(\"Mean Image\", result); // // //Blur the image with a mean filter 9*9 // //blur(image, result, Size(9, 9)); // //imshow(\"Mean Image(9 * 9)\", result); // // ////高斯滤波 // //GaussianBlur(image, result, Size(5, 5), //滤波尺寸 // // 1.5); //控制高斯曲线形状的参数 // //imshow(\"Gaussian Filtered Image\", result); // // //// Display the blurred image // //cv::namedWindow(\"Gaussian filtered Image (9x9)\"); // //cv::imshow(\"Gaussian filtered Image (9x9)\", result); // // ////Get the gaussian kernel (1.5) // //Mat gauss = getGaussianKernel(9, 1.5, CV_32F); // // ////Display kernal value // //Mat_\u003cfloat\u003e::const_iterator it = gauss.begin\u003cfloat\u003e(); // //Mat_\u003cfloat\u003e::const_iterator itend = gauss.end\u003cfloat\u003e(); // //cout \u003c\u003c \"1.5 = [\"; // //for (; it != itend; ++it) // //{ // // cout \u003c\u003c \"]\" \u003c\u003c endl; // //} // // ////Get the gaussian kernel(0.5) // //gauss = getGaussianKernel(9, 0.5, CV_32F); // //// Display kernel values // //it = gauss.begin\u003cfloat\u003e(); // //itend = gauss.end\u003cfloat\u003e(); // //std::cout \u003c\u003c \"0.5 = [\"; // //for (; it != itend; ++it) { // // std::cout \u003c\u003c *it \u003c\u003c \" \"; // //} // //std::cout \u003c\u003c \"]\" \u003c\u003c std::endl; // // //// Get the gaussian kernel (2.5) // //gauss = cv::getGaussianKernel(9, 2.5, CV_32F); // // //// Display kernel values // //it = gauss.begin\u003cfloat\u003e(); // //itend = gauss.end\u003cfloat\u003e(); // //std::cout \u003c\u003c \"2.5 = [\"; // //for (; it != itend; ++it) { // // std::cout \u003c\u003c *it \u003c\u003c \" \"; // //} // //std::cout \u003c\u003c \"]\" \u003c\u003c std::endl; // // //// Get the gaussian kernel(9 elements) // //gauss = cv::getGaussianKernel(9, -1, CV_32F); // // //// Display kernel values // //it = gauss.begin\u003cfloat\u003e(); // //itend = gauss.end\u003cfloat\u003e(); // //std::cout \u003c\u003c \"9 = [\"; // //for (; it != itend; ++it) { // // std::cout \u003c\u003c *it \u003c\u003c \" \"; // //} // //std::cout \u003c\u003c \"]\" \u003c\u003c std::endl; // // //// Get the Deriv kernel (2.5) // //cv::Mat kx, ky; // //cv::getDerivKernels(kx, ky, 2, 2, 7, true); // // //// Display kernel values // //cv::Mat_\u003cfloat\u003e::const_iterator kit = kx.begin\u003cfloat\u003e(); // //cv::Mat_\u003cfloat\u003e::const_iterator kitend = kx.end\u003cfloat\u003e(); // //std::cout \u003c\u003c \"[\"; // //for (; kit != kitend; ++kit) { // // std::cout \u003c\u003c *kit \u003c\u003c \" \"; // //} // //std::cout \u003c\u003c \"]\" \u003c\u003c std::endl; // // //// Read input image with salt\u0026pepper noise // //image = cv::imread(\"./images/salted.bmp\", 0); // //if (!image.data) // // return 0; // // //// Display the S\u0026P image // //cv::namedWindow(\"S\u0026P Image\"); // //cv::imshow(\"S\u0026P Image\", image); // // //// Blur the image with a mean filter // //cv::blur(image, result, cv::Size(5, 5)); // // //// Display the blurred image // //cv::namedWindow(\"Mean filtered S\u0026P Image\"); // //cv::imshow(\"Mean filtered S\u0026P Image\", result); // // //// Applying a median filter // //cv::medianBlur(image, result, 5); // // //// Display the blurred image // //cv::namedWindow(\"Median filtered Image\"); // //cv::imshow(\"Median filtered Image\", result); // // //// Reduce by 4 the size of the image (the wrong way) // ////只保留每四个像素中的一个 // //image = cv::imread(\"./images/boldt.jpg\", 0); // //cv::Mat reduced(image.rows / 4, image.cols / 4, CV_8U); // // //for (int i = 0; i\u003creduced.rows; i++) // // for (int j = 0; j\u003creduced.cols; j++) // // reduced.at\u003cuchar\u003e(i, j) = image.at\u003cuchar\u003e(i * 4, j * 4); // // //// Display the reduced image // //cv::namedWindow(\"Badly reduced Image\"); // //cv::imshow(\"Badly reduced Image\", reduced); // // //cv::resize(reduced, reduced, cv::Size(), 4, 4, cv::INTER_NEAREST); // // //// Display the (resized) reduced image // //cv::namedWind","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:6","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_06----图像滤波"},{"categories":["OpenCV"],"content":" Chapter_07 : 提取直线、轮廓和区域 1.Sobel算子的部分 Canny边缘提取 2.用霍夫变换检测直线 3.用概率霍夫变换检测直线 4.点集的直线拟合 5.提取连续区域 //#include \u003ciostream\u003e //#include \u003copencv2/opencv.hpp\u003e //#include \u003cvector\u003e // //#include \"EdgeDetector.h\" //#include \"LineFinder.h\" // //using namespace std; //using namespace cv; // //int main() //{ // //Mat image = imread(\"./images/road.jpg\", 0); // //if (!image.data) // // return 0; // //imshow(\"Image\", image); // // ////计算Sobel // //EdgeDetector ed; // //ed.computeSobel(image); // // ////展示Sobel // //imshow(\"Sobel (orientation)\", ed.getSobelOrientationImage()); // //imwrite(\"./images/ori.bmp\", ed.getSobelOrientationImage()); // //// Display the Sobel low threshold // //cv::namedWindow(\"Sobel (low threshold)\"); // //cv::imshow(\"Sobel (low threshold)\", ed.getBinaryMap(125)); // // //// Display the Sobel high threshold // //cv::namedWindow(\"Sobel (high threshold)\"); // //cv::imshow(\"Sobel (high threshold)\", ed.getBinaryMap(350)); // // ////运用Canny边缘算子 // //Mat contours; // //Canny(image, //灰度图像 // // contours, //输出轮廓 // // 125, //低阈值 // // 350); //高阈值 // // //imshow(\"Canny\", 255 - contours); // // ////创建一个测试的图像 // //Mat test(200, 200, CV_8U, Scalar(0)); // //cv::line(test, cv::Point(100, 0), cv::Point(200, 200), cv::Scalar(255)); // //cv::line(test, cv::Point(0, 50), cv::Point(200, 200), cv::Scalar(255)); // //cv::line(test, cv::Point(0, 200), cv::Point(200, 0), cv::Scalar(255)); // //cv::line(test, cv::Point(200, 0), cv::Point(0, 200), cv::Scalar(255)); // //cv::line(test, cv::Point(100, 0), cv::Point(100, 200), cv::Scalar(255)); // //cv::line(test, cv::Point(0, 100), cv::Point(200, 100), cv::Scalar(255)); // // //// Display the test image // //cv::namedWindow(\"Test Image\"); // //cv::imshow(\"Test Image\", test); // //cv::imwrite(\"test.bmp\", test); // // ////2.用霍夫变换检测直线 // //vector\u003cVec2f\u003e lines; // //HoughLines(contours, lines, 1, // // PI/180, //步长 // // 60); //最小投票数 // ////画线 // //Mat result(contours.rows, contours.cols, CV_8U, Scalar(255)); // //image.copyTo(result); // // //cout \u003c\u003c \"Lines detected\" \u003c\u003c lines.size() \u003c\u003c endl; // // //vector\u003cVec2f\u003e::const_iterator it = lines.begin(); // //while (it != lines.end()) // //{ // // float rho = (*it)[0]; //第一个元素是距离rho // // float theta = (*it)[1]; //第二个元素是角度theta // // // if (theta \u003c PI/4. || theta \u003e 3.*PI / 4) //垂直线（大致） // // { // // //直线与第一行的交叉点 // // Point pt1(rho/cos(theta), 0); // // //直线与最后一行的交叉点 // // Point pt2((rho - result.rows * sin(theta)) / cos(theta), result.rows); // // //画白色的线 // // line(result, pt1, pt2, Scalar(255), 1); // // // } // // else //水平线（大致） // // { // // //直线与第一列的交叉点 // // Point pt1(0, rho/sin(theta)); // // //直线与最后一列的交叉点 // // Point pt2(result.cols, (rho - result.cols * cos(theta)) / sin(theta)); // // //画白色的线 // // line(result, pt1, pt2, Scalar(255), 1); // // } // // cout \u003c\u003c \"line: (\" \u003c\u003c rho \u003c\u003c \",\" \u003c\u003c theta \u003c\u003c \")\\n\"; // // // ++it; // //} // //imshow(\"HoughLines\", result); // // // ////3.用概率霍夫变换检测直线 // ////创建LineFinder类的实例 // //LineFinder finder; // ////设置概率霍夫变换的参数 // //finder.setLineLengthAndGap(100, 20); // //finder.setMinVote(60); // // ////检测直线并画直线 // //vector\u003cVec4i\u003e linesp = finder.findLines(contours); // //finder.drawDetectedLines(image); // // //std::vector\u003ccv::Vec4i\u003e::const_iterator it2 = linesp.begin(); // //while (it2 != linesp.end()) { // // // std::cout \u003c\u003c \"(\" \u003c\u003c (*it2)[0] \u003c\u003c \",\" \u003c\u003c (*it2)[1] \u003c\u003c \")-(\" // // \u003c\u003c (*it2)[2] \u003c\u003c \",\" \u003c\u003c (*it2)[3] \u003c\u003c \")\" \u003c\u003c std::endl; // // // ++it2; // //} // // //imshow(\"HoughLinesp\", image); // // ////4.点集的直线拟合 // //image = imread(\"./images/road.jpg\", 0); // // //int n = 0; //选用直线0 // //line(image, Point(linesp[n][0], linesp[n][1]), Point(linesp[n][2], linesp[n][3]),Scalar(255), 5); // //imshow(\"One line of the Image\", image); // ////黑白图像 // //Mat oneline(image.size(), CV_8U, Scalar(0)); // ////白色直线 // //line(oneline, Point(linesp[n][0], linesp[n][1]), Point(linesp[n][2], linesp[n][3]), Scalar(255), 3); // ////轮廓与白色直线进行与\u0026操作 // //bitwise_and(contours, oneline, oneline); // //imshow(\"One line\", 255 - oneline); // // //vecto","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:7","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_07--提取直线轮廓和区域"},{"categories":["OpenCV"],"content":" Chapter_08 : 检测兴趣点 1.Harris角点检测 2.GFTT(good-features-to-track) 3.drawKeypoints() 换关键点的通用函数 4.FAST角点检测 5.SURF尺度不变的特征检测 6.SIFT尺度不变特征转换 7.BRISK(二元稳健恒定可扩展关键点)检测法 8.ORB特征检测算法 //#include \u003ciostream\u003e //#include \u003copencv2/opencv.hpp\u003e //#include \u003copencv2/core.hpp\u003e //#include \u003copencv2/features2d.hpp\u003e //#include \u003copencv2/imgproc.hpp\u003e //#include \u003copencv2/highgui.hpp\u003e //#include \u003copencv2/xfeatures2d.hpp\u003e // //#include \"HarrisDetector.h\" // //using namespace std; //using namespace cv; // //int main() //{ // //1.Harris角点检测 // Mat image = imread(\"./images/church01.jpg\", 0); // if (!image.data) // return 0; // //变成水平的 // transpose(image, image); // flip(image, image, 0); // // imshow(\"Image\", image); // // //检测Harris角点 // Mat cornerStrength; // cornerHarris(image, cornerStrength, // 3, //领域尺寸 // 3, //口径尺寸 // 0.01); //Harris参数 // //对角点强度进行阈值化 // Mat harrisCorners; // double threshold = 0.0001; // cv::threshold(cornerStrength, harrisCorners, threshold, 255, THRESH_BINARY_INV); // imshow(\"Harris_threshold\", harrisCorners); // // //用一个类去封装Harris // HarrisDetected harris; // //计算Harris值 // harris.detect(image); // //检测Harris角点 // vector\u003cPoint\u003e pts; // harris.getCorners(pts, 0.02); // //画出Harris角点 // harris.drawOnImage(image, pts); // // imshow(\"HarrisDetected\", image); // // //2.GFTT(good-features-to-track) // image = imread(\"./images/church01.jpg\", 0); // // rotate the image (to produce a horizontal image) // cv::transpose(image, image); // cv::flip(image, image, 0); // //计算适合跟踪的特征 // vector\u003cKeyPoint\u003e keypoints; // //GFTT检测器 // Ptr\u003cGFTTDetector\u003e ptrGFTT = GFTTDetector::create( // 500, //关键点的最大值 // 0.01, //质量等级 // 10 //角点之间允许的最短距离 // ); // //检测GFTT // ptrGFTT-\u003edetect(image, keypoints); // //展示所有关键点 // vector\u003cKeyPoint\u003e::const_iterator it = keypoints.begin(); // while (it != keypoints.end()) // { // //对每个关键点画圆 // circle(image, it-\u003ept, 3, Scalar(255, 255, 255), 1); // ++it; // } // // imshow(\"GFTT\", image); // //3.drawKeypoints() 换关键点的通用函数 // // Read input image // image = cv::imread(\"./images/church01.jpg\", 0); // // rotate the image (to produce a horizontal image) // cv::transpose(image, image); // cv::flip(image, image, 0); // // // Opencv也提供了在图像上画关键点的通用函数 // cv::drawKeypoints(image, // 原始图像 // keypoints, // 关键点的向量 // image, // 输出图像 // cv::Scalar(255, 255, 255), // 关键点颜色 // cv::DrawMatchesFlags::DRAW_OVER_OUTIMG); //画图标志 // // // Display the keypoints // cv::namedWindow(\"Good Features to Track Detector\"); // cv::imshow(\"Good Features to Track Detector\", image); // // //4.FAST角点检测 // image = imread(\"./images/church01.jpg\", 0); // transpose(image, image); // flip(image, image, 0); // //最终的关键点容器 // keypoints.clear(); // //FAST特征检测器，阈值为40 // Ptr\u003cFastFeatureDetector\u003e ptrFAST = FastFeatureDetector::create(40); // //检测关键点 // ptrFAST-\u003edetect(image, keypoints); // //画关键点 // drawKeypoints(image, keypoints, image, Scalar(255, 255,255), DrawMatchesFlags::DRAW_OVER_OUTIMG); // cout \u003c\u003c \"Number of keypoints(FAST): \" \u003c\u003c keypoints.size() \u003c\u003c endl; // // imshow(\"FAST_01\", image); // //FAST_02----非极大抑制 // image = imread(\"./images/church01.jpg\", 0); // transpose(image, image); // flip(image, image, 0); // // keypoints.clear(); // //检测关键点 // ptrFAST-\u003esetNonmaxSuppression(false); // ptrFAST-\u003edetect(image, keypoints); // //画出关键点 // drawKeypoints(image, keypoints, image, Scalar(255, 255, 255), DrawMatchesFlags::DRAW_OVER_OUTIMG); // imshow(\"FAST FEATURES(ALL)\", image); // //FAST_03----Grid // image = imread(\"./images/church01.jpg\", 0); // transpose(image, image); // flip(image, image, 0); // // int total(100); // int hstep(5), vstep(3); // int hsize(image.cols / hstep), vsize(image.rows / vstep); // int subtotal(total / (hstep * vstep)); // // Mat imageROI; // vector\u003cKeyPoint\u003e gridpoints; // cout \u003c\u003c \"Grid of\" \u003c\u003c vstep \u003c\u003c \"by\" \u003c\u003c hstep \u003c\u003c \"each of size\" \u003c\u003c vsize \u003c\u003c \"by\" \u003c\u003c hsize \u003c\u003c endl; // // //检测低阈值 // ptrFAST-\u003esetThreshold(20); // //非极大抑制 // ptrFAST-\u003esetNonmaxSuppression(true); // //最终的关键点容器 // keypoints.clear(); // //检测每一个网格 // for (int i = 0; i \u003c vstep; i++) // { // for","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:8","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_08--检测兴趣点"},{"categories":["OpenCV"],"content":" Chapter_09 : 描述和匹配兴趣点 1.局部模板匹配—-两个图像的匹配和找一样的模板 2.描述并匹配局部强度值模式—-SURF和SIFT 3.用二值描述子匹配关键点 //#include \u003ciostream\u003e //#include \u003copencv2/core.hpp\u003e //#include \u003copencv2/imgproc.hpp\u003e //#include \u003copencv2/highgui.hpp\u003e //#include \u003copencv2/features2d.hpp\u003e //#include \u003copencv2/objdetect.hpp\u003e //#include \u003copencv2/xfeatures2d.hpp\u003e // //using namespace std; //using namespace cv; // //int main() //{ // ////1.局部模板匹配 ---- 两个图像的匹配 // //Mat image1 = imread(\"./images/church01.jpg\", CV_LOAD_IMAGE_GRAYSCALE); // //Mat image2 = imread(\"./images/church02.jpg\", CV_LOAD_IMAGE_GRAYSCALE); // // //imshow(\"image1\", image1); // //imshow(\"image2\", image2); // ////定义关键点容器 // //vector\u003cKeyPoint\u003e keypoints1; // //vector\u003cKeyPoint\u003e keypoints2; // ////定义特征检测器 // //Ptr\u003cFeatureDetector\u003e ptrDetector; //泛型检测器指针 // ////这里使用FAST检测器 // //ptrDetector = FastFeatureDetector::create(80); // ////检测关键点 // //ptrDetector-\u003edetect(image1, keypoints1); // //ptrDetector-\u003edetect(image2, keypoints2); // // //std::cout \u003c\u003c \"Number of keypoints (image 1): \" \u003c\u003c keypoints1.size() \u003c\u003c std::endl; // //std::cout \u003c\u003c \"Number of keypoints (image 2): \" \u003c\u003c keypoints2.size() \u003c\u003c std::endl; // // ////定义一个特定大小的矩形（11 * 11）， 用于表示每个关键点周围的图像块 // ////定义正方形的领域 // //const int nsize(11); // //Rect neighborhood(0, 0, nsize, nsize); //11 * 11 // //Mat patch1; // //Mat patch2; // ////将一副图像的关键点与另一幅图像的所有关键点进行比较，找出最相似的 // ////在第二幅图像中找出与第一幅图像中的每个关键点最匹配的 // //Mat result; // //vector\u003cDMatch\u003e matches; // ////针对图像一的全部关键点 // //for (int i = 0; i \u003c keypoints1.size(); i++) // //{ // // //定义图像块 // // neighborhood.x = keypoints1[i].pt.x - nsize / 2; // // neighborhood.y = keypoints1[i].pt.y - nsize / 2; // // //如果领域超出图像范围，就继续处理下一个点 // // if (neighborhood.x \u003c 0 || neighborhood.y \u003c 0 || // // neighborhood.x + nsize \u003e= image1.cols || neighborhood.y + nsize \u003e= image1.rows) // // continue; // // //第一幅图像的块 // // patch1 = image1(neighborhood); // // //存放最匹配的值 // // DMatch bestMatch; // // //针对第二幅图像的全部关键点 // // for (int j = 0; j \u003c keypoints2.size(); j++) // // { // // //定义图像块 // // neighborhood.x = keypoints2[j].pt.x - nsize / 2; // // neighborhood.y = keypoints2[j].pt.y - nsize / 2; // // // //如果领域超出图像范围，就继续处理下一个点 // // if (neighborhood.x \u003c 0 || neighborhood.y \u003c 0 || // // neighborhood.x + nsize \u003e= image2.cols || neighborhood.y + nsize \u003e= image2.rows) // // continue; // // //第二幅图像的块 // // patch2 = image2(neighborhood); // // // //匹配两个图像块 // // matchTemplate(patch1, patch2, result, TM_SQDIFF); // // //cv::matchTemplate(patch1, patch2, result, cv::TM_SQDIFF); // // //检查是否为最佳匹配 // // if (result.at\u003cfloat\u003e(0, 0) \u003c bestMatch.distance) // // { // // bestMatch.distance = result.at \u003cfloat\u003e(0, 0); // // bestMatch.queryIdx = i; // // bestMatch.trainIdx = j; // // } // // } // // //添加最佳匹配 // // matches.push_back(bestMatch); // //} // // //std::cout \u003c\u003c \"Number of matches: \" \u003c\u003c matches.size() \u003c\u003c std::endl; // ////提取50个最佳匹配项 // //nth_element(matches.begin(), matches.begin() + 50, matches.end()); // //matches.erase(matches.begin() + 50, matches.end()); // // //std::cout \u003c\u003c \"Number of matches (after): \" \u003c\u003c matches.size() \u003c\u003c std::endl; // // ////画出匹配结果 // //Mat matchImage; // //drawMatches(image1, keypoints1, image2, keypoints2, matches, matchImage, Scalar(255, 255, 255), Scalar(255, 255, 255)); // // //imshow(\"Matches\", matchImage); // // ////模板匹配----寻找一样的部分 // ////定义一个模板 // //Mat target(image1, Rect(80, 105, 30, 30)); // ////展示模板 // //imshow(\"Template\", target); // ////定义搜索区域-----这里用图像的上半部分 // //Mat roi(image2, Rect(0, 0, image2.cols, image2.rows / 2)); // ////进行模板匹配 // //matchTemplate(roi, //搜索区域 // // target, //模板 // // result, //结果 // // TM_SQDIFF); //相似度 // ////找到最相似的位置 // //double minVal, maxVal; // //Point minPt, maxPt; // //minMaxLoc(result, \u0026minVal, \u0026maxVal, \u0026minPt, \u0026maxPt); // ////在相似度最高的位置画矩形框 // ////本例为minPt // //rectangle(roi, Rect(minPt.x, minPt.y, target.cols, target.rows), 255); // ////展示模板 // //imshow(\"Best\", image2); // // // //////2.描述并匹配局部强度值模式----SURF和SIFT // ////SURF // ////读图片 // //Mat image1 = imread(\"./images/church01.jpg\", IMREAD_","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:9","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_09--描述和匹配兴趣点"},{"categories":["OpenCV"],"content":" Chapter_10 : 估算图像之间的投影关系 1.计算图像对的基础矩阵 2.用RANSAC算法匹配图像 3.计算两幅图像之间的单应矩阵—-找到对应的点和拼接两幅图像 4.检测图像中的平面目标 //#include \u003ciostream\u003e //#include \u003cvector\u003e //#include \u003copencv2/core.hpp\u003e //#include \u003copencv2/imgproc.hpp\u003e //#include \u003copencv2/highgui.hpp\u003e //#include \u003copencv2/features2d.hpp\u003e //#include \u003copencv2/calib3d.hpp\u003e //#include \u003copencv2/objdetect.hpp\u003e //#include \u003copencv2/xfeatures2d.hpp\u003e //#include \u003copencv2/stitching.hpp\u003e // //#include \"RobustMatcher.h\" //#include \"TargetMatcher.h\" // //using namespace std; //using namespace cv; // //int main() //{ // ////1.计算图像对的基础矩阵 // //Mat image1 = imread(\"./images/church01.jpg\", 0); // //Mat image2 = imread(\"./images/church03.jpg\", 0); // //if (!image1.data || !image2.data) // // return 0; // // //// Display the images // //cv::namedWindow(\"Right Image\"); // //cv::imshow(\"Right Image\", image1); // //cv::namedWindow(\"Left Image\"); // //cv::imshow(\"Left Image\", image2); // // ////定义关键点容器和描述子、 // //vector\u003cKeyPoint\u003e keypoints1; // //vector\u003cKeyPoint\u003e keypoints2; // //Mat descriptors1, descriptors2; // ////构建SIFT特征检测器 // //Ptr\u003cFeature2D\u003e ptrFeature2D = xfeatures2d::SIFT::create(74); // // //ptrFeature2D-\u003edetectAndCompute(image1, noArray(), keypoints1, descriptors1); // //ptrFeature2D-\u003edetectAndCompute(image2, noArray(), keypoints2, descriptors2); // // //std::cout \u003c\u003c \"Number of SIFT points (1): \" \u003c\u003c keypoints1.size() \u003c\u003c std::endl; // //std::cout \u003c\u003c \"Number of SIFT points (2): \" \u003c\u003c keypoints2.size() \u003c\u003c std::endl; // // ////画关键点 // //cv::Mat imageKP; // //cv::drawKeypoints(image1, keypoints1, imageKP, cv::Scalar(255, 255, 255), cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS); // //cv::namedWindow(\"Right SIFT Features\"); // //cv::imshow(\"Right SIFT Features\", imageKP); // //cv::drawKeypoints(image2, keypoints2, imageKP, cv::Scalar(255, 255, 255), cv::DrawMatchesFlags::DRAW_RICH_KEYPOINTS); // //cv::namedWindow(\"Left SIFT Features\"); // //cv::imshow(\"Left SIFT Features\", imageKP); // // ////构建匹配类的实例 // //BFMatcher matcher(NORM_L2, true); // ////匹配描述子 // //vector\u003cDMatch\u003e matches; // //matcher.match(descriptors1, descriptors2, matches); // //std::cout \u003c\u003c \"Number of matched points: \" \u003c\u003c matches.size() \u003c\u003c std::endl; // ////手动的选择一些匹配的描述子 // //vector\u003cDMatch\u003e selMatches; // //// make sure to double-check if the selected matches are valid // //selMatches.push_back(matches[2]); // //selMatches.push_back(matches[5]); // //selMatches.push_back(matches[16]); // //selMatches.push_back(matches[19]); // //selMatches.push_back(matches[14]); // //selMatches.push_back(matches[34]); // //selMatches.push_back(matches[29]); // // ////画出选择的描述子 // //cv::Mat imageMatches; // //cv::drawMatches(image1, keypoints1, // 1st image and its keypoints // // image2, keypoints2, // 2nd image and its keypoints // // selMatches, // the selected matches // // imageMatches, // the image produced // // cv::Scalar(255, 255, 255), // // cv::Scalar(255, 255, 255), // // std::vector\u003cchar\u003e(), // // 2 // //); // color of the lines // //cv::namedWindow(\"Matches\"); // //cv::imshow(\"Matches\", imageMatches); // ////将一维关键点转变为二维的点 // //vector\u003cint\u003e pointIndexes1; // //vector\u003cint\u003e pointIndexes2; // //for (vector\u003cDMatch\u003e::const_iterator it = selMatches.begin(); it != selMatches.end(); ++it) // //{ // // pointIndexes1.push_back(it-\u003equeryIdx); // // pointIndexes2.push_back(it-\u003etrainIdx); // //} // ////为了在findFundamentalMat中使用，需要先把这些关键点转化为Point2f类型 // //vector\u003cPoint2f\u003e selPoints1, selPoints2; // //KeyPoint::convert(keypoints1, selPoints1, pointIndexes1); // //KeyPoint::convert(keypoints2, selPoints2, pointIndexes2); // ////通过画点来检查 // //vector\u003cPoint2f\u003e ::const_iterator it = selPoints1.begin(); // //while (it != selPoints1.end()) // //{ // // //在每个角点位置画圆 // // circle(image1, *it, 3, Scalar(255, 255, 255), 2); // // ++it; // //} // //it = selPoints2.begin(); // //while (it != selPoints2.end()) { // // // // draw a circle at each corner location // // cv::circle(image2, *it, 3, cv::Scalar(255, 255, 255), 2); // // ++it; // //} // ////用7对匹配项计算基础矩阵 // //Mat fundamental = findFun","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:10","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_10--估算图像之间的投影关系"},{"categories":["OpenCV"],"content":" Chapter_11 : 三维重建 1.相机标定 2.相机姿态还原 3.用标定相机实现三维重建 4.计算立体图像的深度 //#include \u003ciostream\u003e //#include \u003ciomanip\u003e //#include \u003cvector\u003e // //#include \u003copencv2/core.hpp\u003e //#include \u003copencv2/imgproc.hpp\u003e //#include \u003copencv2/highgui.hpp\u003e //#include \u003copencv2/features2d.hpp\u003e // //#include \u003copencv2/viz.hpp\u003e //#include \u003copencv2/calib3d.hpp\u003e // //#include \"CameraCalibrator.h\" // //int main() //{ // ////1.相机标定 // //cv::Mat image; // //std::vector\u003cstd::string\u003e filelist; // // //// generate list of chessboard image filename // //// named chessboard01 to chessboard27 in chessboard sub-dir // //for (int i = 1; i \u003c= 27; i++) { // // // std::stringstream str; // // str \u003c\u003c \"images/chessboards/chessboard\" \u003c\u003c std::setw(2) \u003c\u003c std::setfill('0') \u003c\u003c i \u003c\u003c \".jpg\"; // // std::cout \u003c\u003c str.str() \u003c\u003c std::endl; // // // filelist.push_back(str.str()); // // image = cv::imread(str.str(), 0); // // // // cv::imshow(\"Board Image\",image); // // // cv::waitKey(100); // //} // // //// Create calibrator object // //CameraCalibrator cameraCalibrator; // //// add the corners from the chessboard // //cv::Size boardSize(7, 5); // //cameraCalibrator.addChessboardPoints( // // filelist, // filenames of chessboard image // // boardSize, \"Detected points\"); // size of chessboard // // // // calibrate the camera // //cameraCalibrator.setCalibrationFlag(true, true); // //cameraCalibrator.calibrate(image.size()); // // //// Exampple of Image Undistortion // //image = cv::imread(filelist[14], 0); // //cv::Size newSize(static_cast\u003cint\u003e(image.cols*1.5), static_cast\u003cint\u003e(image.rows*1.5)); // //cv::Mat uImage = cameraCalibrator.remap(image, newSize); // // //// display camera matrix // //cv::Mat cameraMatrix = cameraCalibrator.getCameraMatrix(); // //std::cout \u003c\u003c \" Camera intrinsic: \" \u003c\u003c cameraMatrix.rows \u003c\u003c \"x\" \u003c\u003c cameraMatrix.cols \u003c\u003c std::endl; // //std::cout \u003c\u003c cameraMatrix.at\u003cdouble\u003e(0, 0) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(0, 1) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(0, 2) \u003c\u003c std::endl; // //std::cout \u003c\u003c cameraMatrix.at\u003cdouble\u003e(1, 0) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(1, 1) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(1, 2) \u003c\u003c std::endl; // //std::cout \u003c\u003c cameraMatrix.at\u003cdouble\u003e(2, 0) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(2, 1) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(2, 2) \u003c\u003c std::endl; // // //cv::namedWindow(\"Original Image\"); // //cv::imshow(\"Original Image\", image); // //cv::namedWindow(\"Undistorted Image\"); // //cv::imshow(\"Undistorted Image\", uImage); // // //// Store everything in a xml file // //cv::FileStorage fs(\"calib.xml\", cv::FileStorage::WRITE); // //fs \u003c\u003c \"Intrinsic\" \u003c\u003c cameraMatrix; // //fs \u003c\u003c \"Distortion\" \u003c\u003c cameraCalibrator.getDistCoeffs(); // // // // // // Read the camera calibration parameters // cv::Mat cameraMatrix; // cv::Mat cameraDistCoeffs; // cv::FileStorage fs(\"calib.xml\", cv::FileStorage::READ); // fs[\"Intrinsic\"] \u003e\u003e cameraMatrix; // fs[\"Distortion\"] \u003e\u003e cameraDistCoeffs; // std::cout \u003c\u003c \" Camera intrinsic: \" \u003c\u003c cameraMatrix.rows \u003c\u003c \"x\" \u003c\u003c cameraMatrix.cols \u003c\u003c std::endl; // std::cout \u003c\u003c cameraMatrix.at\u003cdouble\u003e(0, 0) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(0, 1) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(0, 2) \u003c\u003c std::endl; // std::cout \u003c\u003c cameraMatrix.at\u003cdouble\u003e(1, 0) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(1, 1) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(1, 2) \u003c\u003c std::endl; // std::cout \u003c\u003c cameraMatrix.at\u003cdouble\u003e(2, 0) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(2, 1) \u003c\u003c \" \" \u003c\u003c cameraMatrix.at\u003cdouble\u003e(2, 2) \u003c\u003c std::endl \u003c\u003c std::endl; // cv::Matx33d cMatrix(cameraMatrix); // // // Input image points // std::vector\u003ccv::Point2f\u003e imagePoints; // imagePoints.push_back(cv::Point2f(136, 113)); // imagePoints.push_back(cv::Point2f(379, 114)); // imagePoints.push_back(cv::Point2f(379, 150)); // imagePoints.push_back(cv::Point2f(138, 135)); // imagePoints.push_back(cv::Point2f(143, 146)); // imagePoints.push_back(cv::Point2f(381, 166)); // imagePoints.push_back(cv::Point2f(345, 194)); // imagePoints.push_back(cv::Point2f(103, 161)); // // // Input object points // std::vector\u003ccv::Point3f\u003e objectPoints; // objectPoints.push_back(cv::","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:11","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_11-----三维重建"},{"categories":["OpenCV"],"content":" Chapter_12 : 处理视频序列 1.读取视频处理 2.处理视频帧 3.提取视频中的眼前物体 #include \u003copencv2/bgsegm.hpp\u003e #include \"VideoProcessor.h\" #include \"BGFGSegmentor.h\" void draw(const cv::Mat\u0026 img, cv::Mat\u0026 out) { img.copyTo(out); cv::circle(out, cv::Point(100, 100), 5, cv::Scalar(255, 0, 0), 2); } void canny(cv::Mat\u0026 img, cv::Mat\u0026 out) { // 转换为灰度 if (img.channels() == 3) cv::cvtColor(img, out, cv::COLOR_BGR2GRAY); // 计算Canny边缘 cv::Canny(out, out, 100, 200); // 反转图像 cv::threshold(out, out, 128, 255, cv::THRESH_BINARY_INV); } int main() { ////1.读取视频处理 // //打开视频文件 //VideoCapture capture(\"./images/bike.avi\"); ////检查视频是否打开成功 //if (!capture.isOpened() ) // return 1; ////取得帧速率 //double rate = capture.get(CV_CAP_PROP_FPS); //bool stop(false); //Mat frame; //当前视频帧 //namedWindow(\"Extracted Frame\"); ////根据帧速率计算帧之间的等待时间， 单位为ms //int delay = 1000 / rate; ////循环遍历视频中的全部帧 //while (!stop) //{ // //读取下一帧（如果有） // if (!capture.read(frame) ) // break; // imshow(\"Extracted Frame\", frame); // //等待一段时间，或者通过按键停止 // if (waitKey(delay) \u003e= 0) // stop = true; // //} ////关闭视频文件 ////不是必须的，因为类的析构函数会调用 //capture.release(); ////2.处理视频帧 ////打开视频文件 //VideoCapture capture(\"./images/bike.avi\"); ////检查视频是否打开 //if (!capture.isOpened()) // return 1; ////获取帧速率 //double rate = capture.get(CV_CAP_PROP_FPS); //cout \u003c\u003c \"Frame rate: \" \u003c\u003c rate \u003c\u003c \"fps\" \u003c\u003c endl; //double stop(false); //Mat frame; ////根据帧速率计算镇之间的等待时间，单位为ms //int delay = 1000 / rate; //long long i = 0; //string b = \"bike\"; //string ext = \".bmp\"; ////循环遍历视频中的全部帧 //while (!stop) //{ // //读取下一帧（如果有） // if (!capture.read(frame)) // break; // imshow(\"Extracted Frame\", frame); // string name(b); // ostringstream ss; // ss \u003c\u003c setfill('0') \u003c\u003c setw(3) \u003c\u003c i; // name += ss.str(); // i++; // name += ext; // cout \u003c\u003c name \u003c\u003c endl; // Mat test; // //等待一段时间， 或者通过按键停止 // if (waitKey(delay) \u003e= 0) // stop = true; //} //capture.release(); //waitKey(); ////现在创建一个自定义类VideoProcessor完整的封装了视频处理任务 //VideoProcessor processor; ////打开视频文件 //processor.setInput(\"./images/bike.avi\"); ////声明显示视频的窗口 //processor.displayInput(\"Input Video\"); //processor.displayOutput(\"Output Video\"); ////用原始帧速率播放视频 //processor.setDelay(1000. / processor.getFrameRate()); ////设置处理帧的回调函数 //processor.setFrameProcessor(canny); ////输出视频 //processor.setOutput(\"./images/bikeCanny.avi\", -1, 15); ////在当前帧停止处理 //processor.stopAtFrameNo(51); ////开始处理 //processor.run(); //waitKey(); //3.提取视频中的眼前物体 // 打开视频 cv::VideoCapture capture(\"./images/bike.avi\"); // 检查是否打开成功 if (!capture.isOpened()) return 0; // 当前视频帧 cv::Mat frame; // 前景的二值图像 cv::Mat foreground; // 背景图 cv::Mat background; cv::namedWindow(\"Extracted Foreground\"); // 混合高斯模型类的对象，全部采用默认参数 cv::Ptr\u003ccv::BackgroundSubtractor\u003e ptrMOG = cv::bgsegm::createBackgroundSubtractorMOG(); bool stop(false); // 遍历视频中的所有帧 while (!stop) { // 读取下一帧（如果有） if (!capture.read(frame)) break; // 更新背景并返回前景 ptrMOG-\u003eapply(frame, foreground, 0.01); // 改进图像效果 cv::threshold(foreground, foreground, 128, 255, cv::THRESH_BINARY_INV); // 显示前景 cv::imshow(\"Extracted Foreground\", foreground); // 产生延时或者按键结束 if (cv::waitKey(10) \u003e= 0) stop = true; } cv::waitKey(); //// Create video procesor instance //VideoProcessor processor; //// Create background/foreground segmentor //BGFGSegmentor segmentor; //segmentor.setThreshold(25); //// Open video file //processor.setInput(\"./images/bike.avi\"); //// set frame processor //processor.setFrameProcessor(\u0026segmentor); //// Declare a window to display the video //processor.displayOutput(\"Extracted Foreground\"); //// Play the video at the original frame rate //processor.setDelay(1000. / processor.getFrameRate()); //// Start the process //processor.run(); //cv::waitKey(); return 0; } #pragma once #if !defined VPROCESSOR #define VPROCESSOR #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; //帧处理的接口 class FrameProcessor { public: //处理方法 virtual void process(Mat \u0026input, Mat \u0026output) = 0; }; class VideoProcessor { private: //Opencv视频捕获对象 VideoCapture capture; //处理每一帧时都会调用的回调函数 void(*process) (Mat\u0026, Mat\u0026); FrameProcessor *fr","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:12","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_12--处理视频序列"},{"categories":["OpenCV"],"content":" Chapter_13 : 跟踪运动物体 1.跟踪视频中的特征点 2.估算光流 3.跟踪视频中的物体 #include \"FeatureTracker.h\" #include \"VideoProcessor.h\" #include \"VisualTracker.h\" //绘制光流向量图 void drawOpticalFlow(const Mat\u0026 oflow, //光流 Mat \u0026flowImage, //绘制的图像 int stride, //显示向量的步长 float scale, //放大因子 const Scalar\u0026 color) //显示向量的颜色 { //必要时创建图像 if (flowImage.size() != oflow.size()) { flowImage.create(oflow.size(), CV_8UC3); flowImage = Vec3i(255, 255, 255); } //对所有向量， 以stride作为步长 for (int y = 0; y \u003c oflow.rows; y += stride) { for (int x = 0; x \u003c oflow.cols; x += stride) { // 获取向量 Point2f vector = oflow.at\u003c Point2f\u003e(y, x); // 画线条 line(flowImage, Point(x, y), Point(static_cast\u003cint\u003e(x + scale*vector.x + 0.5), static_cast\u003cint\u003e(y + scale*vector.y + 0.5)), color); // 画顶端圆圈 circle(flowImage, Point(static_cast\u003cint\u003e(x + scale*vector.x + 0.5), static_cast\u003cint\u003e(y + scale*vector.y + 0.5)), 1, color, -1); } } } int main() { ////1.跟踪视频中的特征点 ////创建视频处理类的实例 //VideoProcessor processor; ////创建特征跟踪类的实例 //FeatureTracker tracker; ////打开视频文件 //processor.setInput(\"./images/bike.avi\"); ////设置帧处理类 //processor.setFrameProcessor(\u0026tracker); ////声明显示视频的窗口 //processor.displayOutput(\"Tracked Feature\"); ////以原始帧速率播放视频 //processor.setDelay(1000. / processor.getFrameRate()); //processor.stopAtFrameNo(90); ////开始处理 //processor.run(); // //waitKey(); ////2.估算光流 //Mat frame1 = imread(\"./images/goose/goose230.bmp\", 0); //Mat frame2 = imread(\"./images/goose/goose237.bmp\", 0); //imshow(\"frame1\", frame1); //imshow(\"frame2\", frame2); //// Combined display //Mat combined(frame1.rows, frame1.cols + frame2.cols, CV_8U); //frame1.copyTo(combined.colRange(0, frame1.cols)); //frame2.copyTo(combined.colRange(frame1.cols, frame1.cols + frame2.cols)); //imshow(\"Frames\", combined); //// 创建光流算法 //Ptr\u003cDualTVL1OpticalFlow\u003e tvl1 = createOptFlow_DualTVL1(); //cout \u003c\u003c \"regularization coeeficient: \" \u003c\u003c tvl1-\u003egetLambda() \u003c\u003c endl; // the smaller the soomther //cout \u003c\u003c \"Number of scales: \" \u003c\u003c tvl1-\u003egetScalesNumber() \u003c\u003c endl; // number of scales //cout \u003c\u003c \"Scale step: \" \u003c\u003c tvl1-\u003egetScaleStep() \u003c\u003c endl; // size between scales //cout \u003c\u003c \"Number of warpings: \" \u003c\u003c tvl1-\u003egetWarpingsNumber() \u003c\u003c endl; // size between scales //cout \u003c\u003c \"Stopping criteria: \" \u003c\u003c tvl1-\u003egetEpsilon() \u003c\u003c \" and \" \u003c\u003c tvl1-\u003egetOuterIterations() \u003c\u003c endl; // size between scales // // compute the optical flow between 2 frames //Mat oflow; // 二维光流向量的图像 //// 计算frame1和frame2之间的光流 //tvl1-\u003ecalc(frame1, frame2, oflow); //// 绘制光流 //Mat flowImage; //drawOpticalFlow(oflow, // 输入光流向量 // flowImage, // 生成的图像 // 8, // 每隔8个像素显示一个向量 // 2, // 长度延长2倍 // Scalar(0, 0, 0)); // 向量颜色 //imshow(\"Optical Flow\", flowImage); //// 计算两个帧之间更加光滑的光流 //tvl1-\u003esetLambda(0.075); //tvl1-\u003ecalc(frame1, frame2, oflow); //// Draw the optical flow image //Mat flowImage2; //drawOpticalFlow(oflow, // input flow vectors // flowImage2, // image to be generated // 8, // display vectors every 8 pixels // 2, // multiply size of vectors by 2 // Scalar(0, 0, 0)); // vector color //imshow(\"Smoother Optical Flow\", flowImage2); // waitKey(); //3.跟踪视频中的物体 // 创建视频处理器实例 VideoProcessor processor; // 生成文件名 std::vector\u003cstd::string\u003e imgs; std::string prefix = \"images/goose/goose\"; std::string ext = \".bmp\"; // 添加用于跟踪的图像名称 for (long i = 130; i \u003c 317; i++) { string name(prefix); ostringstream ss; ss \u003c\u003c setfill('0') \u003c\u003c setw(3) \u003c\u003c i; name += ss.str(); name += ext; cout \u003c\u003c name \u003c\u003c endl; imgs.push_back(name); } // 创建特征提取器实例 Ptr\u003cTrackerMedianFlow\u003e ptr = TrackerMedianFlow::create(); VisualTracker tracker(ptr); // VisualTracker tracker(TrackerKCF::createTracker()); // 打开视频文件 processor.setInput(imgs); // 设置帧处理器 processor.setFrameProcessor(\u0026tracker); // 声明显示视频的窗口 processor.displayOutput(\"Tracked object\"); // 定义显示的帧速率 processor.setDelay(50); // 指定初始目标位置 cv::Rect bb(290, 100, 65, 40); tracker.setBoundingBox(bb); // 开始跟踪 processor.run(); cv::waitKey(); // 中值流量跟踪算法 Mat image1 = imread(\"./images/goose/goose130.bmp\", ImreadModes::IMREAD_GRAYSCALE); // 定义一个10 * 10 的网格 vector\u003cPoint2f\u003e grid; for (int i = 0; i \u003c 10; i++) { for (int j = 0; j \u003c 10; j++) { Po","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:13","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_13-----跟踪运动物体"},{"categories":["OpenCV"],"content":" Chapter_14 : 实用案列 1.用最邻近局部二值模式实现人脸识别 2.通过级联Haar特征实现物体和人脸定位 3.用支持向量机和方向梯度直方图实现物体与行人检测 #include \u003ciostream\u003e #include \u003copencv2/core.hpp\u003e #include \u003copencv2/highgui.hpp\u003e #include \u003copencv2/imgproc.hpp\u003e #include \u003copencv2/face.hpp\u003e #include \u003ciomanip\u003e #include \u003copencv2/objdetect.hpp\u003e #include \u003copencv2/ml.hpp\u003e using namespace std; using namespace cv; //1.用最邻近局部二值模式实现人脸识别 //计算灰度图像的局部二值模式 void lbp(const Mat \u0026image, Mat \u0026result) { assert(image.channels() == 1); // input image must be gray scale result.create(image.size(), CV_8U); // 必要时分配空间 // 逐行处理，除了第一行和最后一行 for (int j = 1; j\u003cimage.rows - 1; j++) { // 输入行的指针 const uchar* previous = image.ptr\u003cconst uchar\u003e(j - 1); const uchar* current = image.ptr\u003cconst uchar\u003e(j); const uchar* next = image.ptr\u003cconst uchar\u003e(j + 1); uchar* output = result.ptr\u003cuchar\u003e(j); // 输出行 for (int i = 1; i\u003cimage.cols - 1; i++) { // 构建局部二值模式 *output = previous[i - 1] \u003e current[i] ? 1 : 0; *output |= previous[i] \u003e current[i] ? 2 : 0; *output |= previous[i + 1] \u003e current[i] ? 4 : 0; *output |= current[i - 1] \u003e current[i] ? 8 : 0; *output |= current[i + 1] \u003e current[i] ? 16 : 0; *output |= next[i - 1] \u003e current[i] ? 32 : 0; *output |= next[i] \u003e current[i] ? 64 : 0; *output |= next[i + 1] \u003e current[i] ? 128 : 0; output++; // 下一个像素 } } // 未处理的设置为0 result.row(0).setTo(Scalar(0)); result.row(result.rows - 1).setTo(Scalar(0)); result.col(0).setTo(Scalar(0)); result.col(result.cols - 1).setTo(Scalar(0)); } //3.用支持向量机和方向梯度直方图实现物体与行人检测 // 画出一个单元格的HOG void drawHOG(vector\u003cfloat\u003e::const_iterator hog, // HOG迭代器 int numberOfBins, // HOG中的箱子数量 Mat \u0026image, // 单元格的图像 float scale = 1.0) // 长度缩放比例 { const float PI = 3.1415927; float binStep = PI / numberOfBins; float maxLength = image.rows; float cx = image.cols / 2.; float cy = image.rows / 2.; // 逐个箱子 for (int bin = 0; bin \u003c numberOfBins; bin++) { // 箱子方向 float angle = bin*binStep; float dirX = cos(angle); float dirY = sin(angle); // 线条长度， 与箱子大小成正比 float length = 0.5*maxLength* *(hog + bin); // 画线条 float x1 = cx - dirX * length * scale; float y1 = cy - dirY * length * scale; float x2 = cx + dirX * length * scale; float y2 = cy + dirY * length * scale; line(image, Point(x1, y1), Point(x2, y2), CV_RGB(255, 255, 255), 1); } } // 在图像上绘制 HOG void drawHOGDescriptors(const Mat \u0026image, // 输入图像 Mat \u0026hogImage, // 结果HOG图像 Size cellSize, // 每个单元格的大小（忽略区块） int nBins) // 箱子数量 { // 区块大小等于图像大小 HOGDescriptor hog(Size( (image.cols / cellSize.width) * cellSize.width,(image.rows / cellSize.height) * cellSize.height), Size( (image.cols / cellSize.width) * cellSize.width,(image.rows / cellSize.height) * cellSize.height), cellSize, // 区块步长（这里只有一个区块） cellSize, // 单元格大小 nBins); // 箱子数量 // 计算 HOG vector\u003cfloat\u003e descriptors; hog.compute(image, descriptors); float scale = 2.0 / * max_element(descriptors.begin(), descriptors.end()); hogImage.create(image.rows, image.cols, CV_8U); vector\u003cfloat\u003e::const_iterator itDesc = descriptors.begin(); for (int i = 0; i \u003c image.rows / cellSize.height; i++) { for (int j = 0; j \u003c image.cols / cellSize.width; j++) { // 画出每个单元格 hogImage(Rect(j*cellSize.width, i*cellSize.height, cellSize.width, cellSize.height)); drawHOG(itDesc, nBins, hogImage(Rect(j*cellSize.width, i*cellSize.height,cellSize.width, cellSize.height)), scale); itDesc += nBins; } } } int main() { ////1.用最邻近局部二值模式实现人脸识别 //Mat image = imread(\"./images/girl.jpg\", IMREAD_GRAYSCALE); //imshow(\"Original image\", image); //Mat lbpImage; //lbp(image, lbpImage); //imshow(\"LBP image\", lbpImage); //Ptr\u003cface::FaceRecognizer\u003e recognizer = face::LBPHFaceRecognizer::create(1, //LBP模式的半径 // 8, //使用邻近像素的数量 // 8, 8, //网格大小 // 200.); //最邻近的距离阈值 ////参考图像和标签的向量 //vector\u003cMat\u003e referenceImages; //vector\u003cint\u003e labels; ////打开参考图像 //referenceImages.push_back(imread(\"./images/face0_1.png\", IMREAD_GRAYSCALE)); //labels.push_back(0); // 编号为0的人 //referenceImages.push_back(imread(\"./images/face0_2.png\", IMREAD_GRAYSCALE)); //labels.push_back(0); // 编号为0的人 //referenceImages.push_back(imread(\"./images/face0_2.png\", IMREAD_GRAYSCALE)); //labels.push_back(0); /","date":"2019-03-28","objectID":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/:0:14","series":null,"tags":["OpenCV"],"title":"OpenCV计算机视觉编程攻略","uri":"/opencv%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%BC%96%E7%A8%8B%E6%94%BB%E7%95%A5/#chapter_14------实用案列"},{"categories":["OpenCV"],"content":" 计算两个矩阵的相似性 画三角形、圆形检测出来 Haar特征与Viola-Joines人脸检测 基于OpenCV的运动物体追踪 基于Opencv的数字图像识别 ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bpractice/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV之Practice","uri":"/opencv%E4%B9%8Bpractice/#"},{"categories":["OpenCV"],"content":" 计算两个矩阵的相似性 //计算两个图的相似性 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; Mat srcImage, dstImage; // 函数说明 // 这个函数是用来返回两个黑白图片的相似度的 // 输入是两个图片，输出是相似度 float isSimilar(Mat \u0026src, Mat \u0026dst) { // 先定义两个图片的矩阵 Mat src_copy, dst_copy; // resize(dst, src.row, src.col); // 讲两个图片缩放到src的尺寸上，并且用心的矩阵来储存 resize(src, src_copy, src.size(), 0, 0, INTER_LINEAR); resize(dst, dst_copy, src.size(), 0, 0, INTER_LINEAR); float same = 0, diff = 0; // 两层for循环访问所有的元素点 for (int i = 0; i \u003c src_copy.rows; i++) { for (int j = 0; j \u003c src_copy.cols; j++) { // 如果像素相同，那么same加1 // 这里的代码明明是学习笔记上的，要好好看啊 if (src_copy.at\u003cuchar\u003e(i, j) == dst_copy.at\u003cuchar\u003e(i, j)) same += 1; // 如果像素不同，那么diff加1 else diff += 1; } } // 返回相似度 return same / (same + diff); } int main() { // 在main函数里面读入两个图片 srcImage = imread(\"8.png\"); dstImage = imread(\"9.png\"); // 对两个图片转换成灰度 cvtColor(srcImage, srcImage, CV_BGR2GRAY); cvtColor(dstImage, dstImage, CV_BGR2GRAY); // 将两个图片转成黑白图，阈值化 threshold(srcImage, srcImage, 100, 255, CV_THRESH_BINARY); threshold(dstImage, dstImage, 100, 255, CV_THRESH_BINARY); // 定义一个float float sim = 0; sim = isSimilar(srcImage, dstImage); cout \u003c\u003c \" the sim is \" \u003c\u003c sim \u003c\u003c endl; system(\"pause\"); waitKey(0); return 0; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bpractice/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之Practice","uri":"/opencv%E4%B9%8Bpractice/#计算两个矩阵的相似性"},{"categories":["OpenCV"],"content":" 画三角形、圆形检测出来 任务说明： 首先创建一个512×512的3通道的白色的图像 然后在这个图像上画三角形，四角形，五角形，六角形，圆，以及椭圆 对这个图像，使用霍夫曼检查直线和圆 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; //绘制椭圆 void drawEllipse(Mat img, double angle) { int thickness = 2; int lineType = 8; ellipse(img, Point(40, 40), Size(20, 40), angle, 0, 360, Scalar(255, 129, 0), thickness, lineType); } //绘制圆 void drawCircle(Mat img, Point center) { int thickness = 2; int lineType = 8; circle(img, center, 30, Scalar(168, 109, 255), thickness, lineType); } //画线 void drawLine(Mat img, Point start, Point end) { int thickness = 2; int lineType = 8; line(img, start, end, Scalar(0, 0, 0), thickness, lineType); } //绘制矩形 void drawRectangle(Mat img, Point pt1, Point pt2) { int thickness = 2; int lineType = 8; rectangle(img, pt1, pt2, Scalar(0, 255, 0), thickness, lineType); } //绘制三角形 void drawLine_Triangle(Mat img) { int thickness = 2; int lineType = 8; Point l1_start(200, 150), l1_end(50, 250); Point l2_start(50, 250), l2_end(400, 380); Point l3_start(400, 380), l3_end(200, 150); line(img, l1_start, l1_end, Scalar(0, 0, 0), thickness, lineType); line(img, l2_start, l2_end, Scalar(0, 0, 0), thickness, lineType); line(img, l3_start, l3_end, Scalar(0, 0, 0), thickness, lineType); } ////绘制多边形 //void drawfillPloy(Mat img) //{ // int lineType = 8; // // Point rookPoints[1][3]; // rookPoints[0][0] = Point(30, 30); // rookPoints[0][1] = Point(60, 60); // rookPoints[0][2] = Point(90, 90); // // Point *pts[1] = {rookPoints[0]}; //点序列的序列 // int npts[] = {3}; //pts[i]中点的数目 // int ncontours = 3; //pts中的序列数 // // fillPoly(img, pts, npts, ncontours, Scalar(168, 200, 120), lineType); //} //使用霍夫线检测出线 //Mat check_line(Mat img) //{ // Mat dstImage; // // cvtColor(img, dstImage, COLOR_BGR2GRAY); // adaptiveThreshold(dstImage, dstImage, 50, ADAPTIVE_THRESH_GAUSSIAN_C, THRESH_BINARY_INV, 10, 10); // // vector\u003cVec2f\u003e lines; // HoughLines(dstImage, lines, 1, CV_PI/180, 80, 0, 0); // // vector\u003cVec2f\u003e ::iterator itl = lines.begin(); // while (itl != lines.end()) // { // rectangle(img, Point((*itl)[0] + 5, (*itl)[1] + 5), Point((*itl)[2] + 5, (*itl)[3] + 5), 2, 8); // ++itl; // } // // imshow(\"check_lines\", img); // // return img; //} //使用霍夫线检测出线 Mat check_line(Mat\u0026 img) { Mat dstImage; cvtColor(img, dstImage, COLOR_BGR2GRAY); Canny(dstImage, dstImage, 150, 255, 3); //使用霍夫曼检测 //vector\u003cVec4f\u003e lines; // 注意这里的第二个参数 //HoughLines(dstImage, lines, 1, CV_PI/180.0, 80,50,10); // 以下修改为霍夫曼概率直线检测函数 // 注意这里的参数 vector\u003cVec4i\u003e lines; // 注意一下，使用霍夫曼概率直线检测与霍夫曼直线检测的区别 // 注意这里的第二个参数 HoughLinesP(dstImage, lines, 1, CV_PI / 180.0, 80, 50, 10); Scalar color = Scalar(0, 0, 255); for (size_t i = 0; i \u003c lines.size(); i++) { cout \u003c\u003c \" i is \" \u003c\u003c i \u003c\u003c \" \"; Vec4i hline = lines[i]; cout \u003c\u003c hline[0] \u003c\u003c \" ||| \" \u003c\u003c hline[1] \u003c\u003c \" ||| \" \u003c\u003c hline[2] \u003c\u003c \" |||| \" \u003c\u003c hline[3] \u003c\u003c endl; line(img, Point(hline[0], hline[1]), Point(hline[2], hline[3]), color, 3, LINE_AA); } imshow(\"check_lines\", img); return img; } //使用霍夫圆检测出圆形 Mat check_circle(Mat img) { Mat dstImage; cvtColor(img, dstImage, COLOR_BGR2GRAY); GaussianBlur(dstImage, dstImage, Size(7, 7), 1.5); vector\u003cVec3f\u003e circles; HoughCircles(dstImage, circles, CV_HOUGH_GRADIENT, 2, 30, 200, 100, 20, 80); /*vector\u003cVec3f\u003e ::iterator itc = circles.begin(); while (itc != circles.end()) { circle(img, Point((*itc)[0], (*itc)[1]), (*itc)[2] + 20, Scalar(0, 0, 255), 6); ++itc; }*/ for (size_t i = 0; i \u003c circles.size(); i++) { Point center(cvRound(circles[i][0]), cvRound(circles[i][1])); int r = cvRound(circles[i][2]); circle(img, center, r, Scalar(0, 0, 255), 3, 8, 0); } imshow(\"check_Circle\", img); return img; } int main() { Mat srcImage(512, 512, CV_8UC3 ,Scalar(255, 255, 255)); //imshow(\"srcImage\", srcImage); //绘制椭圆形 drawEllipse(srcImage, 30); //绘制圆形 Point center(300, 100); drawCircle(srcImage, center); //绘制线段 Point start(100, 100); Point end(200, 200); drawLine(srcImage, start, end); //绘制矩形 Point pt1(300, 300); //矩形的第一个顶点 Point pt2(500, 500); //矩形的对角线顶点 drawRectangle(srcImage, pt1, pt2); //","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bpractice/:0:2","series":null,"tags":["OpenCV"],"title":"OpenCV之Practice","uri":"/opencv%E4%B9%8Bpractice/#画三角形圆形检测出来"},{"categories":["OpenCV"],"content":" Haar特征与Viola-Joines人脸检测 //OpenCV3/C++ Haar特征与Viola-Joines人脸检测 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/objdetect.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImage; CascadeClassifier face_cacade; VideoCapture capture(0); if (!face_cacade.load(\"D:\\\\Project_OpenCV\\\\Cmake\\\\install\\\\etc\\\\haarcascades\\\\haarcascade_frontalface_alt.xml\")) { printf(\"can not load the file...\\n\"); return -1; } while (1) { capture \u003e\u003e srcImage; vector\u003cRect\u003e faces; Mat gray_image; cvtColor(srcImage, gray_image, CV_BGR2GRAY); //直方图均衡化 equalizeHist(gray_image, gray_image); face_cacade.detectMultiScale(gray_image, faces, 1.1, 2, 0 | CV_HAAR_SCALE_IMAGE, Size(1, 1)); //框选出脸部区域 for (int i = 0; i \u003c faces.size(); i++) { RNG rng(i); Scalar color = Scalar(rng.uniform(0, 255), rng.uniform(0, 255), 20); rectangle(srcImage, faces[static_cast\u003cint\u003e(i)], color, 2, 8, 0); } imshow(\"face\", srcImage); waitKey(30); //延时30 } Mat srcImage; srcImage = imread(\"7.jpg\"); CascadeClassifier face_cacade; VideoCapture capture(0); if (!face_cacade.load(\"D:\\\\Project_OpenCV\\\\Cmake\\\\install\\\\etc\\\\haarcascades\\\\haarcascade_frontalface_alt.xml\")) { printf(\"can not load the file...\\n\"); return -1; } vector\u003cRect\u003e faces; Mat gray_image; cvtColor(srcImage, gray_image, CV_BGR2GRAY); //直方图均衡化 equalizeHist(gray_image, gray_image); face_cacade.detectMultiScale(gray_image, faces, 1.1, 2, 0 | CV_HAAR_SCALE_IMAGE, Size(1, 1)); //框选出脸部区域 for (int i = 0; i \u003c faces.size(); i++) { RNG rng(i); Scalar color = Scalar(rng.uniform(0, 255), rng.uniform(0, 255), 20); rectangle(srcImage, faces[static_cast\u003cint\u003e(i)], color, 2, 8, 0); } imshow(\"face\", srcImage); waitKey(0); return 0; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bpractice/:0:3","series":null,"tags":["OpenCV"],"title":"OpenCV之Practice","uri":"/opencv%E4%B9%8Bpractice/#haar特征与viola-joines人脸检测"},{"categories":["OpenCV"],"content":" 基于OpenCV的运动物体追踪 给一段视频，程序对视频中运动的物体进行追踪，并用框框起来 ////1.运动物体检测----背景减法 //#include \u003ciostream\u003e //#include \u003copencv2/opencv.hpp\u003e //using namespace std; //using namespace cv; // ////运动物体检测函数声明 //Mat MoveDetect(Mat backgroung, Mat frame); // //int main() //{ // // VideoCapture video(\"move.avi\"); // if (!video.isOpened()) // { // printf(\"Oh, no, video is error\"); // return -1; // } // // int frameCount = video.get(CV_CAP_PROP_FRAME_COUNT); //获取帧数 // double FPS = video.get(CV_CAP_PROP_FPS);//获取FPS // Mat frame; //存储帧 // Mat background; //存储背景图像 // Mat result; //存储结果图像 // // for (int i = 0; i \u003c frameCount; i++) // { // video \u003e\u003e frame; //读帧进frame // imshow(\"frame\", frame); // if (frame.empty()) //对帧进行异常检测 // { // cout \u003c\u003c \"frame is empty\" \u003c\u003c endl; // break; // } // int framePosition = video.get(CV_CAP_PROP_POS_FRAMES); //获取帧位置(第几帧) // cout \u003c\u003c \"framePosition: \" \u003c\u003c framePosition \u003c\u003c endl; // if (framePosition == 1) //将第一帧作为背景图像 // background = frame.clone(); // result = MoveDetect(background, frame); //调用MoveDect()进行运动物体检测，返回值存入result // imshow(\"result\", result); // if (waitKey(1000.0 / FPS) == 27) //按原FPS显示 // { // cout \u003c\u003c \"ESC退出！\" \u003c\u003c endl; // break; // } // } // // // return 0; //} // //Mat MoveDetect(Mat background, Mat frame) //{ // Mat result = frame.clone(); // //1.将background和frame转为灰度图 // Mat gray1, gray2; // cvtColor(background, gray1, CV_BGR2GRAY); // cvtColor(frame, gray2, CV_BGR2GRAY); // //2.将background和frame做差 // Mat diff; // absdiff(gray1, gray2, diff); // imshow(\"diff\", diff); // //3. 对差值图diff_thresh进行阈值化处理 // Mat diff_thresh; // threshold(diff, diff_thresh, 50, 255, CV_THRESH_BINARY); // imshow(\"diff_thresh\", diff_thresh); // //4.腐蚀 // Mat kernel_erode = getStructuringElement(MORPH_RECT, Size(3, 3)); // Mat kernel_dilate = getStructuringElement(MORPH_RECT, Size(15, 15)); // erode(diff_thresh, diff_thresh, kernel_erode); // imshow(\"erode\", diff_thresh); // //5.膨胀 // dilate(diff_thresh, diff_thresh, kernel_dilate); // imshow(\"dilate\", diff_thresh); // //6.查找轮廓并绘制轮廓 // vector\u003cvector\u003cPoint\u003e\u003e contours; // findContours(diff_thresh, contours, CV_RETR_EXTERNAL, CV_CHAIN_APPROX_NONE); // drawContours(result, contours, -1, Scalar(0, 0, 255), 2);//在result上绘制轮廓 // //7.查找正外接矩形 // vector\u003cRect\u003e boundRect(contours.size()); // for (int i = 0; i \u003c contours.size(); i++) // { // boundRect[i] = boundingRect(contours[i]); // rectangle(result, boundRect[i], Scalar(0, 255, 0), 2); //在result上绘制正外接矩阵 // } // // return result; //返回result //} #include \"opencv2/opencv.hpp\" #include\u003ciostream\u003e using namespace std; using namespace cv; Mat MoveDetect(Mat frame1, Mat frame2) { Mat result = frame2.clone(); Mat gray1, gray2; cvtColor(frame1, gray1, CV_BGR2GRAY); cvtColor(frame2, gray2, CV_BGR2GRAY); Mat diff; absdiff(gray1, gray2, diff); imshow(\"absdiss\", diff); threshold(diff, diff, 45, 255, CV_THRESH_BINARY); imshow(\"threshold\", diff); Mat element = getStructuringElement(MORPH_RECT, Size(3, 3)); Mat element2 = getStructuringElement(MORPH_RECT, Size(25, 25)); erode(diff, diff, element); imshow(\"erode\", diff); dilate(diff, diff, element2); imshow(\"dilate\", diff); vector\u003cvector\u003cPoint\u003e\u003e contours; vector\u003cVec4i\u003e hierarcy; //画椭圆及中心 findContours(diff, contours, hierarcy, CV_RETR_EXTERNAL, CV_CHAIN_APPROX_NONE); cout \u003c\u003c \"num=\" \u003c\u003c contours.size() \u003c\u003c endl; vector\u003cRotatedRect\u003e box(contours.size()); for (int i = 0; i\u003ccontours.size(); i++) { box[i] = fitEllipse(Mat(contours[i])); ellipse(result, box[i], Scalar(0, 255, 0), 2, 8); circle(result, box[i].center, 3, Scalar(0, 0, 255), -1, 8); } return result; } void main() { VideoCapture cap(\"move.avi\"); if (!cap.isOpened()) //检查打开是否成功 return; Mat frame; Mat result; Mat background; int count = 0; while (1) { cap \u003e\u003e frame; if (frame.empty()) break; else { count++; if (count == 1) background = frame.clone(); //提取第一帧为背景帧 imshow(\"video\", frame); result = MoveDetect(background, frame); imshow(\"result\", result); if (waitKey(50) == 27) break; } } cap.release(); } ////2.运动物体检测----帧查法 //#include \u003ciostream\u003e //#include \u003copencv2/opencv.hpp\u003e ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bpractice/:0:4","series":null,"tags":["OpenCV"],"title":"OpenCV之Practice","uri":"/opencv%E4%B9%8Bpractice/#基于opencv的运动物体追踪"},{"categories":["OpenCV"],"content":" 基于Opencv的数字图像识别 准备： 1.10个模板图像 2.相似度检测函数、轮廓检测函数 具体内容： 1.读入10个模板图像 2.对目标图像进行轮廓检测并剪切出识别的数字 3.对剪切出的数字与10个模板图像相似度计算挑选出最相似的 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003cvector\u003e #include \u003calgorithm\u003e using namespace std; using namespace cv; Mat srcImage, dstImage; vector\u003cMat\u003e vec; vector\u003cMat\u003e tutu; //相似性函数 float isSimilar(Mat \u0026src, Mat \u0026dst) { // 先定义两个图片的矩阵 Mat src_copy, dst_copy; // resize(dst, src.row, src.col); // 讲两个图片缩放到src的尺寸上，并且用新的矩阵来储存 resize(src, src_copy, src.size(), 0, 0, INTER_LINEAR); resize(dst, dst_copy, src.size(), 0, 0, INTER_LINEAR); float same = 0, diff = 0; // 两层for循环访问所有的元素点 for (int i = 0; i \u003c src_copy.rows; i++) { for (int j = 0; j \u003c src_copy.cols; j++) { // 如果像素相同，那么same加1 // 这里的代码明明是学习笔记上的，要好好看啊 if (src_copy.at\u003cuchar\u003e(i, j) == dst_copy.at\u003cuchar\u003e(i, j)) same += 1; // 如果像素不同，那么diff加1 else diff += 1; } } // 返回相似度 return same / (same + diff); } //轮廓检测函数 void isContours(Mat \u0026src) { Mat img_gray; resize(src, img_gray, src.size(), 0, 0, INTER_LINEAR); cvtColor(src, img_gray, CV_BGR2GRAY); // 注意在取阈值的时候，最后一个参数 threshold(img_gray, dstImage, 100, 255, CV_THRESH_BINARY); vector\u003cvector\u003cPoint\u003e \u003e contours; vector\u003cVec4i\u003e hierarchy; // 注意这里的第四个参数，决定了检测的准确性 findContours(dstImage, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point(0, 0)); RNG rng(0); for (int i = 0; i \u003c contours.size(); i++) { Scalar color = Scalar(rng.uniform(0, 255), rng.uniform(0, 255), rng.uniform(0, 255)); Rect rect = boundingRect(contours[i]);//检测外轮廓 Mat temp = src(rect); imwrite(\"shacun\" + to_string(i) + \".png\", temp); rectangle(src, rect, Scalar(0, 0, 255), 3);//对外轮廓加矩形框 } } int main() { //调用轮廓检测函数 srcImage = imread(\"digits_4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return 0; } // 轮廓检测并保存数据 isContours(srcImage); //调用相似性函数 for (int i = 0; i \u003c 10; i++) { //模板10个图像 Mat s_src = imread(\"tutu\" + to_string(i) + \".png\"); vec.push_back(s_src); //切割的10个图像 Mat tu = imread(\"shacun\" + to_string(i) + \".png\"); tutu.push_back(tu); } for (int j = 0; j \u003c 10; j++) { int val = -1; float maxSim = 0; for (int i = 0; i \u003c 10; i++) { float sim = 0; sim = isSimilar(vec[i], tutu[j]); if (maxSim \u003c= sim) { maxSim = sim; val = i; } } imshow(\"aim is \", tutu[j]); imshow(\"model is \", vec[val]); waitKey(0); cout \u003c\u003c \" the val is \" \u003c\u003c val \u003c\u003c \" the sim is \" \u003c\u003c maxSim \u003c\u003c endl; } system(\"pause\"); return 0; } #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003cvector\u003e #include \u003calgorithm\u003e using namespace std; using namespace cv; Mat srcImage, dstImage; vector\u003cMat\u003e vec; /** 代码如何理解 从main函数开始，首先调用init函数 我们用init函数来加载模板图像到全局的数组vec里面 然后调用isContours函数，使用isContours函数对目标图片检测 检测完成后，接着就调用predict函数来预测轮廓检测的数数字 在predict函数里面，我们调用isSimilar函数来检测出最相似的模板图片 然后，输出对应的数字 */ // 函数声明 void init(); void isContours(Mat \u0026src); float isSimilar(Mat \u0026src, Mat \u0026dst); void predict(Mat \u0026dst); // 初始化加载模板函数 void init() { // 初始化函数，用来加载模板图片 for (int i = 0; i \u003c 10; i++) { //模板10个图像 Mat s_src = imread(\"tutu\" + to_string(i) + \".png\"); vec.push_back(s_src); } cout \u003c\u003c \"***************\" \u003c\u003c \"init is OK.\" \u003c\u003c \"***************\" \u003c\u003c endl; } // 预测函数 void predict(Mat \u0026dst) { // 检测数字函数 int val = -1; float maxSim = 0; for (int i = 0; i \u003c 10; i++) { // cvtColor(vec[i], vec[i], CV_BGR2GRAY); // threshold(vec[j], vec[j], 100, 255, CV_THRESH_BINARY); float sim = 0; sim = isSimilar(vec[i], dst); if (maxSim \u003c= sim) { maxSim = sim; val = i; } } imshow(\"aim is \", dst); imshow(\"model is \", vec[val]); waitKey(0); cout \u003c\u003c \" the val is \" \u003c\u003c val \u003c\u003c \" the sim is \" \u003c\u003c maxSim \u003c\u003c endl; } //轮廓检测函数 void isContours(Mat \u0026src) { Mat img_gray; resize(src, img_gray, src.size(), 0, 0, INTER_LINEAR); cvtColor(src, img_gray, CV_BGR2GRAY); // 注意在取阈值的时候，最后一个参数 threshold(img_gray, dstImage, 100, 255, CV_THRESH_BINARY); // imshow(\"thre\",dstImage); vector\u003cvector\u003cPoint\u003e \u003e contours; vector\u003cVec4i\u003e hierarchy; // 注意这里的第四个参数，决定了检测的准确性 findContours(dstImage, contours, hierarchy, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE, Point(0, 0)); RNG rng(0); for (int i = 0; i \u003c contours.size(); i++) { Scalar color = Scalar(rng.uniform(0, 255), ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bpractice/:0:5","series":null,"tags":["OpenCV"],"title":"OpenCV之Practice","uri":"/opencv%E4%B9%8Bpractice/#基于opencv的数字图像识别"},{"categories":["OpenCV"],"content":" 角点检测 特征检测与匹配 ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV之feature2d组件","uri":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/#"},{"categories":["OpenCV"],"content":" 角点检测 Harris角点检测 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; int main() { //以灰度模式载入图像并显示 Mat srcImage = imread(\"4.jpg\", 0); imshow(\"srcImage\", srcImage); //进行Harris角点检测找出角点 Mat cornerStrength; cornerHarris(srcImage, cornerStrength, 2, 3, 0.01); //对灰度图进行阈值操作，得到二值图并显示 Mat harrisCorner; threshold(cornerStrength, harrisCorner, 0.00001, 255, THRESH_BINARY); imshow(\"harrisCorner\", harrisCorner); waitKey(0); return 0; } #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; #define WINDOW_NAME1 \"dstImage\" #define WINDOW_NAME2 \"scaledImage\" Mat srcImage, dstImage, grayImage; int thresh = 30; //当前阈值 int max_thresh = 175; //最大阈值 static void on_CornerHarris(int , void *); void Show(); int main() { Show(); srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } imshow(\"srcImage\", srcImage); dstImage = srcImage.clone(); cvtColor(dstImage, grayImage, CV_BGR2GRAY); //imshow(\"grayImage\", grayImage); //创建窗口和滚动条 namedWindow(WINDOW_NAME1, CV_WINDOW_AUTOSIZE); createTrackbar(\"CornerHarris\", WINDOW_NAME1, \u0026thresh, max_thresh, on_CornerHarris); on_CornerHarris(0, 0); waitKey(0); return 0; } void Show() { printf(\"\\n\\n\\n\\t\\t\\t【欢迎来到Harris角点检测示例程序~】\\n\\n\"); printf(\"\\n\\n\\n\\t请调整滚动条观察图像效果\\n\\n\"); printf(\"\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t by 晴宝\"); } static void on_CornerHarris(int, void *) { //1.定义一些局部变量 Mat n_dstImage;//目标图 Mat normImage;//归一化后的图 Mat scaledImage;//线性变换后的八位无符号整型的图 //2.初始化 //置零当前需要显示的两幅图，即清除上一次调用此函数时他们的值 n_dstImage = Mat::zeros(srcImage.size(), CV_32FC1); dstImage = srcImage.clone(); //3.正式检测 //进行角点检测 cornerHarris(grayImage, n_dstImage, 2, 3, 0.04, BORDER_DEFAULT); //归一化与转换 normalize(n_dstImage, normImage, 0, 255, NORM_MINMAX, CV_32FC1, Mat()); //将归一化的图像线性变换成8位无符号整型 convertScaleAbs(normImage, scaledImage); //4.进行绘制 //将检测到的，且符合阈值条件的角点绘制出来 for (int j = 0; j \u003c normImage.rows; j++) { for (int i = 0; i \u003c normImage.cols; i++) { if ((int) normImage.at\u003cfloat\u003e(j, i) \u003e thresh + 80) { circle(dstImage, Point(i, j), 5, Scalar(10, 10, 255), 2, 8, 0); circle(scaledImage, Point(i, j), 5, Scalar(0, 10, 255), 2, 8, 0); } } } imshow(WINDOW_NAME1, dstImage); imshow(WINDOW_NAME2, scaledImage); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之feature2d组件","uri":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/#角点检测"},{"categories":["OpenCV"],"content":" 角点检测 Harris角点检测 #include #include #include #include using namespace std; using namespace cv; int main() { //以灰度模式载入图像并显示 Mat srcImage = imread(\"4.jpg\", 0); imshow(\"srcImage\", srcImage); //进行Harris角点检测找出角点 Mat cornerStrength; cornerHarris(srcImage, cornerStrength, 2, 3, 0.01); //对灰度图进行阈值操作，得到二值图并显示 Mat harrisCorner; threshold(cornerStrength, harrisCorner, 0.00001, 255, THRESH_BINARY); imshow(\"harrisCorner\", harrisCorner); waitKey(0); return 0; } #include #include #include #include using namespace std; using namespace cv; #define WINDOW_NAME1 \"dstImage\" #define WINDOW_NAME2 \"scaledImage\" Mat srcImage, dstImage, grayImage; int thresh = 30; //当前阈值 int max_thresh = 175; //最大阈值 static void on_CornerHarris(int , void *); void Show(); int main() { Show(); srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } imshow(\"srcImage\", srcImage); dstImage = srcImage.clone(); cvtColor(dstImage, grayImage, CV_BGR2GRAY); //imshow(\"grayImage\", grayImage); //创建窗口和滚动条 namedWindow(WINDOW_NAME1, CV_WINDOW_AUTOSIZE); createTrackbar(\"CornerHarris\", WINDOW_NAME1, \u0026thresh, max_thresh, on_CornerHarris); on_CornerHarris(0, 0); waitKey(0); return 0; } void Show() { printf(\"\\n\\n\\n\\t\\t\\t【欢迎来到Harris角点检测示例程序~】\\n\\n\"); printf(\"\\n\\n\\n\\t请调整滚动条观察图像效果\\n\\n\"); printf(\"\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t by 晴宝\"); } static void on_CornerHarris(int, void *) { //1.定义一些局部变量 Mat n_dstImage;//目标图 Mat normImage;//归一化后的图 Mat scaledImage;//线性变换后的八位无符号整型的图 //2.初始化 //置零当前需要显示的两幅图，即清除上一次调用此函数时他们的值 n_dstImage = Mat::zeros(srcImage.size(), CV_32FC1); dstImage = srcImage.clone(); //3.正式检测 //进行角点检测 cornerHarris(grayImage, n_dstImage, 2, 3, 0.04, BORDER_DEFAULT); //归一化与转换 normalize(n_dstImage, normImage, 0, 255, NORM_MINMAX, CV_32FC1, Mat()); //将归一化的图像线性变换成8位无符号整型 convertScaleAbs(normImage, scaledImage); //4.进行绘制 //将检测到的，且符合阈值条件的角点绘制出来 for (int j = 0; j \u003c normImage.rows; j++) { for (int i = 0; i \u003c normImage.cols; i++) { if ((int) normImage.at(j, i) \u003e thresh + 80) { circle(dstImage, Point(i, j), 5, Scalar(10, 10, 255), 2, 8, 0); circle(scaledImage, Point(i, j), 5, Scalar(0, 10, 255), 2, 8, 0); } } } imshow(WINDOW_NAME1, dstImage); imshow(WINDOW_NAME2, scaledImage); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之feature2d组件","uri":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/#harris角点检测"},{"categories":["OpenCV"],"content":" 特征检测与匹配 SURF特征点检测 //SURF特征点检测 /* 1.使用FeatureDetector接口发现兴趣点 2.使用SurFeatureDetector以及其函数detect来实现检测过程 3.使用函数drawKeypoints绘制检测到的关键点 */ #include \u003ciostream\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/xfeatures2d.hpp\u003e #include \u003copencv2/core/core.hpp\u003e using namespace cv; using namespace std; using namespace cv::xfeatures2d; void Show(); int main() { Show(); Mat srcImage1 = imread(\"4.jpg\"); Mat srcImage2 = imread(\"5.jpg\"); if (!srcImage1.data || !srcImage2.data) { printf(\"Oh, no, srcImage is error\"); return false; } imshow(\"srcImage1\", srcImage1); imshow(\"srcImage2\", srcImage2); //定义需要用到的变量和类 int minHessian = 400; //定义SURF中的hessian阈值特征值检测算子 //SurfFeatureDetector detector(minHessian);//定义一个SurfFeatureDetector（SURF）特征检测对象 Ptr\u003cSURF\u003e surfDetector = SURF::create(2000); vector\u003cKeyPoint\u003e keypoints_1, keypoints_2;//vector模板类是能够存放任意类型的动态数组，能够增加和压缩数据 //调用detect函数检测出SURF特征关键点，保存在vector容器中 //detector.detect(srcImage1, keypoints_1); //detector.detect(srcImage2, keypoints_2); surfDetector-\u003edetect(srcImage1, keypoints_1); surfDetector-\u003edetect(srcImage2, keypoints_2); //绘制特征关键点 Mat img_keypoints_1, img_keypoints_2; drawKeypoints(srcImage1, keypoints_1, img_keypoints_1, Scalar::all(-1), DrawMatchesFlags::DEFAULT); drawKeypoints(srcImage2, keypoints_2, img_keypoints_2, Scalar::all(-1), DrawMatchesFlags::DEFAULT); imshow(\"img_keypoints_1\", img_keypoints_1); imshow(\"img_keypoints_2\", img_keypoints_2); waitKey(0); return 0; } void Show() { printf(\"\\n\\n\\n\\t欢迎来到【SURF特征检测】示例程序~\\n\\n\"); printf(\"\\n\\n\\t按键操作说明：\\n\\n\" \"\\t\\t键盘按键任意键 - 退出程序\\n\\n\" \"\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t by 晴宝\\n\\n\\n\"); } ///* //程序的核心思想： // 1.使用DescriptorExtractor接口来寻找关键点对应的特征向量 // 2.使用SurfdescriptorExtractor以及它的函数compute来完成特定的计算 // 3.使用BruteForceMatcher来匹配特征向量 // 4.使用函数drawMatches来绘制检测到的匹配点 //*/ #include \u003ciostream\u003e #include \u003copencv2/features2d/features2d.hpp\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/xfeatures2d.hpp\u003e #include \u003copencv2/core/core.hpp\u003e using namespace cv; using namespace std; using namespace cv::xfeatures2d; void Show(); int main() { Show(); Mat srcImage1 = imread(\"4.jpg\"); Mat srcImage2 = imread(\"5.jpg\"); if (!srcImage1.data || !srcImage2.data) { printf(\"Oh, no, srcImage is error\"); return -1; } //使用SURF算子检测关键点 int minHessian = 700; //Ptr\u003cSIFT\u003e SiftDescriptor = SIFT::create(); Ptr\u003cSURF\u003e surfDetector = SURF::create(2000); //海塞矩阵阈值，在这里调整精度，值越大点越少，越精准 vector\u003cKeyPoint\u003e keyPoints1, keyPoints2; //调用detect函数检测出SURF特征关键点，保存在vector容器中 surfDetector-\u003edetect(srcImage1, keyPoints1); surfDetector-\u003edetect(srcImage2, keyPoints2); //计算描述符（特征向量） //特征点描述，为下边的特征点匹配做准备 Ptr\u003cSURF\u003e SurfDescriptor = SURF::create(); Mat descriptors1, descriptors2; SurfDescriptor-\u003ecompute(srcImage1, keyPoints1, descriptors1); SurfDescriptor-\u003ecompute(srcImage2, keyPoints2, descriptors2); //使用BruteForce进行匹配 //实例化一个匹配器 //BruteForceMatcher\u003cL2\u003cfloat\u003e\u003e matcher; BFMatcher matcher; vector\u003cDMatch\u003e matches; //匹配两幅图中的描述子（descriptors） matcher.match(descriptors1, descriptors2, matches); //绘制从两个图像中匹配出的关键点 Mat imgMatches; //进行绘制 drawMatches(srcImage1, keyPoints1, srcImage2, keyPoints2, matches, imgMatches); imshow(\"imgMatches\", imgMatches); waitKey(0); return 0; } void Show() { printf(\"\\n\\n\\n\\t欢迎来到【SURF特征描述】示例程序~\\n\\n\"); printf(\"\\n\\n\\t\\t\\t\\t\\t\\t\\t by 晴宝\\n\\n\\n\"); } #include \u003ciostream\u003e #include \"opencv2/core.hpp\" #include \"opencv2/core/utility.hpp\" #include \"opencv2/core/ocl.hpp\" #include \"opencv2/imgcodecs.hpp\" #include \"opencv2/highgui.hpp\" #include \"opencv2/features2d.hpp\" #include \"opencv2/calib3d.hpp\" #include \"opencv2/imgproc.hpp\" #include\"opencv2/flann.hpp\" #include\"opencv2/xfeatures2d.hpp\" #include\"opencv2/ml.hpp\" using namespace cv; using namespace std; using namespace cv::xfeatures2d; using namespace cv::ml; int main() { Mat a = imread(\"4.jpg\", IMREAD_GRAYSCALE); //读取灰度图像 Mat b = imread(\"5.jpg\", IMREAD_GRAYSCALE); Ptr\u003cSURF\u003e surf; //创建方式和2中的不一样 surf = SURF::create(800); BFMatcher matcher; Mat c, d; vector\u003cKey","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/:0:2","series":null,"tags":["OpenCV"],"title":"OpenCV之feature2d组件","uri":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/#特征检测与匹配"},{"categories":["OpenCV"],"content":" 特征检测与匹配 SURF特征点检测 //SURF特征点检测 /* 1.使用FeatureDetector接口发现兴趣点 2.使用SurFeatureDetector以及其函数detect来实现检测过程 3.使用函数drawKeypoints绘制检测到的关键点 */ #include #include #include #include #include #include using namespace cv; using namespace std; using namespace cv::xfeatures2d; void Show(); int main() { Show(); Mat srcImage1 = imread(\"4.jpg\"); Mat srcImage2 = imread(\"5.jpg\"); if (!srcImage1.data || !srcImage2.data) { printf(\"Oh, no, srcImage is error\"); return false; } imshow(\"srcImage1\", srcImage1); imshow(\"srcImage2\", srcImage2); //定义需要用到的变量和类 int minHessian = 400; //定义SURF中的hessian阈值特征值检测算子 //SurfFeatureDetector detector(minHessian);//定义一个SurfFeatureDetector（SURF）特征检测对象 Ptr surfDetector = SURF::create(2000); vector keypoints_1, keypoints_2;//vector模板类是能够存放任意类型的动态数组，能够增加和压缩数据 //调用detect函数检测出SURF特征关键点，保存在vector容器中 //detector.detect(srcImage1, keypoints_1); //detector.detect(srcImage2, keypoints_2); surfDetector-\u003edetect(srcImage1, keypoints_1); surfDetector-\u003edetect(srcImage2, keypoints_2); //绘制特征关键点 Mat img_keypoints_1, img_keypoints_2; drawKeypoints(srcImage1, keypoints_1, img_keypoints_1, Scalar::all(-1), DrawMatchesFlags::DEFAULT); drawKeypoints(srcImage2, keypoints_2, img_keypoints_2, Scalar::all(-1), DrawMatchesFlags::DEFAULT); imshow(\"img_keypoints_1\", img_keypoints_1); imshow(\"img_keypoints_2\", img_keypoints_2); waitKey(0); return 0; } void Show() { printf(\"\\n\\n\\n\\t欢迎来到【SURF特征检测】示例程序~\\n\\n\"); printf(\"\\n\\n\\t按键操作说明：\\n\\n\" \"\\t\\t键盘按键任意键 - 退出程序\\n\\n\" \"\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t by 晴宝\\n\\n\\n\"); } ///* //程序的核心思想： // 1.使用DescriptorExtractor接口来寻找关键点对应的特征向量 // 2.使用SurfdescriptorExtractor以及它的函数compute来完成特定的计算 // 3.使用BruteForceMatcher来匹配特征向量 // 4.使用函数drawMatches来绘制检测到的匹配点 //*/ #include #include #include #include #include #include using namespace cv; using namespace std; using namespace cv::xfeatures2d; void Show(); int main() { Show(); Mat srcImage1 = imread(\"4.jpg\"); Mat srcImage2 = imread(\"5.jpg\"); if (!srcImage1.data || !srcImage2.data) { printf(\"Oh, no, srcImage is error\"); return -1; } //使用SURF算子检测关键点 int minHessian = 700; //Ptr SiftDescriptor = SIFT::create(); Ptr surfDetector = SURF::create(2000); //海塞矩阵阈值，在这里调整精度，值越大点越少，越精准 vector keyPoints1, keyPoints2; //调用detect函数检测出SURF特征关键点，保存在vector容器中 surfDetector-\u003edetect(srcImage1, keyPoints1); surfDetector-\u003edetect(srcImage2, keyPoints2); //计算描述符（特征向量） //特征点描述，为下边的特征点匹配做准备 Ptr SurfDescriptor = SURF::create(); Mat descriptors1, descriptors2; SurfDescriptor-\u003ecompute(srcImage1, keyPoints1, descriptors1); SurfDescriptor-\u003ecompute(srcImage2, keyPoints2, descriptors2); //使用BruteForce进行匹配 //实例化一个匹配器 //BruteForceMatcher","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/:0:2","series":null,"tags":["OpenCV"],"title":"OpenCV之feature2d组件","uri":"/opencv%E4%B9%8Bfeature2d%E7%BB%84%E4%BB%B6/#surf特征点检测"},{"categories":["OpenCV"],"content":" 模版匹配 ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%92%8C%E5%8C%B9%E9%85%8D/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV之直方图和匹配","uri":"/opencv%E4%B9%8B%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%92%8C%E5%8C%B9%E9%85%8D/#"},{"categories":["OpenCV"],"content":" 模版匹配 //模板匹配 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; int main() { Mat img, temp, result; img = imread(\"test_2.jpg\"); temp = imread(\"test_1.png\"); int result_cols = img.cols - temp.cols + 1; int result_rows = img.rows - temp.rows + 1; result.create(result_cols, result_rows, CV_32FC1); //使用的匹配算法是标准平方差匹配 method = CV_TM_SQDIFF_NORMED, 数值越小匹配度越好 matchTemplate(img, temp, result, CV_TM_SQDIFF_NORMED); normalize(result, result, 0, 1, NORM_MINMAX, -1, Mat()); double minVal = -1; double maxVal; Point minLoc; Point maxLoc; Point matchLoc; cout \u003c\u003c \"匹配度：\" \u003c\u003c minVal \u003c\u003c endl; minMaxLoc(result, \u0026minVal, \u0026maxVal, \u0026minLoc, \u0026maxLoc, Mat()); cout \u003c\u003c \"匹配度：\" \u003c\u003c minVal \u003c\u003c endl; matchLoc = minLoc; rectangle(img, matchLoc, Point(matchLoc.x + temp.cols, matchLoc.y + temp.rows), Scalar(0, 255, 0), 2, 8, 0); imshow(\"img\", img); waitKey(0); return 0; } //模板匹配 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; Mat img, temp, result; int MatchMethod; int MaxTrackbarNum = 5; void on_matching(int ,void *) { Mat srcImage; img.copyTo(srcImage); int result_cols = img.cols - temp.cols + 1; int result_rows = img.rows - temp.rows + 1; result.create(result_cols, result_rows, CV_32FC1); //使用的匹配算法是标准平方差匹配 method = CV_TM_SQDIFF_NORMED, 数值越小匹配度越好 matchTemplate(img, temp, result, MatchMethod); normalize(result, result, 0, 1, NORM_MINMAX, -1, Mat()); double minVal, maxVal; Point minLoc, maxLoc, matchLoc; minMaxLoc(result, \u0026minVal, \u0026maxVal, \u0026minLoc, \u0026maxLoc); if (MatchMethod == TM_SQDIFF || MatchMethod == CV_TM_SQDIFF_NORMED) { matchLoc = minLoc; } else { matchLoc = maxLoc; } rectangle(srcImage, matchLoc, Point(matchLoc.x + temp.cols, matchLoc.y + temp.rows), Scalar(0, 255, 0), 2, 8, 0); rectangle(img, matchLoc, Point(matchLoc.x + temp.cols, matchLoc.y + temp.rows), Scalar(0, 255, 0), 2, 8, 0); imshow(\"srcImage\", srcImage); imshow(\"img\", img); } int main() { img = imread(\"test_2.jpg\"); temp = imread(\"test_1.png\"); if (!img.data) { printf(\"img is error\"); return -1; } if (!temp.data) { printf(\"temp is error\"); return -1; } namedWindow(\"img\", CV_WINDOW_AUTOSIZE); createTrackbar(\"bar\", \"img\", \u0026MatchMethod, MaxTrackbarNum, on_matching); on_matching(0, NULL); waitKey(0); return 0; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%92%8C%E5%8C%B9%E9%85%8D/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之直方图和匹配","uri":"/opencv%E4%B9%8B%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%92%8C%E5%8C%B9%E9%85%8D/#模版匹配"},{"categories":["OpenCV"],"content":" 使用多边形将轮廓包围: 矩形、椭圆、十字型结构 ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E8%BD%AE%E5%BB%93%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BF%AE%E5%A4%8D/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV之图像轮廓与图像分割修复","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E8%BD%AE%E5%BB%93%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BF%AE%E5%A4%8D/#"},{"categories":["OpenCV"],"content":" 使用多边形将轮廓包围 矩形、椭圆、十字型结构 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; Mat srcImg, dstImg; //构建形状 int elementshape = MORPH_RECT; //接收TrackBar位置参数 int ocvalue = 1; int tbvalue = 1; int devalue = 1; int maxvalue = 10; //回调函数 void OpenClose(int , void *); void DilateErode(int , void *); void TopBlackHat(int , void *); //显示帮助信息 static void Show(); int main() { Show(); srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"OpenClose\"); namedWindow(\"DilateErode\"); namedWindow(\"TopBlackHat\"); imshow(\"srcImg\", srcImg); createTrackbar(\"OpenC\", \"OpenClose\", \u0026ocvalue, maxvalue * 2 + 1, OpenClose); createTrackbar(\"TopB\", \"TopBlackHat\", \u0026tbvalue, maxvalue * 2 + 1, TopBlackHat); createTrackbar(\"DilateE\", \"DilateErode\", \u0026devalue, maxvalue * 2 + 1, DilateErode); //轮询获取按键信息 while (1) { int c; //执行回调函数 OpenClose(ocvalue, 0); TopBlackHat(tbvalue, 0); DilateErode(devalue, 0); //获取按键 c = waitKey(0); //按下键盘按键Q或者ESC，程序退出 if ((char)c == 'q' || (char)c == 27) break; //按下键盘按键1， 使用椭圆（Elliptic）结构元素MORPH_ELLIPSE if ((char)c == 49) elementshape = MORPH_ELLIPSE; //按下键盘按键2， 使用矩形（Rectangle）结构元素MORPH_RECT else if ((char)c == 50) elementshape = MORPH_RECT; //按下键盘按键3， 使用十字形（Cross-Shaped）结构元素MORPH_CROSS else if ((char)c == 51) elementshape = MORPH_CROSS; //按下键盘按键space， 在矩形，椭圆，十字形中循环 else if ((char)c == ' ') elementshape = (elementshape + 1) % 3; } return 0; } void OpenClose(int, void *) { //偏移量的定义 int offset = ocvalue - maxvalue; int absolute_offset = offset \u003e 0 ? offset : -offset; Mat element = getStructuringElement(elementshape, Size(absolute_offset * 2 +1 , absolute_offset * 2 + 1), Point(absolute_offset, absolute_offset)); //进行操作 if(offset \u003c 0) morphologyEx(srcImg, dstImg, MORPH_OPEN, element); else morphologyEx(srcImg, dstImg, MORPH_CLOSE, element); imshow(\"OpenClose\", dstImg); } void TopBlackHat(int, void *) { //偏移量的定义 int offset = tbvalue - maxvalue; int absolute_offset = offset \u003e 0 ? offset : -offset; Mat element = getStructuringElement(elementshape, Size(absolute_offset * 2 + 1, absolute_offset * 2 + 1), Point(absolute_offset, absolute_offset)); //进行操作 if (offset \u003c 0) morphologyEx(srcImg, dstImg, MORPH_TOPHAT, element); else morphologyEx(srcImg, dstImg, MORPH_BLACKHAT, element); imshow(\"TopBlackHat\", dstImg); } void DilateErode(int, void *) { //偏移量的定义 int offset = devalue - maxvalue; int absolute_offset = offset \u003e 0 ? offset : -offset; Mat element = getStructuringElement(elementshape, Size(absolute_offset * 2 + 1, absolute_offset * 2 + 1), Point(absolute_offset, absolute_offset)); //进行操作 if (offset \u003c 0) erode(srcImg, dstImg, element); else dilate(srcImg, dstImg, element); imshow(\"DilateErode\", dstImg); } static void Show() { printf(\"\\n\\n\\n请调整滚动条观察图像效果\\n\\n\"); printf( \"\\n\\n\\t按键说明操作：\\n\\n\" \"\\t\\t键盘按键【ESC】或者【Q】- 退出程序\\n\" \"\\t\\t键盘按键【1】 - 使用椭圆(Elliptic)结构元素\\n\" \"\\t\\t键盘按键【2】 - 使用矩形(Rectangle)结构元素\\n\" \"\\t\\t键盘按键【3】 - 使用十字型(Cross-shaped)结构元素\\n\" \"\\t\\t键盘按键【SPACE】 - 在矩形、椭圆、十字型结构元素循环\\n\" \"\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t by晴宝\" ); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E8%BD%AE%E5%BB%93%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BF%AE%E5%A4%8D/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之图像轮廓与图像分割修复","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E8%BD%AE%E5%BB%93%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BF%AE%E5%A4%8D/#使用多边形将轮廓包围"},{"categories":["OpenCV"],"content":" 使用多边形将轮廓包围 矩形、椭圆、十字型结构 #include #include #include #include using namespace std; using namespace cv; Mat srcImg, dstImg; //构建形状 int elementshape = MORPH_RECT; //接收TrackBar位置参数 int ocvalue = 1; int tbvalue = 1; int devalue = 1; int maxvalue = 10; //回调函数 void OpenClose(int , void *); void DilateErode(int , void *); void TopBlackHat(int , void *); //显示帮助信息 static void Show(); int main() { Show(); srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"OpenClose\"); namedWindow(\"DilateErode\"); namedWindow(\"TopBlackHat\"); imshow(\"srcImg\", srcImg); createTrackbar(\"OpenC\", \"OpenClose\", \u0026ocvalue, maxvalue * 2 + 1, OpenClose); createTrackbar(\"TopB\", \"TopBlackHat\", \u0026tbvalue, maxvalue * 2 + 1, TopBlackHat); createTrackbar(\"DilateE\", \"DilateErode\", \u0026devalue, maxvalue * 2 + 1, DilateErode); //轮询获取按键信息 while (1) { int c; //执行回调函数 OpenClose(ocvalue, 0); TopBlackHat(tbvalue, 0); DilateErode(devalue, 0); //获取按键 c = waitKey(0); //按下键盘按键Q或者ESC，程序退出 if ((char)c == 'q' || (char)c == 27) break; //按下键盘按键1， 使用椭圆（Elliptic）结构元素MORPH_ELLIPSE if ((char)c == 49) elementshape = MORPH_ELLIPSE; //按下键盘按键2， 使用矩形（Rectangle）结构元素MORPH_RECT else if ((char)c == 50) elementshape = MORPH_RECT; //按下键盘按键3， 使用十字形（Cross-Shaped）结构元素MORPH_CROSS else if ((char)c == 51) elementshape = MORPH_CROSS; //按下键盘按键space， 在矩形，椭圆，十字形中循环 else if ((char)c == ' ') elementshape = (elementshape + 1) % 3; } return 0; } void OpenClose(int, void *) { //偏移量的定义 int offset = ocvalue - maxvalue; int absolute_offset = offset \u003e 0 ? offset : -offset; Mat element = getStructuringElement(elementshape, Size(absolute_offset * 2 +1 , absolute_offset * 2 + 1), Point(absolute_offset, absolute_offset)); //进行操作 if(offset \u003c 0) morphologyEx(srcImg, dstImg, MORPH_OPEN, element); else morphologyEx(srcImg, dstImg, MORPH_CLOSE, element); imshow(\"OpenClose\", dstImg); } void TopBlackHat(int, void *) { //偏移量的定义 int offset = tbvalue - maxvalue; int absolute_offset = offset \u003e 0 ? offset : -offset; Mat element = getStructuringElement(elementshape, Size(absolute_offset * 2 + 1, absolute_offset * 2 + 1), Point(absolute_offset, absolute_offset)); //进行操作 if (offset \u003c 0) morphologyEx(srcImg, dstImg, MORPH_TOPHAT, element); else morphologyEx(srcImg, dstImg, MORPH_BLACKHAT, element); imshow(\"TopBlackHat\", dstImg); } void DilateErode(int, void *) { //偏移量的定义 int offset = devalue - maxvalue; int absolute_offset = offset \u003e 0 ? offset : -offset; Mat element = getStructuringElement(elementshape, Size(absolute_offset * 2 + 1, absolute_offset * 2 + 1), Point(absolute_offset, absolute_offset)); //进行操作 if (offset \u003c 0) erode(srcImg, dstImg, element); else dilate(srcImg, dstImg, element); imshow(\"DilateErode\", dstImg); } static void Show() { printf(\"\\n\\n\\n请调整滚动条观察图像效果\\n\\n\"); printf( \"\\n\\n\\t按键说明操作：\\n\\n\" \"\\t\\t键盘按键【ESC】或者【Q】- 退出程序\\n\" \"\\t\\t键盘按键【1】 - 使用椭圆(Elliptic)结构元素\\n\" \"\\t\\t键盘按键【2】 - 使用矩形(Rectangle)结构元素\\n\" \"\\t\\t键盘按键【3】 - 使用十字型(Cross-shaped)结构元素\\n\" \"\\t\\t键盘按键【SPACE】 - 在矩形、椭圆、十字型结构元素循环\\n\" \"\\n\\n\\n\\n\\t\\t\\t\\t\\t\\t\\t by晴宝\" ); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E8%BD%AE%E5%BB%93%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BF%AE%E5%A4%8D/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之图像轮廓与图像分割修复","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E8%BD%AE%E5%BB%93%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E4%BF%AE%E5%A4%8D/#矩形椭圆十字型结构"},{"categories":["OpenCV"],"content":" 边缘检测：canny算子、sobel算子、Laplace算子、Scharr滤波器 霍夫变换 重映射 仿射变换 ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV之图像变换","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/#"},{"categories":["OpenCV"],"content":" 边缘检测 canny算子、sobel算子、Laplace算子、Scharr滤波器 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } namedWindow(\"srcImage\"); namedWindow(\"Canny1\"); namedWindow(\"Canny2\"); namedWindow(\"Sobel\"); namedWindow(\"Laplace\"); namedWindow(\"Scharr\"); imshow(\"srcImage\", srcImage); //一.最简单的Canny用法 Mat dstImage; dstImage = srcImage.clone(); Canny(srcImage, dstImage, 3, 9, 3); imshow(\"Canny1\", dstImage); //二.高阶的Canny用法， 转成灰度图，降噪，用Canny，最后将得到的边缘作为掩码， //拷贝原图到效果图上，得到彩色的边缘图 Mat dst, edge, gray; //1.创建与src同类型和大小的矩阵dst dst.create(srcImage.size(), srcImage.type()); //2.将原图转换为灰色图像 cvtColor(srcImage, gray, CV_BGR2GRAY); //3.先用使用3*3内核降噪 blur(gray, edge, Size(3, 3)); //4.运行Canny算子 Canny(edge, edge, 3, 9, 3); //5.将dst内的所有元素设为0 dst = Scalar::all(0); //6.使用Canny算子输出的边缘图g_cannyDetectedEdges作为掩码，来将原图srcImage拷到目标图dst中 srcImage.copyTo(dst, edge); imshow(\"Canny2\", dst); //sobel算子 Mat dst_x, dst_y; Mat s_dst_x, s_dst_y; Mat ddst; //1.求x方向的梯度 Sobel(srcImage, dst_x, CV_16S, 1, 0, 3, 1, 1, BORDER_DEFAULT); convertScaleAbs(dst_x, s_dst_x); imshow(\"X_Sobel\", s_dst_x); //2.求y方向的梯度 Sobel(srcImage, dst_y, CV_16S, 0, 1, 3, 1, 1, BORDER_DEFAULT); convertScaleAbs(dst_y, s_dst_y); imshow(\"Y_Sobel\", s_dst_y); //合并梯度 addWeighted(s_dst_x, 0.5, s_dst_y, 0.5, 0, ddst); imshow(\"Sobel\", ddst); //Laplace算子 Mat l_gray, l_dst, l_abs_dst; //1.使用高斯滤波消除噪声 GaussianBlur(srcImage, l_dst, Size(3, 3), 0, 0, BORDER_DEFAULT); //2.转为灰度图 cvtColor(srcImage, l_gray, CV_RGB2GRAY); //3.使用Laplace函数 Laplacian(l_gray, l_ds, CV_16S, 3, t1, 0, BORDER_DEFAULT); //4.计算绝对值，并将结果转换成8位 convertScaleAbs(l_dst, l_abs_dst); imshow(\"Laplace\", l_abs_dst); //Scharr滤波器 Mat s_x, s_y; Mat s_abs_x, s_abs_y, s_dst; //1.求X方向梯度 Scharr(srcImage, s_x, CV_16S, 1, 0, 1, 0, BORDER_DEFAULT); convertScaleAbs(s_x, s_abs_x); imshow(\"X_Scharr\", s_abs_x); //2.求Y方向梯度 Scharr(srcImage, s_y, CV_16S, 0, 1, 1, 0, BORDER_DEFAULT); convertScaleAbs(s_y, s_abs_y); imshow(\"Y_Scharr\", s_abs_y); //3.合并梯度 addWeighted(s_abs_x, 0.5, s_abs_y, 0.5, 0, s_dst); imshow(\"Scharr\", s_dst); waitKey(0); return 0; } #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; Mat srcImage, cImage, sImage, scImage, grayImage; //Canny相关变量 Mat cannyImg; int cannyvalue = 1; //Sobel相关参数 Mat sobel_x, sobel_y; Mat sobel_abs_x, sobel_abs_y; int sobelvalue = 1; //Scharr相关参数 Mat scharr_x, scharr_y; Mat scharr_abs_x, scharr_abs_y; //回调函数 void Show(); void on_Canny(int , void *); void on_Sobel(int, void *); void Scharr(); int main() { Show(); srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } namedWindow(\"srcImage\"); namedWindow(\"Canny\"); namedWindow(\"Sobel\"); namedWindow(\"Scharr\"); imshow(\"srcImage\", srcImage); cImage.create(srcImage.size(), srcImage.type()); cvtColor(srcImage, grayImage, CV_BGR2GRAY); createTrackbar(\"canny\", \"Canny\", \u0026cannyvalue, 120, on_Canny); createTrackbar(\"sobel\", \"Sobel\", \u0026sobelvalue, 3, on_Sobel); on_Canny(cannyvalue, 0); on_Sobel(sobelvalue, 0); Scharr(); while (char(waitKey(1)) != 'q') {} return 0; } void Show() { printf(\"By 晴宝\"); } void on_Canny(int, void *) { //降噪 blur(grayImage, cannyImg, Size(3, 3)); //canny算子 Canny(cannyImg, cannyImg, cannyvalue, cannyvalue * 3, 3); //将dstImage所有元素设为0 cImage = Scalar::all(0); srcImage.copyTo(cImage, cannyImg); imshow(\"Canny\", cImage); } void on_Sobel(int, void *) { //x方向 Sobel(srcImage, sobel_x, CV_16S, 1, 0, (2 * sobelvalue + 1), 1, 0, BORDER_DEFAULT); convertScaleAbs(sobel_x, sobel_abs_x); //y方向 Sobel(srcImage, sobel_y, CV_16S, 0, 1, (2 * sobelvalue + 1), 1, 0, BORDER_DEFAULT); convertScaleAbs(sobel_y, sobel_abs_y); //合并 addWeighted(sobel_abs_x, 0.5, sobel_abs_y, 0.5, 0, sImage); imshow(\"Sobel\", sImage); } void Scharr() { Scha","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之图像变换","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/#边缘检测"},{"categories":["OpenCV"],"content":" 边缘检测 canny算子、sobel算子、Laplace算子、Scharr滤波器 #include #include #include #include using namespace std; using namespace cv; int main() { Mat srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } namedWindow(\"srcImage\"); namedWindow(\"Canny1\"); namedWindow(\"Canny2\"); namedWindow(\"Sobel\"); namedWindow(\"Laplace\"); namedWindow(\"Scharr\"); imshow(\"srcImage\", srcImage); //一.最简单的Canny用法 Mat dstImage; dstImage = srcImage.clone(); Canny(srcImage, dstImage, 3, 9, 3); imshow(\"Canny1\", dstImage); //二.高阶的Canny用法， 转成灰度图，降噪，用Canny，最后将得到的边缘作为掩码， //拷贝原图到效果图上，得到彩色的边缘图 Mat dst, edge, gray; //1.创建与src同类型和大小的矩阵dst dst.create(srcImage.size(), srcImage.type()); //2.将原图转换为灰色图像 cvtColor(srcImage, gray, CV_BGR2GRAY); //3.先用使用3*3内核降噪 blur(gray, edge, Size(3, 3)); //4.运行Canny算子 Canny(edge, edge, 3, 9, 3); //5.将dst内的所有元素设为0 dst = Scalar::all(0); //6.使用Canny算子输出的边缘图g_cannyDetectedEdges作为掩码，来将原图srcImage拷到目标图dst中 srcImage.copyTo(dst, edge); imshow(\"Canny2\", dst); //sobel算子 Mat dst_x, dst_y; Mat s_dst_x, s_dst_y; Mat ddst; //1.求x方向的梯度 Sobel(srcImage, dst_x, CV_16S, 1, 0, 3, 1, 1, BORDER_DEFAULT); convertScaleAbs(dst_x, s_dst_x); imshow(\"X_Sobel\", s_dst_x); //2.求y方向的梯度 Sobel(srcImage, dst_y, CV_16S, 0, 1, 3, 1, 1, BORDER_DEFAULT); convertScaleAbs(dst_y, s_dst_y); imshow(\"Y_Sobel\", s_dst_y); //合并梯度 addWeighted(s_dst_x, 0.5, s_dst_y, 0.5, 0, ddst); imshow(\"Sobel\", ddst); //Laplace算子 Mat l_gray, l_dst, l_abs_dst; //1.使用高斯滤波消除噪声 GaussianBlur(srcImage, l_dst, Size(3, 3), 0, 0, BORDER_DEFAULT); //2.转为灰度图 cvtColor(srcImage, l_gray, CV_RGB2GRAY); //3.使用Laplace函数 Laplacian(l_gray, l_ds, CV_16S, 3, t1, 0, BORDER_DEFAULT); //4.计算绝对值，并将结果转换成8位 convertScaleAbs(l_dst, l_abs_dst); imshow(\"Laplace\", l_abs_dst); //Scharr滤波器 Mat s_x, s_y; Mat s_abs_x, s_abs_y, s_dst; //1.求X方向梯度 Scharr(srcImage, s_x, CV_16S, 1, 0, 1, 0, BORDER_DEFAULT); convertScaleAbs(s_x, s_abs_x); imshow(\"X_Scharr\", s_abs_x); //2.求Y方向梯度 Scharr(srcImage, s_y, CV_16S, 0, 1, 1, 0, BORDER_DEFAULT); convertScaleAbs(s_y, s_abs_y); imshow(\"Y_Scharr\", s_abs_y); //3.合并梯度 addWeighted(s_abs_x, 0.5, s_abs_y, 0.5, 0, s_dst); imshow(\"Scharr\", s_dst); waitKey(0); return 0; } #include #include #include #include using namespace std; using namespace cv; Mat srcImage, cImage, sImage, scImage, grayImage; //Canny相关变量 Mat cannyImg; int cannyvalue = 1; //Sobel相关参数 Mat sobel_x, sobel_y; Mat sobel_abs_x, sobel_abs_y; int sobelvalue = 1; //Scharr相关参数 Mat scharr_x, scharr_y; Mat scharr_abs_x, scharr_abs_y; //回调函数 void Show(); void on_Canny(int , void *); void on_Sobel(int, void *); void Scharr(); int main() { Show(); srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } namedWindow(\"srcImage\"); namedWindow(\"Canny\"); namedWindow(\"Sobel\"); namedWindow(\"Scharr\"); imshow(\"srcImage\", srcImage); cImage.create(srcImage.size(), srcImage.type()); cvtColor(srcImage, grayImage, CV_BGR2GRAY); createTrackbar(\"canny\", \"Canny\", \u0026cannyvalue, 120, on_Canny); createTrackbar(\"sobel\", \"Sobel\", \u0026sobelvalue, 3, on_Sobel); on_Canny(cannyvalue, 0); on_Sobel(sobelvalue, 0); Scharr(); while (char(waitKey(1)) != 'q') {} return 0; } void Show() { printf(\"By 晴宝\"); } void on_Canny(int, void *) { //降噪 blur(grayImage, cannyImg, Size(3, 3)); //canny算子 Canny(cannyImg, cannyImg, cannyvalue, cannyvalue * 3, 3); //将dstImage所有元素设为0 cImage = Scalar::all(0); srcImage.copyTo(cImage, cannyImg); imshow(\"Canny\", cImage); } void on_Sobel(int, void *) { //x方向 Sobel(srcImage, sobel_x, CV_16S, 1, 0, (2 * sobelvalue + 1), 1, 0, BORDER_DEFAULT); convertScaleAbs(sobel_x, sobel_abs_x); //y方向 Sobel(srcImage, sobel_y, CV_16S, 0, 1, (2 * sobelvalue + 1), 1, 0, BORDER_DEFAULT); convertScaleAbs(sobel_y, sobel_abs_y); //合并 addWeighted(sobel_abs_x, 0.5, sobel_abs_y, 0.5, 0, sImage); imshow(\"Sobel\", sImage); } void Scharr() { Scha","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之图像变换","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/#canny算子sobel算子laplace算子scharr滤波器"},{"categories":["OpenCV"],"content":" 霍夫变换 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"Canny\"); namedWindow(\"Lines\"); namedWindow(\"Linesp\"); namedWindow(\"Circle\"); imshow(\"srcImg\", srcImg); Mat tmpImg, dstImg1, dstImg2, dstImg3; //1.进行边缘检测和转化为灰度图 Canny(srcImg, tmpImg, 50, 200, 3); cvtColor(tmpImg, dstImg1, CV_GRAY2BGR); cvtColor(tmpImg, dstImg2, CV_GRAY2BGR); //1.转为灰度图，进行图像平滑 cvtColor(srcImg, dstImg3, CV_BGR2GRAY); GaussianBlur(dstImg3, dstImg3, Size(9, 9), 2, 2); //2.进行霍夫线变换 vector\u003cVec2f\u003e lines; //定义一个矢量结构lines用于存放得到的线段矢量集合 HoughLines(tmpImg, lines, 1, CV_PI / 180, 150, 0, 0); vector\u003cVec4i\u003e linesp; HoughLinesP(tmpImg, linesp, 1, CV_PI / 180, 80, 50, 10); //2.进行霍夫圆变换 vector\u003cVec3f\u003e circles; HoughCircles(dstImg3, circles, CV_HOUGH_GRADIENT, 1.5, 10, 200, 100, 0, 0); //3.依次在图中绘制每条线段 for (size_t i = 0; i \u003c lines.size(); i++) { float rho = lines[i][0], theta = lines[i][1]; Point pt1, pt2; double a = cos(theta), b = sin(theta); double x0 = a * rho, y0 = b * rho; pt1.x = cvRound(x0 + 1000 * (-b)); pt1.y = cvRound(y0 + 1000 * (a)); pt2.x = cvRound(x0 - 1000 * (-b)); pt2.y = cvRound(y0 - 1000 * (a)); line(dstImg1, pt1, pt2, Scalar(55, 100, 195), 1, CV_AA); } for (size_t j = 0; j \u003c linesp.size(); j++) { Vec4i l = linesp[j]; line(dstImg2, Point(l[0], l[1]), Point(l[2], l[3]), Scalar(186, 88, 255), 1, CV_AA); } //3.依次在图中绘制图圆 for (size_t z = 0; z \u003c circles.size(); z++) { Point center(cvRound(circles[z][0]), cvRound(circles[z][1])); int radius = cvRound(circles[z][2]); //绘制圆心 circle(srcImg, center, 3, Scalar(0, 255, 0), -1, 8, 0); //绘制圆轮廓 circle(srcImg, center, radius, Scalar(155, 50, 255), 3, 8, 0); } imshow(\"Canny\", tmpImg); imshow(\"Lines\", dstImg1); imshow(\"Linesp\", dstImg2); imshow(\"Circle\", srcImg); waitKey(0); return 0; } #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; Mat srcImage, dstImage, tmpImage; vector\u003cVec4i\u003e lines; //变量接收的TrackBar位置参数 int threadhold = 100; //回调函数 static void on_HoughLinesp(int , void *); void Show(); int main() { Show(); srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } namedWindow(\"srcImage\"); namedWindow(\"HLP\"); imshow(\"srcImage\", srcImage); createTrackbar(\"HoughLinesp\", \"HLP\", \u0026threadhold, 200, on_HoughLinesp); //进行边缘检测和转化为灰度图 Canny(srcImage, tmpImage, 50, 200, 3); cvtColor(tmpImage, dstImage, CV_GRAY2BGR); //调用一次回调函数，调用一次HoughLinesp函数 on_HoughLinesp(threadhold, 0); HoughLinesP(tmpImage, lines, 1, CV_PI/180, 80, 50, 10); imshow(\"HLP\", dstImage); while (char(waitKey(1) != 'q') ) {} return 0; } static void on_HoughLinesp(int, void *) { Mat mydstImage = dstImage.clone(); Mat mytmpImage = tmpImage.clone(); vector\u003cVec4i\u003e mylines; HoughLinesP(mytmpImage, mylines, 1, CV_PI/180, threadhold + 1, 50, 10); for (size_t i = 0; i \u003c mylines.size(); i++) { Vec4i l = mylines[i]; line(mydstImage, Point(l[0],l[1]), Point(l[2],l[3]), Scalar(23, 180, 55), 1, CV_AA); } imshow(\"HLP\", mydstImage); } void Show() { printf(\"\\n\\n\\n\\t请调整滚动条观察图像效果\\n\\n\"); printf(\"\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t by 晴宝\"); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/:0:2","series":null,"tags":["OpenCV"],"title":"OpenCV之图像变换","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/#霍夫变换"},{"categories":["OpenCV"],"content":" 重映射 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImage, dstImage; Mat map_x, map_y; srcImage = imread(\"4.jpg\"); if(!srcImage.data) { printf(\"Oh,no, srcImage is error\"); return false; } imshow(\"srcImage\", srcImage); //创建和原图一样的效果图，x重映射图， y重映射图 dstImage.create(srcImage.size(), srcImage.type()); map_x.create(srcImage.size(), CV_32FC1); map_y.create(srcImage.size(), CV_32FC1); //双层循环，遍历每一个像素点，改变map_x \u0026 map_y的值 for(int j = 0; j \u003c srcImage.rows; j++) { for(int i = 0; i \u003c srcImage.cols; i++) { //改变map_x \u0026 map_y的值 map_x.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(i); map_y.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(srcImage.rows - j); } } //进行重映射操作 remap(srcImage, dstImage, map_x, map_y, CV_INTER_LINEAR, BORDER_CONSTANT, Scalar(0, 0)); imshow(\"dstImage\", dstImage); waitKey(0); return 0; } //重映射操作 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; Mat srcImage, dstImage; Mat map_x, map_y; int update_map(int key);//更新按键按键 void Show(); int main() { Show(); srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return false; } imshow(\"srcImage\", srcImage); dstImage.create(srcImage.size(), srcImage.type()); map_x.create(srcImage.size(), CV_32FC1); map_y.create(srcImage.size(), CV_32FC1); //轮询按键，更新map_x和map_y的值，进行重映射操作并显示效果图 while (1) { //获取键盘按键 int key = waitKey(0); //判断ESC是否按下，若按下便退出 if ((key \u0026 255) == 27) { cout \u003c\u003c \"程序退出。。。。。。\\n\"; break; } //根据按下的键盘按键更新map_x \u0026 map_y的值，然后调用remap()进行重映射 update_map(key); remap(srcImage, dstImage, map_x, map_y, CV_INTER_LINEAR, BORDER_CONSTANT, Scalar(0, 0, 0)); imshow(\"dstImage\", dstImage); } return 0; } void Show() { printf(\"\\n\\n\\t\\t\\t\\t欢迎来到重映射示例程序~\\n\"); printf(\"\\t当前使用的OpenCV的版本为\",CV_VERSION); printf(\"\\n\\t\\t按键操作说明：\\n\\n\" \"\\t\\t键盘按键【ESC】- 退出程序\\n\" \"\\t\\t键盘按键【1】 - 第一种映射方式\\n\" \"\\t\\t键盘按键【2】 - 第二种映射方式\\n\" \"\\t\\t键盘按键【3】 - 第三种映射方式\\n\" \"\\t\\t键盘按键【4】 - 第四种映射方式\\n\" \"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t by 晴宝\\n\"); } int update_map(int key) { //双层循环，遍历每一个像素点 for (int j = 0; j \u003c srcImage.rows; j++) { for (int i = 0; i \u003c srcImage.cols; i++) { switch (key) { case '1': if (i \u003e srcImage.cols * 0.25 \u0026\u0026 i \u003c srcImage.cols * 0.75 \u0026\u0026 j \u003e srcImage.rows * 0.25 \u0026\u0026 j \u003c srcImage.rows * 0.75) { map_x.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(2 * (i - srcImage.cols * 0.25) + 0.5); map_y.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(2 * (j - srcImage.rows * 0.25) + 0.5); } else { map_x.at\u003cfloat\u003e(j, i) = 0; map_y.at\u003cfloat\u003e(j, i) = 0; } break; case '2': map_x.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(i); map_y.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(srcImage.rows - j); break; case '3': map_x.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(srcImage.cols - i); map_y.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(j); break; case '4': map_x.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(srcImage.cols - i); map_y.at\u003cfloat\u003e(j, i) = static_cast\u003cfloat\u003e(srcImage.rows - j); break; } } } return 1; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/:0:3","series":null,"tags":["OpenCV"],"title":"OpenCV之图像变换","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/#重映射"},{"categories":["OpenCV"],"content":" 仿射变换 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; void Show(); int main() { Show(); //1.参数准备 //定义两组点，代表两个三角形 Point2f srcTriangle[3]; Point2f dstTriangle[3]; //定义一些Mat变量 Mat rotMat(2, 3, CV_32FC1); Mat warpMat(2, 3, CV_32FC1); Mat srcImage, dstImage_warp, dstImage_rotate; srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return false; } imshow(\"srcImage\", srcImage); //设置目标图像的大小和类型与源图像一致 dstImage_warp = Mat::zeros(srcImage.rows, srcImage.cols, srcImage.type()); //设置源图像和目标图像上的三组点以计算仿射变换 srcTriangle[0] = Point2f(0, 0); srcTriangle[1] = Point2f(static_cast\u003cfloat\u003e(srcImage.cols - 1), 0); srcTriangle[2] = Point2f(static_cast\u003cfloat\u003e(srcImage.rows - 1)); dstTriangle[0] = Point2f(static_cast\u003cfloat\u003e(srcImage.cols*0.0), static_cast\u003cfloat\u003e(srcImage.rows*0.33)); dstTriangle[1] = Point2f(static_cast\u003cfloat\u003e(srcImage.cols*0.65), static_cast\u003cfloat\u003e(srcImage.rows*0.35)); dstTriangle[2] = Point2f(static_cast\u003cfloat\u003e(srcImage.cols*0.15), static_cast\u003cfloat\u003e(srcImage.rows*0.6)); //求得仿射变换 warpMat = getAffineTransform(srcTriangle, dstTriangle); //对源图像应用刚刚求得的仿射变换 warpAffine(srcImage, dstImage_warp, warpMat, dstImage_warp.size()); //对图像进行缩放后再旋转 //计算绕图像中点顺时针旋转50度缩放因子为0.6的旋转矩阵 Point center = Point(dstImage_warp.cols / 2, dstImage_warp.rows / 2); double angle = -30.0; double scale = 0.8; //通过上面的旋转细节信息求得旋转矩阵 rotMat = getRotationMatrix2D(center, angle, scale); //旋转已缩放后的图像 warpAffine(dstImage_warp, dstImage_rotate, rotMat, dstImage_warp.size()); imshow(\"dstImage_warp\", dstImage_warp); imshow(\"dstImage_rotate\", dstImage_rotate); waitKey(0); return 0; } void Show() { printf(\"\\n\\n\\n\\t欢迎来到【仿射变换】示例程序~\\n\\n\"); printf(\"\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tby 晴宝\\n\\n\\n\"); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/:0:4","series":null,"tags":["OpenCV"],"title":"OpenCV之图像变换","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%8F%98%E6%8D%A2/#仿射变换"},{"categories":["OpenCV"],"content":" 线性滤波：方框滤波、均值滤波、高斯滤波 非线性滤波：中值滤波、双边滤波 形态学滤波： 腐蚀、膨胀、开运算、闭运算、形态学梯度、顶帽、黑帽 漫水填充 图像金字塔和图像尺寸缩放 ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#"},{"categories":["OpenCV"],"content":" 线性滤波： 方框滤波、均值滤波、高斯滤波 // // main.cpp // BoxFilter // // Created by 吕晴 on 2018/9/19. // Copyright 2018年 吕晴. All rights reserved. // #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; //int main() //{ // Mat img = imread(\"/Users/lvqing/Project/Project_OpenCV_C++/image/7.jpg\"); // // //创建窗口 // namedWindow(\"均值滤波【原图】\"); // namedWindow(\"均值滤波【效果图】\"); // // imshow(\"均值滤波【原图】\", img); // // //进行均值滤波操作 //// Mat out; //// boxFilter(img, out, -1, Size(5, 5)); // // // //进行高斯滤波 // Mat out; // GaussianBlur(img, out, Size(0, 0), 0, 0); // // imshow(\"均值滤波【效果图】\", out); // // waitKey(0); //} Mat srcImage, dstImage1, dstImage2, dstImage3; int nBoxFilter = 3; int nBlur = 3; int nGaussianBlur = 3; //轨迹条的回调函数 static void On_BoxFilter(int , void *); //方框滤波 static void On_Blur(int , void *); //均值滤波 static void On_GaussianBlur(int , void *); //高斯滤波 int main() { srcImage = imread(\"/Users/lvqing/Project/Project_OpenCV_C++/image/7.jpg\"); if(!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } dstImage1 = srcImage.clone(); dstImage2 = srcImage.clone(); dstImage3 = srcImage.clone(); namedWindow(\"原图\", 1); imshow(\"原图\", srcImage); namedWindow(\"方框滤波\", 1); createTrackbar(\"内核\", \"方框滤波\", \u0026nBoxFilter, 40, On_BoxFilter); On_BoxFilter(nBoxFilter, 0); namedWindow(\"均值滤波\", 1); createTrackbar(\"内核\", \"均值滤波\", \u0026nBlur, 40, On_Blur); On_Blur(nBlur, 0); namedWindow(\"高斯滤波\", 1); createTrackbar(\"内核\", \"高斯滤波\", \u0026nGaussianBlur, 40, On_GaussianBlur); On_GaussianBlur(nGaussianBlur, 0); while(char(waitKey(1)) != 'q'){} return 0; } static void On_BoxFilter(int , void *) { boxFilter(srcImage, dstImage1, -1, Size( nBoxFilter + 1, nBoxFilter + 1)); imshow(\"方框滤波\", dstImage1); } static void On_Blur(int , void *) { blur(srcImage, dstImage2, Size( nBlur + 1, nBlur + 1), Point(-1, -1)); imshow(\"均值滤波\", dstImage2); } static void On_GaussianBlur(int , void *) { GaussianBlur(srcImage, dstImage3, Size( nGaussianBlur * 2 + 1, nGaussianBlur * 2 + 1), 0, 0); imshow(\"高斯滤波\", dstImage3); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#线性滤波"},{"categories":["OpenCV"],"content":" 线性滤波： 方框滤波、均值滤波、高斯滤波 // // main.cpp // BoxFilter // // Created by 吕晴 on 2018/9/19. // Copyright 2018年 吕晴. All rights reserved. // #include #include #include #include #include using namespace std; using namespace cv; //int main() //{ // Mat img = imread(\"/Users/lvqing/Project/Project_OpenCV_C++/image/7.jpg\"); // // //创建窗口 // namedWindow(\"均值滤波【原图】\"); // namedWindow(\"均值滤波【效果图】\"); // // imshow(\"均值滤波【原图】\", img); // // //进行均值滤波操作 //// Mat out; //// boxFilter(img, out, -1, Size(5, 5)); // // // //进行高斯滤波 // Mat out; // GaussianBlur(img, out, Size(0, 0), 0, 0); // // imshow(\"均值滤波【效果图】\", out); // // waitKey(0); //} Mat srcImage, dstImage1, dstImage2, dstImage3; int nBoxFilter = 3; int nBlur = 3; int nGaussianBlur = 3; //轨迹条的回调函数 static void On_BoxFilter(int , void *); //方框滤波 static void On_Blur(int , void *); //均值滤波 static void On_GaussianBlur(int , void *); //高斯滤波 int main() { srcImage = imread(\"/Users/lvqing/Project/Project_OpenCV_C++/image/7.jpg\"); if(!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } dstImage1 = srcImage.clone(); dstImage2 = srcImage.clone(); dstImage3 = srcImage.clone(); namedWindow(\"原图\", 1); imshow(\"原图\", srcImage); namedWindow(\"方框滤波\", 1); createTrackbar(\"内核\", \"方框滤波\", \u0026nBoxFilter, 40, On_BoxFilter); On_BoxFilter(nBoxFilter, 0); namedWindow(\"均值滤波\", 1); createTrackbar(\"内核\", \"均值滤波\", \u0026nBlur, 40, On_Blur); On_Blur(nBlur, 0); namedWindow(\"高斯滤波\", 1); createTrackbar(\"内核\", \"高斯滤波\", \u0026nGaussianBlur, 40, On_GaussianBlur); On_GaussianBlur(nGaussianBlur, 0); while(char(waitKey(1)) != 'q'){} return 0; } static void On_BoxFilter(int , void *) { boxFilter(srcImage, dstImage1, -1, Size( nBoxFilter + 1, nBoxFilter + 1)); imshow(\"方框滤波\", dstImage1); } static void On_Blur(int , void *) { blur(srcImage, dstImage2, Size( nBlur + 1, nBlur + 1), Point(-1, -1)); imshow(\"均值滤波\", dstImage2); } static void On_GaussianBlur(int , void *) { GaussianBlur(srcImage, dstImage3, Size( nGaussianBlur * 2 + 1, nGaussianBlur * 2 + 1), 0, 0); imshow(\"高斯滤波\", dstImage3); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#方框滤波均值滤波高斯滤波"},{"categories":["OpenCV"],"content":" 非线性滤波： 中值滤波、双边滤波 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcimg = imread(\"4.jpg\"); if (!srcimg.data) { printf(\"Oh, no, srcimg is error\"); return -1; } //namedWindow(\"MedianBlur SrcImage\"); namedWindow(\"BilateralFilter SrcImage\"); //imshow(\"MedianBlur\", srcimg); imshow(\"BilateralFilter\", srcimg); //namedWindow(\"MedianBlur out\"); namedWindow(\"BilateralFilter out\"); Mat out; //medianBlur(srcimg, out, 7); bilateralFilter(srcimg, out,25, 25*2, 25/2); //imshow(\"MedianBlur out\", out); imshow(\"BilateralFilter out\", out); waitKey(0); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:2","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#非线性滤波"},{"categories":["OpenCV"],"content":" 非线性滤波： 中值滤波、双边滤波 #include #include #include #include #include using namespace std; using namespace cv; int main() { Mat srcimg = imread(\"4.jpg\"); if (!srcimg.data) { printf(\"Oh, no, srcimg is error\"); return -1; } //namedWindow(\"MedianBlur SrcImage\"); namedWindow(\"BilateralFilter SrcImage\"); //imshow(\"MedianBlur\", srcimg); imshow(\"BilateralFilter\", srcimg); //namedWindow(\"MedianBlur out\"); namedWindow(\"BilateralFilter out\"); Mat out; //medianBlur(srcimg, out, 7); bilateralFilter(srcimg, out,25, 25*2, 25/2); //imshow(\"MedianBlur out\", out); imshow(\"BilateralFilter out\", out); waitKey(0); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:2","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#中值滤波双边滤波"},{"categories":["OpenCV"],"content":" 线性、非线性总结 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; Mat srcImage, dstImage1, dstImage2, dstImage3, dstImage4, dstImage5; int n_boxFilter = 6; //方框滤波 int n_blur = 10; //均值滤波 int n_gaussianBlur = 10; //高斯滤波 int n_medianBlur = 6; //中值滤波 int n_bilateralFilter = 10; //双边滤波 //回调函数声明 static void BoxFilter(int, void *); static void Blur(int, void *); static void GaussianBlur(int, void *); static void MedianBlur(int, void *); static void BilateralFilter(int, void *); int main() { srcImage = imread(\"4.jpg\"); if (!srcImage.data) { printf(\"Oh, no, SrcImage is error\"); return -1; } dstImage1 = srcImage.clone(); dstImage2 = srcImage.clone(); dstImage3 = srcImage.clone(); dstImage4 = srcImage.clone(); dstImage5 = srcImage.clone(); namedWindow(\"SrcImage\"); imshow(\"SrcImage\", srcImage); namedWindow(\"BoxFilter\"); createTrackbar(\"box\", \"BoxFilter\", \u0026n_boxFilter, 40, BoxFilter); BoxFilter(n_boxFilter, 0); namedWindow(\"Blur\"); createTrackbar(\"blur\", \"Blur\", \u0026n_blur, 40, Blur); Blur(n_blur, 0); namedWindow(\"GaussianBlur\"); createTrackbar(\"gaussian\", \"GaussianBlur\", \u0026n_gaussianBlur, 40, GaussianBlur); GaussianBlur(n_gaussianBlur, 0); namedWindow(\"MedianBlur\"); createTrackbar(\"median\", \"MedianBlur\", \u0026n_medianBlur, 50, MedianBlur); MedianBlur(n_medianBlur, 0); namedWindow(\"BilateralFilter\"); createTrackbar(\"bilateral\", \"BilateralFilter\", \u0026n_bilateralFilter, 50, BilateralFilter); BilateralFilter(n_bilateralFilter, 0); while (char(waitKey(1) != '9')) {} return 0; } static void BoxFilter(int, void *) { boxFilter(srcImage, dstImage1, -1, Size(n_boxFilter + 1, n_boxFilter + 1)); imshow(\"BoxFilter\", dstImage1); } static void Blur(int, void *) { blur(srcImage, dstImage2, Size(n_blur + 1, n_blur + 1), Point(-1, -1)); imshow(\"Blur\", dstImage2); } static void GaussianBlur(int, void *) { GaussianBlur(srcImage, dstImage3, Size(n_gaussianBlur * 2 + 1, n_gaussianBlur * 2 + 1), 0, 0); imshow(\"n_gaussianBlur\", 0); } static void MedianBlur(int, void *) { medianBlur(srcImage, dstImage4, n_medianBlur * 2 + 1); imshow(\"n_medianBlur\", 0); } static void BilateralFilter(int, void *) { bilateralFilter(srcImage, dstImage5, n_bilateralFilter, n_bilateralFilter * 2, n_bilateralFilter / 2); imshow(\"n_bilateralFilter\", 0); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:3","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#线性非线性总结"},{"categories":["OpenCV"],"content":" 形态学滤波： 膨胀与腐蚀 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"dilate\"); namedWindow(\"erode\"); imshow(\"srcImg\", srcImg); Mat out1; Mat out2; Mat element = getStructuringElement(MORPH_RECT, Size(15, 15)); //膨胀操作 dilate(srcImg, out1, element); //腐蚀操作 erode(srcImg, out2, element); imshow(\"dilate\", out1); imshow(\"erode\", out2); waitKey(0); return 0; } //使用滚动条进行膨胀腐蚀操作 #include \u003ciostream\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; Mat srcImg, dstImg; int g_nStructElementSize = 3; int g_nTrackbarNumber = 1; static void on_event(int, void *); static void on_chengdu(int, void *); void process(); int main() { srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh. no. srcImg is error\"); return -1; } namedWindow(\"SrcImg\"); namedWindow(\"dande\"); imshow(\"srcImg\", srcImg); createTrackbar(\"dande\", \"dande\", \u0026g_nTrackbarNumber, 1, on_event); createTrackbar(\"chengdu\", \"dande\", \u0026g_nStructElementSize, 5, on_chengdu); while (char(waitKey(1) != 'q')) {} return 0; } static void on_event(int, void *) { process(); } static void on_chengdu(int, void *) { process(); } void process() { Mat element = getStructuringElement(MORPH_RECT, Size(2 * g_nStructElementSize + 1, 2 * g_nStructElementSize + 1), Point(g_nStructElementSize, g_nStructElementSize)); if (g_nTrackbarNumber == 0) { dilate(srcImg, dstImg, element); } else { erode(srcImg, dstImg, element); } imshow(\"dande\", dstImg); } 开运算、闭运算、形态学梯度、顶帽、黑帽 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"Open\"); namedWindow(\"Close\"); namedWindow(\"Gradient\"); namedWindow(\"TopHat\"); namedWindow(\"BlackHat\"); imshow(\"srcImg\", srcImg); Mat element = getStructuringElement(MORPH_RECT, Size(15, 15)); Mat out1, out2, out3, out4, out5; //先腐蚀后膨胀 morphologyEx(srcImg, out1, MORPH_OPEN, element); //先膨胀后腐蚀 morphologyEx(srcImg, out2, MORPH_CLOSE, element); //膨胀图 - 腐蚀图 morphologyEx(srcImg, out3, MORPH_GRADIENT, element); //原图 - 开运算图 morphologyEx(srcImg, out4, MORPH_TOPHAT, element); //闭运算 - 原图 morphologyEx(srcImg, out5, MORPH_BLACKHAT, element); imshow(\"Open\", out1); imshow(\"Close\", out2); imshow(\"Gradient\", out3); imshow(\"Tophat\", out4); imshow(\"Blackhat\", out5); waitKey(0); return 0; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:4","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#形态学滤波"},{"categories":["OpenCV"],"content":" 形态学滤波： 膨胀与腐蚀 #include #include #include #include using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"dilate\"); namedWindow(\"erode\"); imshow(\"srcImg\", srcImg); Mat out1; Mat out2; Mat element = getStructuringElement(MORPH_RECT, Size(15, 15)); //膨胀操作 dilate(srcImg, out1, element); //腐蚀操作 erode(srcImg, out2, element); imshow(\"dilate\", out1); imshow(\"erode\", out2); waitKey(0); return 0; } //使用滚动条进行膨胀腐蚀操作 #include #include #include #include using namespace std; using namespace cv; Mat srcImg, dstImg; int g_nStructElementSize = 3; int g_nTrackbarNumber = 1; static void on_event(int, void *); static void on_chengdu(int, void *); void process(); int main() { srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh. no. srcImg is error\"); return -1; } namedWindow(\"SrcImg\"); namedWindow(\"dande\"); imshow(\"srcImg\", srcImg); createTrackbar(\"dande\", \"dande\", \u0026g_nTrackbarNumber, 1, on_event); createTrackbar(\"chengdu\", \"dande\", \u0026g_nStructElementSize, 5, on_chengdu); while (char(waitKey(1) != 'q')) {} return 0; } static void on_event(int, void *) { process(); } static void on_chengdu(int, void *) { process(); } void process() { Mat element = getStructuringElement(MORPH_RECT, Size(2 * g_nStructElementSize + 1, 2 * g_nStructElementSize + 1), Point(g_nStructElementSize, g_nStructElementSize)); if (g_nTrackbarNumber == 0) { dilate(srcImg, dstImg, element); } else { erode(srcImg, dstImg, element); } imshow(\"dande\", dstImg); } 开运算、闭运算、形态学梯度、顶帽、黑帽 #include #include #include #include using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"Open\"); namedWindow(\"Close\"); namedWindow(\"Gradient\"); namedWindow(\"TopHat\"); namedWindow(\"BlackHat\"); imshow(\"srcImg\", srcImg); Mat element = getStructuringElement(MORPH_RECT, Size(15, 15)); Mat out1, out2, out3, out4, out5; //先腐蚀后膨胀 morphologyEx(srcImg, out1, MORPH_OPEN, element); //先膨胀后腐蚀 morphologyEx(srcImg, out2, MORPH_CLOSE, element); //膨胀图 - 腐蚀图 morphologyEx(srcImg, out3, MORPH_GRADIENT, element); //原图 - 开运算图 morphologyEx(srcImg, out4, MORPH_TOPHAT, element); //闭运算 - 原图 morphologyEx(srcImg, out5, MORPH_BLACKHAT, element); imshow(\"Open\", out1); imshow(\"Close\", out2); imshow(\"Gradient\", out3); imshow(\"Tophat\", out4); imshow(\"Blackhat\", out5); waitKey(0); return 0; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:4","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#膨胀与腐蚀"},{"categories":["OpenCV"],"content":" 形态学滤波： 膨胀与腐蚀 #include #include #include #include using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"dilate\"); namedWindow(\"erode\"); imshow(\"srcImg\", srcImg); Mat out1; Mat out2; Mat element = getStructuringElement(MORPH_RECT, Size(15, 15)); //膨胀操作 dilate(srcImg, out1, element); //腐蚀操作 erode(srcImg, out2, element); imshow(\"dilate\", out1); imshow(\"erode\", out2); waitKey(0); return 0; } //使用滚动条进行膨胀腐蚀操作 #include #include #include #include using namespace std; using namespace cv; Mat srcImg, dstImg; int g_nStructElementSize = 3; int g_nTrackbarNumber = 1; static void on_event(int, void *); static void on_chengdu(int, void *); void process(); int main() { srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh. no. srcImg is error\"); return -1; } namedWindow(\"SrcImg\"); namedWindow(\"dande\"); imshow(\"srcImg\", srcImg); createTrackbar(\"dande\", \"dande\", \u0026g_nTrackbarNumber, 1, on_event); createTrackbar(\"chengdu\", \"dande\", \u0026g_nStructElementSize, 5, on_chengdu); while (char(waitKey(1) != 'q')) {} return 0; } static void on_event(int, void *) { process(); } static void on_chengdu(int, void *) { process(); } void process() { Mat element = getStructuringElement(MORPH_RECT, Size(2 * g_nStructElementSize + 1, 2 * g_nStructElementSize + 1), Point(g_nStructElementSize, g_nStructElementSize)); if (g_nTrackbarNumber == 0) { dilate(srcImg, dstImg, element); } else { erode(srcImg, dstImg, element); } imshow(\"dande\", dstImg); } 开运算、闭运算、形态学梯度、顶帽、黑帽 #include #include #include #include using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } namedWindow(\"srcImg\"); namedWindow(\"Open\"); namedWindow(\"Close\"); namedWindow(\"Gradient\"); namedWindow(\"TopHat\"); namedWindow(\"BlackHat\"); imshow(\"srcImg\", srcImg); Mat element = getStructuringElement(MORPH_RECT, Size(15, 15)); Mat out1, out2, out3, out4, out5; //先腐蚀后膨胀 morphologyEx(srcImg, out1, MORPH_OPEN, element); //先膨胀后腐蚀 morphologyEx(srcImg, out2, MORPH_CLOSE, element); //膨胀图 - 腐蚀图 morphologyEx(srcImg, out3, MORPH_GRADIENT, element); //原图 - 开运算图 morphologyEx(srcImg, out4, MORPH_TOPHAT, element); //闭运算 - 原图 morphologyEx(srcImg, out5, MORPH_BLACKHAT, element); imshow(\"Open\", out1); imshow(\"Close\", out2); imshow(\"Gradient\", out3); imshow(\"Tophat\", out4); imshow(\"Blackhat\", out5); waitKey(0); return 0; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:4","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#开运算闭运算形态学梯度顶帽黑帽"},{"categories":["OpenCV"],"content":" 漫水填充 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; Mat srcImage; int main() { srcImage = imread(\"4.jpg\"); if(!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } namedWindow(\"srcImage\"); namedWindow(\"floodFill\"); imshow(\"srcImage\", srcImage); Rect ccomp; floodFill(srcImage, Point(50, 300), Scalar(155, 255, 55), \u0026ccomp, Scalar(20, 20, 20), Scalar(20, 20, 20)); imshow(\"floodFill\", srcImage); waitKey(0); return 0; } //漫水填充 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; //定义原始图、目标图、灰度图、掩膜图 Mat srcImage, dstImage; Mat grayImage, maskImage; //漫水填充的模式 int fillMode = 1; //负差最大值、正差最大值 int n_LowDifference = 20, n_UpDifference = 20; //表示floodFill函数标识符低八位的连通值 int Connectivity = 4; //是否为彩色图的标识符布尔值 bool IsColor = true; //是否显示掩膜窗口的布尔值 bool UseMask = false; //新的重新绘制的像素值 int NewMaskVal = 255; static void Show() { //输出一些帮助信息 printf(\"\\n\\n\\n欢迎来到漫水填充示例程序\\n\\n\"); printf(\"\\n\\n\\t按键操作说明：\\n\\n\" \"\\t\\t鼠标点击图中区域- 进行漫水填充操作\\n\" \"\\t\\t键盘按键【ESC】 - 退出程序\\n\" \"\\t\\t键盘按键【1】- 切换彩色图/灰度图模式\\n\" \"\\t\\t键盘按键【2】- 显示/隐藏掩膜窗口\\n\" \"\\t\\t键盘按键【3】- 恢复原始图像\\n\" \"\\t\\t键盘按键【4】- 使用空范围的漫水填充\\n\" \"\\t\\t键盘按键【5】- 使用渐变、固定范围的漫水填充\\n\" \"\\t\\t键盘按键【6】- 使用渐变、浮动范围的漫水填充\\n\" \"\\t\\t键盘按键【7】- 操作标志符的低八度使用4位的连接模式\\n\" \"\\t\\t键盘按键【8】- 操作标志符的低八度使用8位的连接模式\\n\" \"\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tby 晴宝\\n\\n\\n\"); } //鼠标onMouse回调函数 static void onMouse(int event, int x, int y, int, void *) { //若鼠标左键没有按下，便返回 if (event != CV_EVENT_LBUTTONDOWN) return; //调用floodFill函数之前的参数准备部分 Point seed = Point(x, y); int LowDifference = fillMode == 0 ? 0 : n_LowDifference; int UpDifference = fillMode == 0 ? 0 : n_UpDifference; //标志符的0～7位为Connectivity, 8~15位为NewMaskVal左移8位的值, 16～23CV_FLOODFILL_FIXED_RANGE或者0 int flags = Connectivity + (NewMaskVal \u003c\u003c 8) + (fillMode == 1 ? CV_FLOODFILL_FIXED_RANGE : 0); //随机生成bgr值 int b = (unsigned)theRNG() \u0026 255; //随机返回一个0～255之间的值 int g = (unsigned)theRNG() \u0026 255; int r = (unsigned)theRNG() \u0026 255; Rect ccomp; //定义重绘区域的最小边界矩形区域 //在重绘区域像素的新值， 若是彩色图模式，取Scalar(b, g, r)，若是灰色图模式，取Scalar(r * 0.229 + g * 0.587 + b * 0.114) Scalar newVal = IsColor ? Scalar(b, g, r) : Scalar(r * 0.229 + g * 0.587 + b * 0.114); //目标图的赋值 Mat dst = IsColor ? dstImage : grayImage; int area; //正式调用floodFill函数 if (UseMask) { threshold(maskImage, maskImage, 1, 128, CV_THRESH_BINARY); area = floodFill(dstImage, maskImage, seed, newVal, \u0026ccomp, Scalar(LowDifference, LowDifference, LowDifference), Scalar(UpDifference, UpDifference, UpDifference), flags); imshow(\"mask\", maskImage); } else { area = floodFill(dstImage, seed, newVal, \u0026ccomp, Scalar(LowDifference, LowDifference, LowDifference), Scalar(UpDifference, UpDifference, UpDifference), flags); } imshow(\"floodFill\", dstImage); cout \u003c\u003c area \u003c\u003c \" 个像素被重绘\\n\"; } int main() { system(\"color 2F\"); srcImage = imread(\"4.jpg\", 1); if (!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } //显示帮助文档 Show(); //拷贝原图到目标图 srcImage.copyTo(dstImage); //转换三通道的image道灰度图 cvtColor(srcImage, grayImage, COLOR_BGR2GRAY); //利用image的尺寸来初始化掩膜mask maskImage.create(srcImage.rows + 2, srcImage.cols + 2, CV_8UC1); namedWindow(\"floodFill\", CV_WINDOW_AUTOSIZE); //创建Trackbar createTrackbar(\"minus\", \"floodFill\", \u0026n_LowDifference, 255, 0); createTrackbar(\"positive\", \"floodFill\", \u0026n_UpDifference, 255, 0); //鼠标回调函数 setMouseCallback(\"floodFill\", onMouse, 0); //循环轮询按键 while (1) { //先显示效果图 imshow(\"floodFill\", IsColor ? dstImage : grayImage); //获取键盘按键 int c = waitKey(0); //判断ESC是否按下，若按下便退出 if ((c \u0026 255) == 27) { cout \u003c\u003c \"程序退出。。。。。。。。。。\\n\"; break; } //根据按键的不同，进行各种操作 switch ((char)c) { //如果键盘“1”被按下， 效果图在灰度图，彩色图之间互换 case '1': //若原来为彩色，并转化为灰度图，并且将掩膜mask所有元素设置为0 if (IsColor) { cout \u003c\u003c \"键盘“1”被按下，切换彩色/灰度模式，当前操作为将【彩色模式】切换为【灰度模式】\\n\"; cvtColor(srcImage, grayImage, COLOR_BGR2GRAY); maskImage = Scalar::all(0); //将标志符设为false， 表示当前图像不为彩色，而是灰度 IsColor = false; } else///若原来为灰度图，便将原来的彩图image0再次拷贝给image，并且将掩膜m","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:5","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#漫水填充"},{"categories":["OpenCV"],"content":" 图像金字塔和图像尺寸缩放 图像尺寸缩放 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } Mat dstImg1, dstImg2, tmpImg, dstImg3, dstImg4; imshow(\"srcImg\", srcImg); tmpImg = srcImg; //将原图给临时变量 //进行尺寸调整操作 resize(tmpImg, dstImg1, Size(tmpImg.cols / 2, tmpImg.rows / 2), 0, 0, 3); resize(tmpImg, dstImg2, Size(tmpImg.cols * 2, tmpImg.rows * 2), 0, 0, 3); //进行向上取样操作 pyrUp(tmpImg, dstImg3, Size(tmpImg.cols * 2, tmpImg.rows * 2)); //进行向下取样操作 pyrDown(tmpImg, dstImg4, Size(tmpImg.cols / 2, tmpImg.rows / 2)); imshow(\"dstImg1\", dstImg1); imshow(\"dstImg2\", dstImg2); imshow(\"dstImg3\", dstImg3); imshow(\"dstImg4\", dstImg4); waitKey(0); return 0; } #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; //定义全局变量 Mat srcImage, tmpImage, dstImage; //全局函数声明 void Show(); int main() { Show(); srcImage = imread(\"4.jpg\"); if(!srcImage.data) { printf(\"Oh, no , srcImage is error\"); return -1; } namedWindow(\"srcImage\"); namedWindow(\"dstImage\"); imshow(\"srcImage\", srcImage); //参数赋值 tmpImage = srcImage; dstImage = tmpImage; int key = 0; //轮询获取按键信息 while (1) { key = waitKey(9); //读取键值到key变量中 //根据key变量的值，进行不同的操作 switch (key) { case 27://按下ESC return 0; break; case 'q': return 0; break; case 'a': pyrUp(tmpImage, dstImage, Size(tmpImage.cols * 2, tmpImage.rows * 2)); break; case 'd': pyrDown(tmpImage, dstImage, Size(tmpImage.cols / 2, tmpImage.rows / 2)); break; case 'w': resize(tmpImage, dstImage, Size(tmpImage.cols * 2, tmpImage.rows * 2), 0, 0); break; case 'S': resize(tmpImage, dstImage, Size(tmpImage.cols / 2, tmpImage.rows / 2), 0, 0); break; case '3': pyrUp(tmpImage, dstImage, Size(tmpImage.cols * 2, tmpImage.rows * 2)); break; case '4': pyrDown(tmpImage, dstImage, Size(tmpImage.cols / 2, tmpImage.rows / 2)); break; case '1': resize(tmpImage, dstImage, Size(tmpImage.cols * 2, tmpImage.rows * 2), 0, 0); break; case '2': resize(tmpImage, dstImage, Size(tmpImage.cols / 2, tmpImage.rows / 2), 0, 0); break; default: break; } imshow(\"dstImage\", dstImage); tmpImage = dstImage; } return 0; } void Show() { printf(\"\\n\\n\\n\\n欢迎来到图像尺寸缩放程序~\\n\\n\"); printf(\"\\n\\n\\n\\n按键说明：\\n\\n\\n\" \"\\t\\t\\t键盘按键【ESC】或者【Q】 - 退出程序\" \"\\t\\t\\t键盘按键【1】或者【W】 - 进行基于【resize】函数的图片放大\" \"\\t\\t\\t键盘按键【2】或者【S】 - 进行基于【resize】函数的图片缩小\" \"\\t\\t\\t键盘按键【3】或者【A】 - 进行基于【pyrUp】函数的图片放大\" \"\\t\\t\\t键盘按键【4】或者【D】 - 进行基于【pyrDown】函数的图片缩小\" \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t by 晴宝\\n\\n\\n\"); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:6","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#图像金字塔和图像尺寸缩放"},{"categories":["OpenCV"],"content":" 图像金字塔和图像尺寸缩放 图像尺寸缩放 #include #include #include #include using namespace std; using namespace cv; int main() { Mat srcImg = imread(\"4.jpg\"); if (!srcImg.data) { printf(\"Oh, no, srcImg is error\"); return -1; } Mat dstImg1, dstImg2, tmpImg, dstImg3, dstImg4; imshow(\"srcImg\", srcImg); tmpImg = srcImg; //将原图给临时变量 //进行尺寸调整操作 resize(tmpImg, dstImg1, Size(tmpImg.cols / 2, tmpImg.rows / 2), 0, 0, 3); resize(tmpImg, dstImg2, Size(tmpImg.cols * 2, tmpImg.rows * 2), 0, 0, 3); //进行向上取样操作 pyrUp(tmpImg, dstImg3, Size(tmpImg.cols * 2, tmpImg.rows * 2)); //进行向下取样操作 pyrDown(tmpImg, dstImg4, Size(tmpImg.cols / 2, tmpImg.rows / 2)); imshow(\"dstImg1\", dstImg1); imshow(\"dstImg2\", dstImg2); imshow(\"dstImg3\", dstImg3); imshow(\"dstImg4\", dstImg4); waitKey(0); return 0; } #include #include #include #include using namespace std; using namespace cv; //定义全局变量 Mat srcImage, tmpImage, dstImage; //全局函数声明 void Show(); int main() { Show(); srcImage = imread(\"4.jpg\"); if(!srcImage.data) { printf(\"Oh, no , srcImage is error\"); return -1; } namedWindow(\"srcImage\"); namedWindow(\"dstImage\"); imshow(\"srcImage\", srcImage); //参数赋值 tmpImage = srcImage; dstImage = tmpImage; int key = 0; //轮询获取按键信息 while (1) { key = waitKey(9); //读取键值到key变量中 //根据key变量的值，进行不同的操作 switch (key) { case 27://按下ESC return 0; break; case 'q': return 0; break; case 'a': pyrUp(tmpImage, dstImage, Size(tmpImage.cols * 2, tmpImage.rows * 2)); break; case 'd': pyrDown(tmpImage, dstImage, Size(tmpImage.cols / 2, tmpImage.rows / 2)); break; case 'w': resize(tmpImage, dstImage, Size(tmpImage.cols * 2, tmpImage.rows * 2), 0, 0); break; case 'S': resize(tmpImage, dstImage, Size(tmpImage.cols / 2, tmpImage.rows / 2), 0, 0); break; case '3': pyrUp(tmpImage, dstImage, Size(tmpImage.cols * 2, tmpImage.rows * 2)); break; case '4': pyrDown(tmpImage, dstImage, Size(tmpImage.cols / 2, tmpImage.rows / 2)); break; case '1': resize(tmpImage, dstImage, Size(tmpImage.cols * 2, tmpImage.rows * 2), 0, 0); break; case '2': resize(tmpImage, dstImage, Size(tmpImage.cols / 2, tmpImage.rows / 2), 0, 0); break; default: break; } imshow(\"dstImage\", dstImage); tmpImage = dstImage; } return 0; } void Show() { printf(\"\\n\\n\\n\\n欢迎来到图像尺寸缩放程序~\\n\\n\"); printf(\"\\n\\n\\n\\n按键说明：\\n\\n\\n\" \"\\t\\t\\t键盘按键【ESC】或者【Q】 - 退出程序\" \"\\t\\t\\t键盘按键【1】或者【W】 - 进行基于【resize】函数的图片放大\" \"\\t\\t\\t键盘按键【2】或者【S】 - 进行基于【resize】函数的图片缩小\" \"\\t\\t\\t键盘按键【3】或者【A】 - 进行基于【pyrUp】函数的图片放大\" \"\\t\\t\\t键盘按键【4】或者【D】 - 进行基于【pyrDown】函数的图片缩小\" \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\t\\t\\t\\t by 晴宝\\n\\n\\n\"); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/:0:6","series":null,"tags":["OpenCV"],"title":"OpenCV之图像处理","uri":"/opencv%E4%B9%8B%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/#图像尺寸缩放"},{"categories":["OpenCV"],"content":" 初级图像混合 分离颜色通道、多通道图像混合 改变图像对比度和亮度 ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bcore%E7%BB%84%E4%BB%B6/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV之core组件进阶","uri":"/opencv%E4%B9%8Bcore%E7%BB%84%E4%BB%B6/#"},{"categories":["OpenCV"],"content":" 初级图像混合 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e using namespace std; using namespace cv; int main() { Mat srcImage = imread(\"src.jpg\"); Mat logoImage = imread(\"logo.jpg\"); if(!srcImage.data) { printf(\"Oh, no, logoImage is error\"); return false; } if(!logoImage.data) { printf(\"Oh,no, srcImage is error\"); return false; } Mat imageROI; imageROI = srcImage(Rect(500, 250, logoImage.cols, logoImage.rows)); addWeighted(imageROI, 1.0, logoImage, 0.5, 0.0, imageROI); imshow(\"初级图像混合\", srcImage); return 0; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bcore%E7%BB%84%E4%BB%B6/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之core组件进阶","uri":"/opencv%E4%B9%8Bcore%E7%BB%84%E4%BB%B6/#初级图像混合"},{"categories":["OpenCV"],"content":" 分离颜色通道、多通道图像混合 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; bool MutiChannelBlending(); int main() { system(\"close5E\"); if (MutiChannelBlending()) { cout \u003c\u003c endl \u003c\u003c \"嗯嗯，好了，得出你想要的混合图像了\"; } waitKey(0); return 0; } //多通道图像混合的实现函数 bool MutiChannelBlending() { //定义相关变量 Mat srcImage; Mat logoImage; vector\u003cMat\u003e channels; Mat imageBlueChannel; //多通道图像混合——蓝色分量部分 //读入图片 logoImage = imread(\"apple.jpg\", 0); srcImage = imread(\"4.jpg\"); if (!logoImage.data) { printf(\"Oh, no, logoImage失败了吧\"); return false; } if (!srcImage.data) { printf(\"Oh, no srcImage失败了吧\"); return false; } //把一个3通道图像转换成3个单通道图像 split(srcImage, channels); //将原图的绿色通道的引用返回给imageBlueChannel，注意是引用，相当于两者等价， //修改其中一个另一个跟着变 imageBlueChannel = channels.at(0); //将原图的蓝色通道的（500,250）坐标处右下方的一块区域和logo图进行加权操作， //将得到的混合结果存到imageBlueChannel中 addWeighted(imageBlueChannel(Rect(500, 250, logoImage.cols, logoImage.rows)), 1.0, logoImage, 0.5, 0, imageBlueChannel(Rect(500, 250, logoImage.cols, logoImage.rows))); //将三个单通道重新合成一个三通道 merge(channels, srcImage); //显示效果图 namedWindow(\"原图+logo蓝色通道\"); imshow(\"原图+logo蓝色通道\", srcImage); //多通道图像混合——绿色分量部分 Mat imageGreenChannel; srcImage = imread(\"src.jpg\"); logoImage = imread(\"logo.jpg\", 0); if (!srcImage.data) { printf(\"Oh, no, srcImage失败了吧\"); return false; } if (!logoImage.data) { printf(\"Oh，no，logoImage失败了吧\"); return false; } split(srcImage, channels); imageGreenChannel = channels.at(1); addWeighted(imageGreenChannel(Rect(500, 250, logoImage.cols, logoImage.rows)), 1.0, logoImage, 0.5, 0.0, imageGreenChannel(Rect(500, 250, logoImage.cols, logoImage.rows))); merge(channels, srcImage); namedWindow(\"原图+logo绿色通道\"); imshow(\"原图+logo绿色通道\", srcImage); //多通道图像混合——红色分量部分 Mat imageRedChannel; logoImage = imread(\"logo.jpg\", 0); srcImage = imread(\"src.jpg\"); if (!logoImage.data) { printf(\"Oh, no, logoImage失败了吧\"); return false; } if (!srcImage.data) { printf(\"Oh, no, srcImage失败了吧\"); return false; } split(srcImage, channels); imageBlueChannel = channels.at(2); addWeighted(imageRedChannel(Rect(500, 250, logoImage.cols, logoImage.rows)), 1.0, logoImage, 0.5, 0.0, imageRedChannel(Rect(500, 250, logoImage.cols, logoImage.rows))); merge(channels, srcImage); namedWindow(\"原图+logo红色通道\"); imshow(\"原图+logo红色通道\", srcImage); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bcore%E7%BB%84%E4%BB%B6/:0:2","series":null,"tags":["OpenCV"],"title":"OpenCV之core组件进阶","uri":"/opencv%E4%B9%8Bcore%E7%BB%84%E4%BB%B6/#分离颜色通道多通道图像混合"},{"categories":["OpenCV"],"content":" 改变图像对比度和亮度 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e Mat srcImage; Mat dstImage; Mat B_position; //亮度 Mat C_Position; //对比度 int main() { srcImage = imread(\"srcImage.jpg\"); if(!srcImage.data) { printf(\"Oh, no, srcImage is error\"); return -1; } dstImage = Mat::zeros(srcImage.size(), srcImage.type()); //设置对比度、亮度的初值 C_Position = 80; B_position = 80; //创建窗口 namedWindow(\"效果图\", 1); createTrackbar(\"滑动条亮度\", \"效果图\", \u0026B_position, 200, ConstractAndBright); createTrackbar(\"滑动条对比度\", \"效果图\", \u0026C_Position, 300, ConstractAndBright); ConstractAndBright(B_position, 0); ConstractAndBright(C_Position, 0); while(waitKey(1) != 'q'){} return 0; } // //描述：改变图像对比度和亮度的回调函数 // static void ConstractAndBright(int , void *) { namedWindow(\"原图\", 1); for(int y = 0; y \u003c srcImage.rows; y++) { for(int x = 0; x \u003c srcImage.cols; x++) { for(int c = 0; c \u003c 3; c++) { dstImage.at\u003cVec3b\u003e(y, x)[c] = saturate_cast\u003cuchar\u003e( (C_Position * 0.01)*srcImage.at\u003cVec3b\u003e(y,x)[c] + B_position) } } } imshow(\"原图\", srcImage); imshow(\"效果图\", dstImage); } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bcore%E7%BB%84%E4%BB%B6/:0:3","series":null,"tags":["OpenCV"],"title":"OpenCV之core组件进阶","uri":"/opencv%E4%B9%8Bcore%E7%BB%84%E4%BB%B6/#改变图像对比度和亮度"},{"categories":["OpenCV"],"content":" 滑动条的创建使用 ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bhighgui%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2/:0:0","series":null,"tags":["OpenCV"],"title":"OpenCV之HighGUI图形用户界面初步","uri":"/opencv%E4%B9%8Bhighgui%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2/#"},{"categories":["OpenCV"],"content":" 滑动条的创建使用 #include \u003ciostream\u003e #include \u003copencv2/opencv.hpp\u003e #include \u003copencv2/core/core.hpp\u003e #include \u003copencv2/imgproc/imgproc.hpp\u003e #include \u003copencv2/highgui/highgui.hpp\u003e using namespace std; using namespace cv; //全局函数声明 Mat img; int threshval = 160; //轨迹条滑块对应的值，给初值160 // //描述：轨迹条的回调函数 // static void on_trackbar(int, void*) { Mat bw = threshval \u003c 128 ? (img \u003c threshval):(img \u003e threshval); //定义点和向量 vector\u003cvector\u003cPoint\u003e\u003e contours; vector\u003cVec4i\u003e hierarchy; //查找轮廓 findContours(bw, contours, hierarchy, CV_RETR_CCOMP, CV_CHAIN_APPROX_SIMPLE); //初始化dst Mat dst = Mat::zeros(img.size(), CV_8UC3); //开始处理 if(!contours.empty() \u0026\u0026 !hierarchy.empty()) { //遍历所有顶层轮廓，随机生成颜色值绘制个连接组成部分 int idx = 0; for( ; idx \u003e= 0; idx = hierarchy[idx][0]) { Scalar color((rand()\u0026255), (rand()\u0026255), (rand()\u0026255)); //绘制填充轮廓 drawContours(dst, contours, idx, color, CV_FILLED, 8, hierarchy); } } //显示窗口 imshow(\"Connected Components\", dst); } int main() { img = imread(\"img.jpg\"); if(!img.data) { printf(\"Oh, no, img失败了 \\n\"); return -1; } //显示原图 namedWindow(\"Image\", 1); imshow(\"Image\", img); //创建处理窗口 namedWindow(\"Connected Components\", 1); //创建轨迹条 createTrackbar(\"Threshold\", \"Connected Components\", \u0026threshval, 255, on_trackbar); on_trackbar(threshval, 0); //轨迹条回调函数 waitKey(0); return 0; } ","date":"2019-03-28","objectID":"/opencv%E4%B9%8Bhighgui%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2/:0:1","series":null,"tags":["OpenCV"],"title":"OpenCV之HighGUI图形用户界面初步","uri":"/opencv%E4%B9%8Bhighgui%E5%9B%BE%E5%BD%A2%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2/#滑动条的创建使用"},{"categories":["剑指Offer"],"content":" 题目描述： 输入两个链表，找出它们的第一个公共结点。 ","date":"2019-03-27","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9/:0:1","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之两个链表的第一个公共结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(n)$,空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* FindFirstCommonNode( ListNode* pHead1, ListNode* pHead2) { ListNode *ff; ListNode *h1 = pHead1, *h2 = pHead2; int count1 = 0, count2 = 0, count3 = 0; while(h1) { count1++; h1 = h1-\u003enext; } while(h2) { count2++; h2 = h2-\u003enext; } if(count1 \u003e= count2) { count3 = count1 - count2; while(count3) { pHead1 = pHead1-\u003enext; count3--; } } if(count1 \u003c count2) { count3 = count2 - count1; while(count3) { pHead2 = pHead2-\u003enext; count3--; } } while(pHead1 \u0026\u0026 pHead2) { if(pHead1 == pHead2) { ff = pHead2; break; } pHead1 = pHead1-\u003enext; pHead2 = pHead2-\u003enext; } return ff; } }; ","date":"2019-03-27","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9/:0:2","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之两个链表的第一个公共结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度：$O(n)$,空间复杂度：$O(1)$. /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* FindFirstCommonNode( ListNode* pHead1, ListNode* pHead2) { ListNode *p1 = pHead1; ListNode *p2 = pHead2; while(p1!=p2) { p1 = (p1==NULL ? pHead2 : p1-\u003enext); p2 = (p2==NULL ? pHead1 : p2-\u003enext); } return p1; } }; ","date":"2019-03-27","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9/:0:3","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之两个链表的第一个公共结点","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%A4%E4%B8%AA%E9%93%BE%E8%A1%A8%E7%9A%84%E7%AC%AC%E4%B8%80%E4%B8%AA%E5%85%AC%E5%85%B1%E7%BB%93%E7%82%B9/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 求出1~13的整数中1出现的次数,并算出100~1300的整数中1出现的次数？为此他特别数了一下1~13中包含1的数字有1、10、11、12、13因此共出现6次,但是对于后面问题他就没辙了。ACMer希望你们帮帮他,并把问题更加普遍化,可以很快的求出任意非负整数区间中1出现的次数（从1 到 n 中1出现的次数）。 ","date":"2019-03-27","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之整数中1出现的次数（从1到n整数中1出现的次数）","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路一： 时间复杂度：$O(n^2)$, 空间复杂度：$O(1)$. class Solution { public: int NumberOf1Between1AndN_Solution(int n) { int count = 0; for(int i = 1; i \u003c= n; i++) { string ss = to_string(i); for(int j = 0; j \u003c ss.length(); j++) { if(ss[j] == '1') count++; } } return count; } }; ","date":"2019-03-27","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之整数中1出现的次数（从1到n整数中1出现的次数）","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/#解题思路一"},{"categories":["剑指Offer"],"content":" 解题思路二： 时间复杂度：$O(n^2)$, 空间复杂度：$O(1)$. class Solution { public: int NumberOf1Between1AndN_Solution(int n) { int count=0; if(n\u003c1) return 0; for(int i=1;i\u003c=n;++i) { int temp=i; while(temp) { if(temp%10==1) ++count; temp/=10; } } return count; } }; ","date":"2019-03-27","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/:0:3","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之整数中1出现的次数（从1到n整数中1出现的次数）","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0%E4%BB%8E1%E5%88%B0n%E6%95%B4%E6%95%B0%E4%B8%AD1%E5%87%BA%E7%8E%B0%E7%9A%84%E6%AC%A1%E6%95%B0/#解题思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一棵二叉树，判断该二叉树是否是平衡二叉树。 ","date":"2019-03-26","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之平衡二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路: 时间复杂度: $O(n^2)$,空间复杂度：$O(1)$. class Solution { public: // 递归的判断是否是平衡二叉树 bool IsBalanced_Solution(TreeNode* pRoot) { // 如果初始节点是NULL，直接返回true if(pRoot == NULL ) return true; int left = 0, right = 0; // 递归计算左子树的深度 left = dep_tree(pRoot-\u003eleft); // 递归的计算右子树的深度 right = dep_tree(pRoot-\u003eright); // 判断两个深度的差值是否大于1，如果大于1，返回false if(abs(left - right) \u003e 1) return false; // 否则，递归的计算左子树和右子树 else return IsBalanced_Solution(pRoot -\u003e left) \u0026\u0026 IsBalanced_Solution(pRoot -\u003e right); } // 计算树的深度 int dep_tree(TreeNode* pRoot) { if(!pRoot) return 0; return max(dep_tree(pRoot-\u003eleft),dep_tree(pRoot-\u003eright))+1; } }; ","date":"2019-03-26","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之平衡二叉树","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%B9%B3%E8%A1%A1%E4%BA%8C%E5%8F%89%E6%A0%91/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 ","date":"2019-03-25","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之二进制中1的个数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 如果一个整数不为0，那么这个整数至少有一位是1。如果我们把这个整数减1，那么原来处在整数最右边的1就会变为0，原来在1后面的所有的0都会变成1(如果最右边的1后面还有0的话)。其余所有位将不会受到影响。 举个例子：一个二进制数1100，从右边数起第三位是处于最右边的一个1。减去1后，第三位变成0，它后面的两位0变成了1，而前面的1保持不变，因此得到的结果是1011.我们发现减1的结果是把最右边的一个1开始的所有位都取反了。这个时候如果我们再把原来的整数和减去1之后的结果做与运算，从原来整数最右边一个1那一位开始所有位都会变成0。如1100\u00261011=1000.也就是说，把一个整数减去1，再和原整数做与运算，会把该整数最右边一个1变成0.那么一个整数的二进制有多少个1，就可以进行多少次这样的操作。 时间复杂度：$O(n)$,空间复杂度: $O(1)$. class Solution { public: int NumberOf1(int n) { int cnt = 0; while(n != 0) { cnt++; n = n \u0026(n-1); } return cnt; } }; ","date":"2019-03-25","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之二进制中1的个数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ ","date":"2019-03-23","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之矩形覆盖","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度：$O(n)$, 空间复杂度：$O(n)$. class Solution { public: int rectCover(int number) { vector\u003cint\u003e vec(number + 1); vec[0] = 0; vec[1] = 1; vec[2] = 2; for(int i = 3; i \u003c= number; i++) vec[i] = vec[i - 1] + vec[i - 2]; return vec[number]; } }; ","date":"2019-03-23","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之矩形覆盖","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： HZ偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为8(从第0个开始,到第3个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是1) ","date":"2019-03-23","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之连续子数组的最大和","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度：$O(n)$, 空间复杂度：$O(n)$. class Solution { public: int FindGreatestSumOfSubArray(vector\u003cint\u003e array) { vector\u003cint\u003e vec(array.size()); vec[0] = array[0]; int temp = array[0]; for(int i = 1; i \u003c array.size(); i++) { if(temp + array[i] \u003e= array[i]) vec[i] = temp + array[i]; else vec[i] = array[i]; temp = vec[i]; } return *max_element(vec.begin(), vec.end()); } }; 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. class Solution { public: int FindGreatestSumOfSubArray(vector\u003cint\u003e array) { int cursum=array[0]; int maxsum=array[0]; for(int i=1;i\u003carray.size();i++) { cursum+=array[i]; if(cursum\u003carray[i]) cursum=array[i]; if(cursum\u003emaxsum) maxsum=cursum; } return maxsum; } }; ","date":"2019-03-23","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之连续子数组的最大和","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%BF%9E%E7%BB%AD%E5%AD%90%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%A4%A7%E5%92%8C/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型 ","date":"2019-03-23","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/:0:1","series":null,"tags":["剑指Offer","stack"],"title":"剑指Offer之用两个栈实现队列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { // 解题思路： // 两个栈，s2加入新数据 // s1用来颠倒数据 public: // push 函数 void push(int node) { // 首先，将s2中的数据出栈放到s1中 while(!s2.empty()) { s1.push(s2.top()); s2.pop(); } // 将新数据放到s2中 s2.push(node); // 如果s1中的数据不为空，将数据放出栈放到s2中 while(!s1.empty()) { s2.push(s1.top()); s1.pop(); } } // 出栈数据 int pop() { int x; x = s2.top(); s2.pop(); return x; } private: stack\u003cint\u003e s1; stack\u003cint\u003e s2; }; ","date":"2019-03-23","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/:0:2","series":null,"tags":["剑指Offer","stack"],"title":"剑指Offer之用两个栈实现队列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 求1+2+3+…+n，要求不能使用乘除法、for、while、if、else、switch、case等关键字及条件判断语句（A?B:C）。 ","date":"2019-03-22","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%B1%821-2-3-n/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之求1+2+3+...+n","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%B1%821-2-3-n/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$， 空间复杂度: $O(1)$. class Solution { public: int Sum_Solution(int n) { int res = n; res \u0026\u0026 (res += Sum_Solution(--n)); //与操作，前面为假，后面就不执行 return res; } }; ","date":"2019-03-22","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%B1%821-2-3-n/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之求1+2+3+...+n","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%B1%821-2-3-n/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 给定一个数组A[0,1,…,n-1],请构建一个数组B[0,1,…,n-1],其中B中的元素$$B[i]=A[0]*A[1]*…*A[i-1]*A[i+1]*…*A[n-1]$$。不能使用除法。 ","date":"2019-03-22","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9E%84%E5%BB%BA%E4%B9%98%E7%A7%AF%E6%95%B0%E7%BB%84/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之构建乘积数组","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9E%84%E5%BB%BA%E4%B9%98%E7%A7%AF%E6%95%B0%E7%BB%84/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n^2)$, 空间复杂度: $O(n)$. class Solution { public: vector\u003cint\u003e multiply(const vector\u003cint\u003e\u0026 A) { vector\u003cint\u003e vec(A.size()); for(int j = 0; j \u003c= A.size()-1; j++) { int temp = 1; for(int i = 0; i \u003c= A.size()-1; i++) { if(i != j) temp = A[i] * temp; } vec[j] = temp; } return vec; } }; 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { public: vector\u003cint\u003e multiply(const vector\u003cint\u003e\u0026 A) { int n=A.size(); vector\u003cint\u003e b(n); int ret=1; for(int i=0;i\u003cn;ret*=A[i++]){ //关键是for语句中的第三条语句，因为它是在循环体之后执行的。 b[i]=ret; //第二是两个for语句中循环体的不同。 } ret=1; for(int i=n-1;i\u003e=0;ret*=A[i--]){ b[i]*=ret; } return b; } }; ","date":"2019-03-22","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9E%84%E5%BB%BA%E4%B9%98%E7%A7%AF%E6%95%B0%E7%BB%84/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之构建乘积数组","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9E%84%E5%BB%BA%E4%B9%98%E7%A7%AF%E6%95%B0%E7%BB%84/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述：操作给定的二叉树，将其变换为源二叉树的镜像。 ","date":"2019-03-21","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树的镜像","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/#题目描述"},{"categories":["剑指Offer"],"content":" 输入描述: 二叉树的镜像定义：源二叉树 8 / \\ 6 10 / \\ / \\ 5 7 9 11 镜像二叉树 8 / \\ 10 6 / \\ / \\ 11 9 7 5 ","date":"2019-03-21","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/:1:0","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树的镜像","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/#输入描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } };*/ class Solution { public: void Mirror(TreeNode *pRoot) { if(pRoot == NULL) return; TreeNode *temp; temp = pRoot-\u003eright; pRoot-\u003eright = pRoot-\u003eleft; pRoot-\u003eleft = temp; Mirror(pRoot-\u003eleft); Mirror(pRoot-\u003eright); } }; ","date":"2019-03-21","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/:1:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树的镜像","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%95%9C%E5%83%8F/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述 写一个函数，求两个整数之和，要求在函数体内不得使用+、-、*、/四则运算符号。 ","date":"2019-03-21","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E5%81%9A%E5%8A%A0%E6%B3%95/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之不用加减乘除","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E5%81%9A%E5%8A%A0%E6%B3%95/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路 加法运算 class Solution { public: int Add(int num1, int num2) { return (num2 == 0)? num1:Add(num1^num2, (num1 \u0026 num2) \u003c\u003c 1); } }; 减法运算 #include \u003ciostream\u003e using namespace std; int add(int num1, int num2) { return (num2 == 0)? num1:add(num1^num2, (num1 \u0026 num2) \u003c\u003c 1); } //求n的相反数 //~：按位取反 //add：加法操作，末位加一 int negative(int n) { return add(~n, 1); } int subtraction(int n1, int n2) { //加上被减数的相反数 return add(n1, negative(n2)); } int main() { int sub = 0; sub = subtraction(-1, 2); cout \u003c\u003c sub; return 0; } ","date":"2019-03-21","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E5%81%9A%E5%8A%A0%E6%B3%95/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之不用加减乘除","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%B8%8D%E7%94%A8%E5%8A%A0%E5%87%8F%E4%B9%98%E9%99%A4%E5%81%9A%E5%8A%A0%E6%B3%95/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述 输入一棵二叉树，求该树的深度。从根结点到叶结点依次经过的结点（含根、叶结点）形成树的一条路径，最长路径的长度为树的深度。 ","date":"2019-03-21","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/:0:1","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树的深度","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路 时间复杂度：${O(n)}$,空间复杂度：$O(1)$. /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } };*/ class Solution{ public: int TreeDepth(TreeNode* pRoot) { int count_left = 0; int count_right = 0; if(pRoot == NULL) return 0; if(pRoot-\u003eleft != NULL) count_left += TreeDepth(pRoot-\u003eleft); if(pRoot-\u003eright != NULL) count_right += TreeDepth(pRoot-\u003eright); return max(count_left, count_right)+1; } }; class Solution { public: int TreeDepth(TreeNode* pRoot) { if(pRoot == NULL) return 0; return max(TreeDepth(pRoot-\u003eleft), TreeDepth(pRoot-\u003eright))+1; } }; ","date":"2019-03-21","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/:0:2","series":null,"tags":["剑指Offer","tree"],"title":"剑指Offer之二叉树的深度","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E6%B7%B1%E5%BA%A6/#解题思路"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#回归与分类"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#回归树与分类树"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#信息熵"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#id3算法"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#c45算法"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#cart算法"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#决策树剪枝"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#决策树算法小结"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#决策树的优缺点"},{"categories":["机器学习"],"content":"决策树算法在机器学习中算是很经典的一个算法系列了。它既可以作为分类算法，也可以作为回归算法，同时也特别适合集成学习比如随机森林，更重要的是CART树是学习GBDT，XGBoost的基础。因此，本文作为复习笔记，记录决策树的理论知识。 回归与分类在机器学习中，经常会遇见两种问题：回归与分类。基本上所有的机器学习任务就是处理回归与分类，所谓的分类问题就是使用已知的被分成多个类别（离散）的训练数据，训练机器学习模型，然后对于新的数据，使用模型进行分类，给出每个样本所属的类别，而回归问题就是将已知的连续变量的训练数据拟合成一个模型函数，对于新的数据，使用模型函数来预测其对应的结果。 这两者的区别就在于输出变量的类型。回归是定量输出，或者说是预测连续变量；分类问题书定量输出，预测离散变量。 如何区分分类与回归，看的不是输入，而是输出。举个例子，预测明天晴或是雨，是分类问题，而预测明天温度，则是回归问题。 回归树与分类树在决策树中，也有回归树与分类树的概念。在二者的区别中，回归树是采用最大均方误差来划分节点，并且每个节点样本的均值作为测试样本的回归预测值；而分类树是采用信息增益或者是信息增益比来划分节点，每个节点样本的类别情况投 票决定测试样本的类别。我们可以看到，这两者的区别主要在于划分方式与工作模式。回归树采用最大均方误差这种对数据精确处理的方式，输出连续变量，可以更好地给我们的数据进行预测；而分类树使用一个非常宽泛的信息增益这个变量，更好的从整体把握这个数据集的分类。 信息熵1970年代，一个叫昆兰的大牛找到了用信息论中的熵来度量决策树的决策选择过程，方法一出，它的简洁和高效就引起了轰动，昆兰把这个算法叫做ID3。 信息论中的熵表示的是不确定性的度量。越不确定的事物，它的熵就越大。 具体的，随机变量X的熵的表达式如下： $$H(X) = -\\sum_{i=1}^{n}p_i log(p_i) \\tag{1}$$ 其中n代表X的n种不同的离散取值。而$p_i$代表了X取值为i的概率，log为以2或者e为底的对数。 联合熵描述的是一对随机变量X和Y的不确定性： $$H(X,Y) = -\\sum_{i=1}^np(x_i,y_i)log(p(x_i,y_i)) \\tag{2}$$ 条件熵是指：在一个随机变量Y已知的情况下，另一个随机变量X的不确定性: $$H(X|Y) = -\\sum_{i = 1}^np(x_i,y_i)log(p(x_i|y_i)) \\tag{3}$$ H(X)度量了X的不确定性，条件熵H(X|Y)度量了我们在知道Y以后X剩下的不确定性，那么H(X)-H(X|Y)呢？它度量了X在知道Y以后不确定性减少程度，这个度量我们在信息论中称为互信息，记为I(X,Y)。在决策树ID3算法中叫做信息增益。ID3算法就是用信息增益来判断当前节点应该用什么特征来构建决策树。信息增益大，则越适合用来分类。 用上面这个图来表示之前的熵公式。左边的椭圆代表H(X),右边的椭圆代表H(Y),中间重合的部分就是互信息或者信息增益I(X,Y), 左边的椭圆去掉重合部分就是H(X|Y),右边的椭圆去掉重合部分就是H(Y|X),两个椭圆的并就是H(X,Y)。 ID3算法 ID3算法的核心实在决策树上的各个节点上用 信息增益 选择特征。在ID3算法生成树的时候，是先计算所有备选特征的信息增益，然后再选择下一个节点是哪一个特征。 下面是《统计学习方法》的例子 ID3算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 ID3的缺点：　ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大。 ID3算法对于缺失值的情况没有做考虑 没有考虑过拟合的问题 ID3 算法的作者昆兰基于上述不足，对ID3算法做了改进，这就是C4.5算法。 C4.5算法 ID3算法有四个主要的不足，一是不能处理连续特征，第二个就是用信息增益作为标准容易偏向于取值较多的特征，最后两个是缺失值处理的问和过拟合问题。C4.5算法中改进了上述4个问题。 问题一：连续特征处理： C4.5的思路是将连续的特征离散化。比如m个样本的连续特征A有m个，从小到大排列为a1,a2,...,am,则C4.5取相邻两样本值的平均数，一共取得m-1个划分点，其中第i个划分点Ti表示为：$Ti = \\frac{a_i+a_i+1}{2}$。对于这m-1个点，分别计算以该点作为二元分类点时的信息增益。选择信息增益最大的点作为该连续特征的二元离散分类点。比如取到的增益最大的点为$a_t$，则小于$a_t$的值为类别1，大于$a_t$的值为类别2，这样我们就做到了连续特征的离散化。要注意的是，与离散属性不同的是，如果当前节点为连续属性，则该属性后面还可以参与子节点的产生选择过程。 问题二：，信息增益作为标准容易偏向于取值较多的特征的问题： C4.5使用信息增益比的变量$I_R(X,Y)$，它是信息增益和特征熵的比值。表达式如下： $$I_R(D,A)=\\frac{I(A,D)}{H_A(D)} \\tag{4}$$ 其中D为样本特征输出的集合，A为样本特征，对于特征熵$H_A(D)$, 表达式如下: $$H_A(D)=-\\sum_{i=1}^n \\frac{D_i}{D}log_2\\frac{D_i}{D}\\tag{5}$$ 其中n为特征A的类别数， Di为特征A的第i个取值对应的样本个数。D为样本个数。 特征数越多的特征对应的特征熵越大，它作为分母，可以校正信息增益容易偏向于取值较多的特征的问题。 通过对比信息增益公式与信息增益比公式，我们可以看出，信息增益就是特征与训练集的互信息，或者说原来数据集的不确定性与确定其中一个特征之后的不确定性之差，称做信息增益。也就是确定这个特征所引入的信息量。而信息增益比则是这一个互信息与D的不确定性的比值。 问题三：缺失值处理: 对于缺失值，主要需要解决的是两个问题，一是在样本某些特征缺失的情况下选择划分的属性，二是选定了划分属性，对于在该属性上缺失特征的样本的处理。 对于第一个子问题，对于某一个有缺失特征值的特征A。C4.5的思路是将数据分成两部分，对每个样本设置一个权重（初始可以都为1），然后划分数据，一部分是有特征值A的数据D1，另一部分是没有特征A的数据D2. 然后对于没有缺失特征A的数据集D1来和对应的A特征的各个特征值一起计算加权重后的信息增益比，最后乘上一个系数，这个系数是无特征A缺失的样本加权后所占加权总样本的比例。 对于第二个子问题，可以将缺失特征的样本同时划分入所有的子节点，不过将该样本的权重按各个子节点样本的数量比例来分配。比如缺失特征A的样本a之前权重为1，特征A有3个特征值A1,A2,A3。 3个特征值对应的无缺失A特征的样本个数为2,3,4.则a同时划分入A1，A2，A3。对应权重调节为2/9,3/9,4/9。 问题四：过拟合问题： C4.5引入了正则化系数进行初步的剪枝。 C4.5算法步骤： 输入：训练数据集D,特征集A，阈值e； 输出:决策树T 1)若D中所有实例属于同一类Ck，则T为单结点树，并将Ck作为该结点的类标记，返回T； 2)若A=空集，则T为单结点树，并将D中的实例数最大的类Ck作为该结点的类标记，返回T； 3)否则，计算A中各特征对D的信息增益比，选择信息增益比最大的特征Ag； 4)如果Ag的信息增益小于阈值e，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T； 5)否则，对Ag的每一个可能值ai；依Ag=ai将D分割为若干非空子集Di；将Di中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树T，返回T； 6)对第i个子结点，以Di为训练集，以A-{Ag}为特征集，递归调用（1）~（5），得到子树Ti，返回Ti。 C4.5的缺点： 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。思路主要是两种，一种是预剪枝，即在生成决策树的时候就决定是否剪枝。另一个是后剪枝，即先生成决策树，再通过交叉验证来剪枝。 C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话","date":"2019-03-16","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/:0:0","series":null,"tags":["机器学习"],"title":"机器学习复习笔记之决策树","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91/#参考链接"},{"categories":["机器学习"],"content":"要想知道什么是机器学习，我们就需要首先理解机器是什么，机器就是电脑、芯片、代码这些东西。让电脑遵循人的指令，完成一件特定的任务从计算机发明那天开始就在研究了，现在的各种编程语言、数据结构和编程算法等都是在做这个。但是它们只能依赖于程序员输入的确定的代码才能 work，也就是说他们不能“自己学习”，这样对于有些问题就很尴尬，比如检测一张图片中有几个人，识别一句话中提到了几个人名，识别一张图片是不是黄图等等。这些任务要是写一个代码，依靠规则去实现，那还是非常困难。但是我们可以换个思路，让我们写代码去实现这些功能很困难，但是让我们去给图片打标签（比如给一批图，人为的打上是不是黄图的标签…）不是很简单的嘛，我们如果能让机器自己从打好标签的图片中自己学习那些是黄图不就万事大吉了嘛。这样看的话，标注的工作就对应了传统写代码中写规则匹配的工作。 那么有了标注好的图片，怎么让程序学习呢？这一步就需要借助于万能的数学了。如果我们能构造一个拟合函数，这个拟合函数可以拟合训练集，比如这里就是给函数输入各个图片的像素 RGB 值，输出就是是不是黄图。拟合完了之后给定一张没见过的图片也就能用这个函数得到是不是黄图了。 那么具体是怎么实现这个思路的呢？想一想最小二乘法的做法。举个例子，比如有 100 个点（x0,y0),...,(x100,y100），我们想用一条直线去拟合它。首先直线的方程就是$$y=ax+b$$, 其实这里我们只要求出 a 和 b 就行了，a 和 b 的选择有无数多种，我们要选一种最好的。没有量化就没有优化，我们首先要把 “最好的” 这个标准进行量化，在数学上就是选一个目标函数，比较好的一个目标函数就是让每个点到 y 的距离之和最小。 所以目标函数就是 $$\\sum_{i=0}^{n}\\frac{\\left | ax_{i} - y_{i} + b \\right |}{\\sqrt{a^{2}+1}}$$，我们要让它最小。 考虑到分母都是一样的，因此，我们在这里只关注距离，所以可以把一个点的误差写成$$ \\left | x_{i}- (ax_{i}+b)\\right |$$。然后把每个点的误差加起来就是我们的目标函数了。我们要让理论值和测试值之间误差最小，这样表示的目标函数就是$$\\sum_{i=0}^{n}\\left | x_{i}- (ax_{i}+b)\\right |$$。 但是绝对值是个令人头痛的东西，是一个分段函数并且还有不可导的点。所以我们用平方代替，最终的目标函数是 $$\\sum_{i=0}^{n} (x_{i}- (ax_{i}+b))^{2}$$，现在的任务只要求使得这个式子最小的 a 和 b 就行了。在高等数学中，对于这个问题只要对 a 和 b 分别求偏导，并令其为 0，然后解一个二元一次方程就行了。 上面例子的分析流程就是机器学习的步骤。其流程都是确定目标函数，然后对每个参数求导的方法找到目标函数的最大值（优化参数的过程）。只不过这里每个步骤都有很多方法可以选择。 针对是否有训练集： 有监督 无监督 半监督 目标函数： 极大似然估计 贝叶斯估计 平方误差 交叉熵 优化参数： 直接解析 EM算法 牛顿法 梯度下降法 ","date":"2019-03-03","objectID":"/%E4%BB%8E%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E7%9C%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/:0:0","series":null,"tags":["机器学习"],"title":"从一个例子看机器学习","uri":"/%E4%BB%8E%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90%E7%9C%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/#"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#最优的通用机器学习算法"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#误差分析"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#机器学习算法优缺点分析"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#朴素贝叶斯"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#logistic-regression逻辑回归-"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#线性回归-"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#最近邻算法knn"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#决策树-"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#svm支持向量机-"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#adaboost"},{"categories":["机器学习"],"content":" 最优的通用机器学习算法在机器学习领域，一个基本的定理就是“没有免费的午餐”。换言之，就是没有算法能完美地解决所有问题，尤其是对监督学习而言（例如预测建模）。 举例来说，你不能去说神经网络任何情况下都能比决策树更有优势，反之亦然。它们要受很多因素的影响，比如你的数据集的规模或结构。 其结果是，在用给定的测试集来评估性能并挑选算法时，你应当根据具体的问题来采用不同的算法。 当然，所选的算法必须要适用于你自己的问题，这就要求选择正确的机器学习任务。作为类比，如果你需要打扫房子，你可能会用到吸尘器、扫帚或是拖把，但你绝对不该掏出铲子来挖地。 如果说最优的通用机器学习算法，那可能就是随机算法了，只要数据符合自然规律，那么随机算法的准确率在50%。 误差分析在统计学中，一个模型好坏，是根据偏差和方差来衡量的： 偏差：描述的是预测值（估计值）的期望E’与真实值Y之间的差距。偏差越大，越偏离真实数据。 $$Bias[\\hat{f(x)}] = E[\\hat{f(x)}] - f(x) \\tag{1}$$ 方差：描述的是预测值P的变化范围，离散程度，是预测值的方差，也就是离其期望值E的距离。方差越大，数据的分布越分散。 $$Var[\\hat{f(x)}] = E[(f(x) - E[\\hat{f(x)}])^2] \\tag{2}$$ 模型的真实误差是两者之和，如公式： $$E[(y - \\hat{f(x)^2}] = Bias[\\hat{f(x)}]^2+Var[\\hat{f(x)}] + \\sigma ^ 2\\tag{3}​$$ 在统计学习框架下，大家刻画模型复杂度的时候，有这么个观点，认为$Error = Bias + Variance$。这里的Error大概可以理解为模型的预测错误率，是有两部分组成的，一部分是由于模型太简单而带来的估计不准确的部分（Bias），另一部分是由于模型太复杂而带来的更大的变化空间和不确定性（Variance）。 简单来讲是我们要在训练集上学习一个模型，然后拿到测试集去用，效果好不好要根据测试集的错误率来衡量。但很多时候，我们只能假设测试集和训练集的是符合同一个数据分布的，但却拿不到真正的测试数据。这时候怎么在只看到训练错误率的情况下，去衡量测试错误率呢？ 由于训练样本很少（至少不足够多），所以通过训练集得到的模型，总不是真正正确的。（就算在训练集上正确率100%，也不能说明它刻画了真实的数据分布，要知道刻画真实的数据分布才是我们的目的，而不是只刻画训练集的有限的数据点）。 而且，实际中，训练样本往往还有一定的噪音误差，所以如果太追求在训练集上的完美而采用一个很复杂的模型，会使得模型把训练集里面的误差都当成了真实的数据分布特征，从而得到错误的数据分布估计。这样的话，到了真正的测试集上就错的一塌糊涂了（这种现象叫过拟合）。但是也不能用太简单的模型，否则在数据分布比较复杂的时候，模型就不足以刻画数据分布了（体现为连在训练集上的错误率都很高，这种现象较欠拟合）。过拟合表明采用的模型比真实的数据分布更复杂，而欠拟合表示采用的模型比真实的数据分布要简单。 在实际中，为了让Error尽量小，我们在选择模型的时候需要平衡Bias和Variance所占的比例，也就是平衡over-fitting和under-fitting。 当模型复杂度上升的时候，偏差会逐渐变小，而方差会逐渐变大。 机器学习算法优缺点分析 朴素贝叶斯 朴素贝叶斯属于生成式模型（关于生成模型和判别式模型，主要还是在于是否需要求联合分布），比较简单，你只需做一堆计数即可。如果注有条件独立性假设（一个比较严格的条件），朴素贝叶斯分类器的收敛速度将快于判别模型，比如逻辑回归，所以你只需要较少的训练数据即可。即使NB条件独立假设不成立，NB分类器在实践中仍然表现的很出色。它的主要缺点是它不能学习特征间的相互作用。引用一个比较经典的例子，比如，虽然你喜欢Brad Pitt和Tom Cruise的电影，但是它不能学习出你不喜欢他们在一起演的电影。 优点： 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率； 对大数量训练和查询时具有较高的速度。即使使用超大规模的训练集，针对每个项目通常也只会有相对较少的特征数，并且对项目的训练和分类也仅仅是特征概率的数学运算而已； 对小规模的数据表现很好，能个处理多分类任务，适合增量式训练（即可以实时的对新增的样本进行训练）； 对缺失数据不太敏感，算法也比较简单，常用于文本分类； 朴素贝叶斯对结果解释容易理解。 缺点： 需要计算先验概率； 分类决策存在错误率； 对输入数据的表达形式很敏感； 由于使用了样本属性独立性的假设，所以如果样本属性有关联时其效果不好。 应用场合： 欺诈检测中使用较多； 一封电子邮件是否是垃圾邮件； 一篇文章应该分到科技、政治，还是体育类； 一段文字表达的是积极的情绪还是消极的情绪； 人脸识别。 **Logistic Regression（逻辑回归） ** **逻辑回归属于判别式模型，同时伴有很多模型正则化的方法（L0， L1，L2，etc），而且你不必像在用朴素贝叶斯那样担心你的特征是否相关。**与决策树、SVM相比，你还会得到一个不错的概率解释，你甚至可以轻松地利用新数据来更新模型（使用在线梯度下降算法-online gradient descent）。如果你需要一个概率架构（比如，简单地调节分类阈值，指明不确定性，或者是要获得置信区间），或者你希望以后将更多的训练数据快速整合到模型中去，那么使用它吧。 Sigmoid函数:$f(x )= \\frac{1}{1 + e^{-x}}$ 优点： 实现简单，广泛的应用于工业问题上； 分类时计算量非常小，速度很快，存储资源低； 便利的观测样本概率分数； 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题； 计算代价不高，易于理解和实现。 缺点： 当特征空间很大时，逻辑回归的性能不是很好； 容易欠拟合，一般准确度不太高； 不能很好地处理大量多类特征或变量； 只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分； 对于非线性特征，需要进行转换。 应用领域： 用于二分类领域，可以得出概率值，适用于根据分类概率排名的领域，如搜索排名等； Logistic回归的扩展softmax可以应用于多分类领域，如手写字识别等； 信用评估； 测量市场营销的成功度； 预测某个产品的收益； 特定的某天是否会发生地震。 **线性回归 ** 线性回归是用于回归的，它不像Logistic回归那样用于分类，其基本思想是用梯度下降法对最小二乘法形式的误差函数进行优化，当然也可以用normal equation直接求得参数的解，结果为： $\\hat{w} = (X^TX)^{-1}X^Ty$ 而在LWLR（局部加权线性回归）中，参数的计算表达式为: $\\hat{w} = (X^TWX)^{-1}X^TWy$ 由此可见LWLR与LR不同，LWLR是一个非参数模型，因为每次进行回归计算都要遍历训练样本至少一次。 优点： 实现简单，计算简单。 缺点： 不能拟合非线性数据。 **优化结果：**L1正则化(Lasso)，L2正则化(Ridge) 最近邻算法——KNN KNN即最近邻算法，其主要过程为： 计算训练样本和测试样本中每个样本点的距离（常见的距离度量有欧式距离，马氏距离等）； 对上面所有的距离值进行排序(升序)； 选前k个最小距离的样本； 根据这k个样本的标签进行投票，得到最后的分类类别。 如何选择一个最佳的K值，这取决于数据。一般情况下，在分类时较大的K值能够减小噪声的影响，但会使类别之间的界限变得模糊。一个较好的K值可通过各种启发式技术来获取，比如，交叉验证。另外噪声和非相关性特征向量的存在会使K近邻算法的准确性减小。近邻算法具有较强的一致性结果，随着数据趋于无限，算法保证错误率不会超过贝叶斯算法错误率的两倍。对于一些好的K值，K近邻保证错误率不会超过贝叶斯理论误差率。 优点: 理论成熟，思想简单，既可以用来做分类也可以用来做回归； 可用于非线性分类； 训练时间复杂度为O(n)； 对数据没有假设，准确度高，对outlier不敏感； KNN是一种在线技术，新数据可以直接加入数据集而不必进行重新训练； KNN理论简单，容易实现。 缺点: 样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少）效果差； 需要大量内存； 对于样本容量大的数据集计算量比较大（体现在距离计算上）； 样本不平衡时，预测偏差比较大。如：某一类的样本比较少，而其它类样本比较多； KNN每一次分类都会重新进行一次全局运算； k值大小的选择没有理论选择最优，往往是结合K-折交叉验证得到最优k值选择。 应用领域 文本分类、模式识别、聚类分析，多分类领域 **决策树 ** 决策树的一大优势就是易于解释。它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别A在某个特征维度x的末端，类别B在中间，然后类别A又出现在特征维度x前端的情况）。它的缺点之一就是不支持在线学习，于是在新样本到来后，决策树需要全部重建。另一个缺点就是容易出现过拟合，但这也就是诸如随机森林RF（或提升树boosted tree）之类的集成方法的切入点。另外，随机森林经常是很多分类问题的赢家（通常比支持向量机好上那么一丁点），它训练快速并且可调，同时你无须担心要像支持向量机那样调一大堆参数，所以在以前都一直很受欢迎。 决策树中很重要的一点就是选择一个","date":"2019-03-03","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/:0:0","series":null,"tags":["机器学习"],"title":"机器学习算法一锅端","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%B8%80%E9%94%85%E7%AB%AF/#k-means聚类"},{"categories":["Python"],"content":" Numpy 里的数据结构和Python不同 ","date":"2019-01-23","objectID":"/python_numpy/:0:0","series":null,"tags":["Python"],"title":"Numpy","uri":"/python_numpy/#"},{"categories":["Python"],"content":" Numpy Array只存一个Type # Numpy Array只存一个Type python_list = ['string', 3, 3.2] print(python_list) import numpy as np np_list1 = np.array([1, 2, 3, 4.0]) np_list2 = np.array([1, 2, 3], dtype = 'int') print(np_list1) np_list2[0] = 2.33333 print(\"2.33333 is trunated to %d\" %np_list2[0]) # 输出如下所示： \"\"\" ['string', 3, 3.2] [1. 2. 3. 4.] 2.33333 is trunated to 2 \"\"\" ","date":"2019-01-23","objectID":"/python_numpy/:0:1","series":null,"tags":["Python"],"title":"Numpy","uri":"/python_numpy/#numpy-array只存一个type"},{"categories":["Python"],"content":" 创建办法 import numpy as np zero_list = np.zeros(5, dtype = int) one_matrix = np.ones((2, 2), dtype = float) same_entry_matrix = np.full((1, 2), 2.333) mean = 0 sd = 1 normal_number_matrix = np.random.normal(mean, sd, (3, 3)) print(zero_list) print(one_matrix) print(same_entry_matrix) print(normal_number_matrix) # 输出如下所示： \"\"\" [0 0 0 0 0] [[1. 1.] [1. 1.]] [[2.333 2.333]] [[-0.16545138 -0.310813 -1.08602781] # 随机的 [ 1.99551174 0.19482657 -0.30166058] [ 0.60243205 -1.3135839 1.87655078]] \"\"\" start = 0 end = 10 step = 2 print(type(np.arange(start, end, step)) ) # 输出如下所示： \"\"\" \u003cclass 'numpy.ndarray'\u003e \"\"\" ","date":"2019-01-23","objectID":"/python_numpy/:0:2","series":null,"tags":["Python"],"title":"Numpy","uri":"/python_numpy/#创建办法"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" \u003cclass 'bool'\u003e \u003cclass 'bool'\u003e \u003cclass 'NoneType'\u003e \"\"\" And Or # and or B = True A = False print(B and True) print(A or B) print(not B) print(not A) print(B and (not A)) # 输出如下所示： \"\"\" True True False True True \"\"\" Expressions can evaluate to a boolean value # Expressions can evaluate to a boolean value print(3 \u003e= 2) print(2 \u003e= 2) print(A is A) print(A is B) print(3 != 2) # 输出如下所示： \"\"\" True True True False True \"\"\" Integer and floats # Integer and floats print(type(3)) print(type(3.0)) # 输出如下所示： \"\"\" \u003cclass 'int'\u003e \u003cclass 'float'\u003e \"\"\" Numerical operations # Numerical operations import math print(3 / 2) print(5 // 2) print(3 % 2) print(float(3)) print(pow(2, 3)) print(math.ceil(3.4)) print(math.floor(3.4)) print(round(3.1415926, 2)) # 输出如下所示： \"\"\" 1.5 2 1 3.0 8 4 3 3.14 \"\"\" String strings are useful because they are immutable（它们不可变） # String Introduction = 'Today is 2019-01-21' print(Introduction) print(str(3)) print(int('3')) print(str(\"b'AS\")) # String Method A = 'abc' print(A.capitalize()) print(A.upper()) print(A.find('b')) print(A.find('d')) # String formatting s = 'My name is Tom' name = 'Tomas' age = 25 score = 98.532421 print('hello, my name is %s and I am %d years old\\n \\ I scored %f on my midterm'%(name, age, score)) print('hello, my name is %s and I am %d years old\\n \\ I scored %.2f on my midterm'%(name, age, score)) # 输出如下所示： \"\"\" Today is 2019-01-21 3 3 b'AS Abc ABC 1 -1 hello, my name is Tomas and I am 25 years old \\ I scored 98.532421 on my midterm hello, my name is Tomas and I am 25 years old I scored 98.53 on my midterm \"\"\" List Mutable（可变的） Iterable Sequence Type # List # Create and Modify list A = [] A.append(1) A # List index and Slicing # length = n, start from 0 and end at (n-1) A = [1, 2, 3, 4, 5, 6] print(len(A)) print(A[0]) A[0] = 100 print(A[0]) A[1:4] print(A[1:4]) # 不会输出索引4的数 # What exactly is \"iterable\"? for item in A[2:5]: print(item) A = [1, 2] A.append('2') print(A) A[0] = 's' print(A) print(list('abc') ) # 输出如下所示： \"\"\" 6 1 100 [2, 3, 4] 3 4 5 [1, 2, '2'] ['s', 2, '2'] ['a', 'b', 'c'] \"\"\" Method and Function # Method A = list((2, 1, 3)) print(A) A.sort(reverse = True) # A的值发生了改变 print(A) # Function print(sorted(A)) # A的值没有改变 print(A) # 输出如下所示： \"\"\" [2, 1, 3] [3, 2, 1] [1, 2, 3] [3, 2, 1] \"\"\" Tuple Similar to list 和list类似 But it is immutable(hashable) 但它是不可变的 support for hash(), a built-in function Can have repeated value可以有重复的元素 Sequence Type # Tuple # Create Tuple A = tuple([1, 2, 3, 2]) print(A) print(A[0]) print(A[2:4]) print(A[:4]) print(A[::]) print(A[:-1]) B = tuple([1, 2, 3, 4]) print(B[::-1]) A[0] = 's' print(A) C = tuple() print(C) C = 1, print(C) D = (1) print(type(D)) # 输出如下所示： \"\"\" (1, 2, 3, 2) 1 (3, 2) (1, 2, 3, 2) (1, 2, 3, 2) (1, 2, 3) (4, 3, 2, 1) A[0] = 's' TypeError: 'tuple' object does not support item assignment () (1,) \u003cclass 'int'\u003e \"\"\" Set Unordered 没有顺序 Contain hashable elements( so it is again immutable) 里面的元素必须是hashable No repetition 没有重复 Not a sequence type # Set A = set([1, 1, 2]) B = set(['Michal', 'Betty']) print(A) print(len(A)) print(B) print(len(B)) C = set([1], [2]) print(C) # Set Arithmetics A = set([1, 2, 3]) B = set([3, 4, 5]) print('A intersects B') print(A \u0026 B) print('A unions B') print(A | B) print('in A not in B') print(A - B) print('add 5 into A') A.add(5) print(A) print('new A after adding 5') print(A) print('remove 2 in A') A.remove(2) print(A) print('new A after reoving 2') print(A) # 输出如下所示： \"\"\" {1, 2} 2 {'Michal', 'Betty'} 2 C = set([1], [2]) TypeError: set expected at most 1 arguments, got 2 A intersects B {3} A unions B {1, 2, 3, 4, 5} in A not in B {1, 2} add 5 into A {1, 2, 3, 5} new A after adding 5 {1, 2, 3, 5} remove 2 in A {1, 3, 5} new A after reoving 2 {1, 3, 5} \"\"\" Common operations on List, Set, Tuple # Use tuple for demonstration here A = (1, 2, 3) print(len(A)) for item in A: print(item) print(1 in A) # 输出如下所示： \"\"\"","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#python基础语法"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#boolean-and-its-opeations"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#and--or"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#expressions-can-evaluate-to-a-boolean-value"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#integer-and-floats"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#numerical-operations"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#string"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#list"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#method-and-function"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#tuple"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#set"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#common-operations-on-list-set-tuple"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#dictionary"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#ranges"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#hashable"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#loop-and-conditionals"},{"categories":["Python"],"content":" Python基础语法 Boolean and its Opeations # Boolean and its Operations print(type(True)) print(type(False)) print(type(None)) # 输出如下所示： \"\"\" ","date":"2019-01-21","objectID":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/:0:1","series":null,"tags":["Python"],"title":"Python基础语法","uri":"/python_%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95/#2-d-list-list-aliasing-shallow-or-deep-copy"},{"categories":["剑指Offer"],"content":" 题目描述： 小明很喜欢数学,有一天他在做数学作业时,要求计算出9~16的和,他马上就写出了正确答案是100。但是他并不满足于此,他在想究竟有多少种连续的正数序列的和为100(至少包括两个数)。没多久,他就得到另一组连续正数和为100的序列:18,19,20,21,22。现在把问题交给你,你能不能也很快的找出所有和为S的连续正数序列? Good Luck! ","date":"2019-01-18","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之和为S的连续正数序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/#题目描述"},{"categories":["剑指Offer"],"content":" 输出描述: 输出所有和为S的连续正数序列。序列内按照从小至大的顺序，序列间按照开始数字从小到大的顺序 ","date":"2019-01-18","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:1:0","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之和为S的连续正数序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/#输出描述"},{"categories":["剑指Offer"],"content":" 解题思路：1.左神的思路，双指针问题 当总和小于sum，大指针继续+ 否则小指针+ 2.根据数学公式 1）由于我们要找的是和为S的连续正数序列，因此这个序列是个公差为1的等差数列， 而这个序列的中间值代表了平均值的大小。假设序列长度为n，那么这个序列的中间值可以通过（S / n）得到， 知道序列的中间值和长度，也就不难求出这段序列了。 2）满足条件的n分两种情况： n为奇数时，序列中间的数正好是序列的平均值，所以条件为：(n \u0026 1) == 1 \u0026\u0026 sum % n == 0； n为偶数时，序列中间两个数的平均值是序列的平均值，而这个平均值的小数部分为0.5，所以条件为：(sum % n) * 2 == n. 3）由题可知n \u003e= 2，那么n的最大值是多少呢？我们完全可以将n从2到S全部遍历一次， 但是大部分遍历是不必要的。为了让n尽可能大，我们让序列从1开始， 根据等差数列的求和公式：S = (1 + n) * n / 2，得到. 最后举一个例子，假设输入sum = 100，我们只需遍历n = 13~2的情况（按题意应从大到小遍历）， n = 8时，得到序列[9, 10, 11, 12, 13, 14, 15, 16]；n = 5时，得到序列[18, 19, 20, 21, 22] 时间复杂度：$O(n)$, 空间复杂度：$O(1)$. class Solution { public: vector\u003cvector\u003cintFindContinuousSequence(int sum) { vector\u003cvector\u003cintvec; // n是代表序列的长度 for(int n = sqrt(2 * sum) ;n \u003e= 2;n--) { // 判断边界是否是奇偶 if ((n \u0026 1) == 1 \u0026\u0026 sum % n == 0 || (sum % n) * 2 == n) { // 定义一个数组，保存数 vector\u003cinttemp; // 将序列加入到数组中 for (int j = 0, k = (sum / n) - (n - 1) / 2; j \u003c n; j++, k++) { temp.push_back(k); } vec.push_back(temp); } } return vec; } }; ","date":"2019-01-18","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/:1:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之和为S的连续正数序列","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E8%BF%9E%E7%BB%AD%E6%AD%A3%E6%95%B0%E5%BA%8F%E5%88%97/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 输出描述: 对应每个测试案例，输出两个数，小的先输出。 ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之和为S的两个数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/#题目描述"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个递增排序的数组和一个数字S，在数组中查找两个数，使得他们的和正好是S，如果有多对数字的和等于S，输出两个数的乘积最小的。 输出描述: 对应每个测试案例，输出两个数，小的先输出。 ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之和为S的两个数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/#输出描述"},{"categories":["剑指Offer"],"content":" 解题思路： 思路一：暴力循环 时间复杂度: $O(n^2)$, 空间复杂度: $O(1)$. class Solution { public: vector\u003cintFindNumbersWithSum(vector\u003cintarray,int sum) { vector\u003cintvec; int i = 0, j = 1; int temp = INT_MAX; for(int i = 0; i \u003c array.size(); i++) { for(int j = i + 1; j \u003c array.size(); j++) { if(array[i] + array[j] == sum) { if(array[i] * array[j] \u003c temp) { temp = array[i] * array[j]; vec.push_back(array[i]); vec.push_back(array[j]); } } } } return vec; } }; 思路二：双指针 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: vector\u003cintFindNumbersWithSum(vector\u003cintarray,int sum) { vector\u003cintvec; int i = 0; int j = array.size() - 1; int Sum = 0; while(i \u003c j) { Sum = array[i] + array[j]; if(Sum sum) j--; if(Sum \u003c sum) i++; if(Sum == sum) { vec.push_back(array[i]); vec.push_back(array[j]); break; } } return vec; } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之和为S的两个数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/#解题思路"},{"categories":["剑指Offer"],"content":" 解题思路： 思路一：暴力循环 时间复杂度: $O(n^2)$, 空间复杂度: $O(1)$. class Solution { public: vector","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之和为S的两个数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/#思路一"},{"categories":["剑指Offer"],"content":" 解题思路： 思路一：暴力循环 时间复杂度: $O(n^2)$, 空间复杂度: $O(1)$. class Solution { public: vector","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之和为S的两个数字","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%92%8C%E4%B8%BAs%E7%9A%84%E4%B8%A4%E4%B8%AA%E6%95%B0%E5%AD%97/#思路二"},{"categories":["剑指Offer"],"content":" 题目描述： 输入n个整数，找出其中最小的K个数。例如输入4,5,1,6,2,7,3,8这8个数字，则最小的4个数字是1,2,3,4,。 ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之最小的K个数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 思路一： 时间复杂度: $O(nlogn)$, 空间复杂度: $O(1)$. class Solution { public: vector\u003cint\u003e GetLeastNumbers_Solution(vector\u003cint\u003e input, int k) { if(k \u003e input.size()) return vector\u003cint\u003e(); sort(input.begin(),input.end()); return vector\u003cint\u003e(input.begin(),input.begin() + k); } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之最小的K个数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/#解题思路"},{"categories":["剑指Offer"],"content":" 解题思路： 思路一： 时间复杂度: $O(nlogn)$, 空间复杂度: $O(1)$. class Solution { public: vector GetLeastNumbers_Solution(vector input, int k) { if(k \u003e input.size()) return vector(); sort(input.begin(),input.end()); return vector(input.begin(),input.begin() + k); } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之最小的K个数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/#思路一"},{"categories":["剑指Offer"],"content":" 思路二： 冒泡排序 每次找最大或者最小的先排好，两个指针都指向前头 时间复杂度: $O(n^2)$, 空间复杂度: $O(1)$. class Solution { public: vector\u003cint\u003e GetLeastNumbers_Solution(vector\u003cint\u003e input, int k) { if(k \u003e input.size()) return vector\u003cint\u003e(); for(int i = 0; i \u003c input.size(); i++) { int temp = 0; for(int j = i + 1; j \u003c input.size(); j++) { if(input[i] \u003e input[j]) { temp = input[i]; input[i] = input[j]; input[j] = temp; } } } return vector\u003cint\u003e(input.begin(), input.begin() + k); } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/:0:3","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之最小的K个数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/#思路二"},{"categories":["剑指Offer"],"content":" 思路三： 快速排序 三个指针，两个指向前头，一个指向后面，然后找到一个数，数的一边比他小，另一边比他大 时间复杂度: $O(nlogn)$, 空间复杂度: $O(1)$. class Solution { public: void qsort(vector\u003cint\u003e \u0026input, int low, int high) { if(low \u003e= high) { return; } int k = input[low]; int i = low; int j = high; while(i \u003c j) { while(i \u003c j \u0026\u0026 input[j] \u003e= k) --j; input[i] = input[j]; while(i \u003c j \u0026\u0026 input[i] \u003c= k) ++i; input[j] = input[i]; } input[i] = k; qsort(input,low,i-1); qsort(input,i+1,high); } vector\u003cint\u003e GetLeastNumbers_Solution(vector\u003cint\u003e input, int k) { if(k \u003e input.size()) return vector\u003cint\u003e(); int i = 0; int j = input.size() - 1; qsort(input, i, j); return vector\u003cint\u003e(input.begin(), input.begin() + k); } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/:0:4","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之最小的K个数","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9C%80%E5%B0%8F%E7%9A%84k%E4%B8%AA%E6%95%B0/#思路三"},{"categories":["剑指Offer"],"content":" 题目描述： 一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之变态跳台阶","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { public: int jumpFloorII(int number) { vector\u003cint\u003e vec(number + 1); vec[0] = 0; vec[1] = 1; vec[2] = 2; for(int i = 3; i \u003c= number; ++i) { vec[i] = vec[i - 1] + vec[i - 2] + pow(2, (i - 3)); } return vec[number]; } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之变态跳台阶","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B7%B3%E5%8F%B0%E9%98%B6/:0:1","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之跳台阶","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B7%B3%E5%8F%B0%E9%98%B6/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. class Solution { public: int jumpFloor(int number) { vector\u003cint\u003e vec(number + 1); vec[0] = 0; vec[1] = 1; vec[2] = 2; for(int i = 3; i \u003c= number; ++i) { vec[i] = vec[i - 1] + vec[i - 2]; } return vec[number]; } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B7%B3%E5%8F%B0%E9%98%B6/:0:2","series":null,"tags":["剑指Offer","other"],"title":"剑指Offer之跳台阶","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E8%B7%B3%E5%8F%B0%E9%98%B6/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/:0:1","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之从尾到头打印链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$, 空间复杂度: $O(n)$. /** * struct ListNode { * int val; * struct ListNode *next; * ListNode(int x) : * val(x), next(NULL) { * } * }; */ class Solution { public: vector\u003cint\u003e printListFromTailToHead(ListNode* head) { vector\u003cint\u003e vec; if(head == NULL) return vec; while(head-\u003enext != NULL) { vec.insert(vec.begin(),head-\u003eval); head = head-\u003enext; } vec.insert(vec.begin(),head-\u003eval); return vec; } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/:0:2","series":null,"tags":["剑指Offer","link"],"title":"剑指Offer之从尾到头打印链表","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/:0:1","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之替换空格","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: void replaceSpace(char *str,int length) { int count=0; for(int i=0; i\u003clength; i++) { if(str[i]==' ') { count++; } } int new_length = count * 2 + length; int j = new_length - 1; for(; length\u003e0; length--) if(str[length-1]==' ') { str[j--]='0'; str[j--]='2'; str[j--]='%'; } else { str[j--]=str[length-1]; } } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/:0:2","series":null,"tags":["剑指Offer","string"],"title":"剑指Offer之替换空格","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/#解题思路"},{"categories":["剑指Offer"],"content":" 题目描述： 在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/:0:1","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之二维数组中的查找","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/#题目描述"},{"categories":["剑指Offer"],"content":" 解题思路： 时间复杂度: $O(n)$, 空间复杂度: $O(1)$. class Solution { public: bool Find(int target, vector\u003cvector\u003cint\u003e \u003e array) { int a = array.size(); //数组的行数 int b = array[0].size(); //数组的列数 for(int i = a - 1, j = 0; i \u003e= 0 \u0026\u0026 j \u003c b;) { if(target == array[i][j]) return true; if(target \u003e array[i][j]) j++; if(target \u003c array[i][j]) i--; } return false; } }; ","date":"2019-01-17","objectID":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/:0:2","series":null,"tags":["剑指Offer","array"],"title":"剑指Offer之二维数组中的查找","uri":"/%E5%89%91%E6%8C%87offer%E4%B9%8B%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/#解题思路"},{"categories":["深度学习"],"content":"在学习了PyTorch的Tensor、Variable和autograd之后，已经可以实现简单的深度学习模型，然而使用autograd实现的深度学习模型，其抽象程度比较较低，如果用其来实现深度学习模型，则需要编写的代码量极大。在这种情况下，torch.nn应运而生，其是专门为深度学习而设计的模块。torch.nn的核心数据结构是Module，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。在实际使用中，最常见的做法是继承nn.Module，撰写自己的网络层。 自定义层Linear必须继承nn.Module，并且在其构造函数中需调用nn.Module的构造函数，即super(Linear, self).__init__() 或nn.Module.__init__(self)，推荐使用第一种用法，尽管第二种写法更直观。 在构造函数__init__中必须自己定义可学习的参数，并封装成Parameter，如在本例中我们把w和b封装成parameter。parameter是一种特殊的Tensor，但其默认需要求导（requires_grad = True），感兴趣的读者可以通过nn.Parameter??，查看Parameter类的源代码。 forward函数实现前向传播过程，其输入可以是一个或多个tensor。 无需写反向传播函数，nn.Module能够利用autograd自动实现反向传播，这点比Function简单许多。 使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于layers.__call__(input)，在__call__函数中，主要调用的是 layer.forward(x)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用layer(x)而不是使用layer.forward(x)。 Module中的可学习参数可以通过named_parameters()或者parameters()返回迭代器，前者会给每个parameter都附上名字，使其更具有辨识度。 可见利用Module实现的全连接层，比利用Function实现的更为简单，因其不再需要写反向传播函数。 例子 # 用nn.Module实现自己的全连接层。 # 继承nn.Module class Linear(nn.Module): def __init__(self,input_size,output_size): # 父类的初始化函数 super(Linear, self).__init__() # 初始化参数 self.w = nn.Parameter(t.randn(input_size,output_size)) self.b = nn.Parameter(t.randn(output_size)) # 前向传播函数 def forward(self,x): x = x.mm(self.w) # 扩维 x = x + self.b.expand_as(x) return x # 调用Liner层 layer = Linear(4,3) input = t.randn(2,4) output = layer(input) output # tensor([[ 0.7737, -0.3466, -1.1029], # [-0.9017, 0.1434, 0.1074]]) # 查看网络层的参数 for name, parameter in layer.named_parameters(): print(name, parameter) # w and b \"\"\" w Parameter containing: tensor([[-2.1490, -0.4335, 0.3410], [ 0.4159, 1.4965, 1.6047], [ 0.2440, -1.0860, 0.3173], [ 1.0884, -0.2328, -0.9765]]) b Parameter containing: tensor([ 0.0847, -0.4250, -0.2058]) \"\"\" 例子 # 使用上面定义好的Linear模块定义感知机 import torch as t from torch import nn class Perceptron(nn.Module): # 定义输入的参数 def __init__(self, in_features, hidden_features, out_features): nn.Module.__init__(self) self.layer1 = Linear(in_features, hidden_features) # 此处的Linear是前面自定义的全连接层 self.layer2 = Linear(hidden_features, out_features) # 前向传播函数 def forward(self,x): x = self.layer1(x) x = t.sigmoid(x) return self.layer2(x) # 查看网络结构 perceptron = Perceptron(3,4,1) for name, param in perceptron.named_parameters(): print(name, param.size()) \"\"\" layer1.w torch.Size([3, 4]) layer1.b torch.Size([4]) layer2.w torch.Size([4, 1]) layer2.b torch.Size([1]) \"\"\" module中parameter的命名规范： 对于类似self.param_name = nn.Parameter(t.randn(3, 4))，命名为param_name 对于子Module中的parameter，会其名字之前加上当前Module的名字。如对于self.sub_module = SubModel()，SubModel中有个parameter的名字叫做param_name，那么二者拼接而成的parameter name 就是sub_module.param_name。 ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:0","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#"},{"categories":["深度学习"],"content":" 常用神经网络层 为方便用户使用，PyTorch实现了神经网络中绝大多数的layer，这些layer都继承于nn.Module，封装了可学习参数parameter，并实现了forward函数，且很多都专门针对GPU运算进行了CuDNN优化，其速度和性能都十分优异。更多的内容可参照官方文档或在IPython/Jupyter中使用nn.layer?来查看。阅读文档时应主要关注以下几点： 构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注这三个参数的作用。 属性、可学习参数和子module。如nn.Linear中有weight和bias两个可学习参数，不包含子module。 输入输出的形状，如nn.linear的输入形状是(N, input_features)，输出为(N，output_features)，N是batch_size。 这些自定义layer对输入形状都有假设：输入的不是单个数据，而是一个batch。输入只有一个数据，则必须调用tensor.unsqueeze(0) 或 tensor[None]将数据伪装成batch_size=1的batch ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:1","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#常用神经网络层"},{"categories":["深度学习"],"content":" 图像相关层 图像相关层主要包括卷积层（Conv）、池化层（Pool）等，这些层在实际使用中可分为一维(1D)、二维(2D)、三维（3D），池化方式又分为平均池化（AvgPool）、最大值池化（MaxPool）、自适应池化（AdaptiveAvgPool）等。而卷积层除了常用的前向卷积之外，还有逆卷积（TransposeConv）。 深度学习当中，经常用到的网络层： Linear：全连接层。 BatchNorm：批规范化层，分为1D、2D和3D。除了标准的BatchNorm之外，还有在风格迁移中常用到的InstanceNorm层。 Dropout：dropout层，用来防止过拟合，同样分为1D、2D和3D。 Conv：卷积层，分为1D、2D和3D Pool：池化层，分为1D、2D和3D from PIL import Image from torchvision.transforms import ToPILImage,ToTensor import torch as t from torch import nn to_tensor = ToTensor() # img -\u003e tensor to_pil = ToPILImage() img = Image.open(\"./imgs/lena.png\") img # 输入是一个batch，batch_size＝1 input = to_tensor(img).unsqueeze(0) print(input.shape) # torch.Size([1, 1, 200, 200]) # 锐化卷积核 kernel = t.ones(3, 3)/-9. kernel[1][1] = 1 conv = nn.Conv2d(1, 1, (3, 3), 1, bias=False) conv.weight.data = kernel.view(1, 1, 3, 3) out = conv(input) to_pil(out.data.squeeze(0)) pool = nn.AvgPool2d(2,2) out = pool(input) to_pil(out.data.squeeze(0)) ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:2","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#图像相关层"},{"categories":["深度学习"],"content":" Sequential和ModuleList 使用nn.module实现的神经网络，基本上都是将每一层的输出直接作为下一层的输入，这种网络称为前馈传播网络（feedforward neural network）。对于此类网络如果每次都写复杂的forward函数会有些麻烦，在此就有两种简化方式，ModuleList和Sequential。其中Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。ModuleList也是一个特殊的module，可以包含几个子module，可以像用list一样使用它，但不能直接把输入传给ModuleList。下面举例说明。 # 1.Sequential实例 # Sequential的三种写法 net1 = nn.Sequential() net1.add_module('conv', nn.Conv2d(3, 3, 3)) net1.add_module('batchnorm', nn.BatchNorm2d(3)) net1.add_module('activation_layer', nn.ReLU()) net2 = nn.Sequential( nn.Conv2d(3, 3, 3), nn.BatchNorm2d(3), nn.ReLU() ) from collections import OrderedDict net3= nn.Sequential(OrderedDict([ ('conv1', nn.Conv2d(3, 3, 3)), ('bn1', nn.BatchNorm2d(3)), ('relu1', nn.ReLU()) ])) print('net1:', net1) print('net2:', net2) print('net3:', net3) \"\"\" net1: Sequential( (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (activation_layer): ReLU() ) net2: Sequential( (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) net3: Sequential( (conv1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)) (bn1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu1): ReLU() ) \"\"\" # 可根据名字或序号取出子module net1.conv, net2[0], net3.conv1 \"\"\" (Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1)), Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))) \"\"\" # 2.ModuleList实例 modellist = nn.ModuleList([nn.Linear(3,4), nn.ReLU(), nn.Linear(4,2)]) input = t.randn(1, 3) for model in modellist: input = model(input) print(input) \"\"\" tensor([[ 0.2275, -0.3564, -0.0518, 0.5852]]) tensor([[ 0.2275, 0.0000, 0.0000, 0.5852]]) tensor([[-0.4349, 0.3610]]) \"\"\" ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#sequential和modulelist"},{"categories":["深度学习"],"content":" 激活函数、损失函数与优化器 # 1.激活函数 nn.ReLU() # 2.损失函数 nn.CrossEntropyLoss() # 3.优化器 torch.optim.SGD(params=net.parameters(), lr=1) ################################################## #### 综合实例 import torch as t from torch import nn from torch import optim # 首先定义一个LeNet网络 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.features = nn.Sequential( nn.Conv2d(3, 6, 5), nn.ReLU(), nn.MaxPool2d(2,2), nn.Conv2d(6, 16, 5), nn.ReLU(), nn.MaxPool2d(2,2) ) self.classifier = nn.Sequential( nn.Linear(16 * 5 * 5, 120), nn.ReLU(), nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, 10) ) def forward(self, x): x = self.features(x) x = x.view(-1, 16 * 5 * 5) x = self.classifier(x) return x # 实例化神经网络 net = Net() optimizer = optim.SGD(params=net.parameters(), lr=1) # 第一步 optimizer.zero_grad() # 梯度清零，等价于net.zero_grad() input = t.randn(1, 3, 32, 32) output = net(input) # 第二步 output.backward(output) # fake backward # 第三步 optimizer.step() # 执行优化 ################################################## # 优化器调参 # 为不同子网络设置不同的学习率，在finetune中经常用到 # 如果对某个参数不指定学习率，就使用最外层的默认学习率 optimizer =optim.SGD([ {'params': net.features.parameters()}, # 学习率为1e-5 {'params': net.classifier.parameters(), 'lr': 1e-2} ], lr=1e-5) optimizer \"\"\" SGD ( Parameter Group 0 dampening: 0 lr: 1e-05 momentum: 0 nesterov: False weight_decay: 0 Parameter Group 1 dampening: 0 lr: 0.01 momentum: 0 nesterov: False weight_decay: 0 ) \"\"\" ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:4","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#激活函数损失函数与优化器"},{"categories":["深度学习"],"content":" nn.Functional和nn.Module nn.Functional nn中还有一个很常用的模块：nn.functional，nn中的大多数layer，在functional中都有一个与之相对应的函数。 nn.functional中的函数和nn.Module的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class layer(nn.Module)定义，会自动提取可学习的参数。而nn.functional中的函数更像是纯函数，由def function(input)定义。 import torch as t from torch import nn input = t.randn(2, 3) model = nn.Linear(3, 4) output1 = model(input) output2 = nn.functional.linear(input, model.weight, model.bias) output1 == output2 \"\"\" tensor([[ 1, 1, 1, 1], [ 1, 1, 1, 1]], dtype=torch.uint8) \"\"\" b = nn.functional.relu(input) b2 = nn.ReLU()(input) b == b2 \"\"\" tensor([[ 1, 1, 1], [ 1, 1, 1]], dtype=torch.uint8) \"\"\" 在实际应用的时候，如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用取决于个人的喜好。 如激活函数（ReLU、sigmoid、tanh），池化（MaxPool）等层由于没有可学习参数，则可以使用对应的functional函数代替，而对于卷积、全连接等具有可学习参数的网络建议使用nn.Module。 虽然dropout操作也没有可学习操作，但建议还是使用nn.Dropout而不是nn.functional.dropout，因为dropout在训练和测试两个阶段的行为有所差别，使用nn.Module对象能够通过model.eval操作加以区分。 import torch as t from torch import nn from torch.nn import functional as F # 实例1 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.pool(F.relu(self.conv1(x)), 2) x = F.pool(F.relu(self.conv2(x)), 2) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # 实例2 class MyLinear(nn.Module): def __init__(self): super(MyLinear, self).__init__() self.weight = nn.Parameter(t.randn(3, 4)) self.bias = nn.Parameter(t.zeros(3)) def forward(self): return F.linear(input, weight, bias) nn.Modulenn.Module基类的构造函数： def __init__(self): self._parameters = OrderedDict() self._modules = OrderedDict() self._buffers = OrderedDict() self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self.training = True 其中每个属性的解释如下： _parameters：字典，保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为param1，value为对应parameter的item。而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。 _modules：子module，通过self.submodel = nn.Linear(3, 4)指定的子module会保存于此。 _buffers：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。 _backward_hooks与_forward_hooks：钩子技术，用来提取中间变量，类似variable的hook。 training：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值来决定前向传播策略。 上述几个属性中，_parameters、_modules和_buffers这三个字典中的键值，都可以通过self.key方式获得，效果等价于self._parameters['key']. import torch as t from torch import nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 等价与self.register_parameter('param1' ,nn.Parameter(t.randn(3, 3))) self.param1 = nn.Parameter(t.rand(3, 3)) self.submodel1 = nn.Linear(3, 4) def forward(self, input): x = self.param1.mm(input) x = self.submodel1(x) return x net = Net() net \"\"\" Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) \"\"\" # 1.查看父属性 net._modules # OrderedDict([('submodel1', Linear(in_features=3, out_features=4, bias=True))]) net._parameters \"\"\" OrderedDict([('param1', Parameter containing: tensor([[ 0.3398, 0.5239, 0.7981], [ 0.7718, 0.0112, 0.8100], [ 0.6397, 0.9743, 0.8300]]))]) \"\"\" # 2.查看子属性param1 net.param1 # 等价于net._parameters['param1'] \"\"\" Parameter containing: tensor([[ 0.3398, 0.5239, 0.7981], [ 0.7718, 0.0112, 0.8100], [ 0.6397, 0.9743, 0.8300]]) \"\"\" # 3.查看所有的参数 for name, param in net.named_parameters(): print(name, param.size()) \"\"\" param1 torch.Size([3, 3]) submodel1.weight torch.Size([4, 3]) submodel1.bias torch.Size([4]) \"\"\" for name, submodel in net.named_modules(): print(name, submodel) \"\"\" Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) submodel1 Linear(in_features=3, out_features=4, bias=True) \"\"\" nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为方便用户访问各个子module，nn.Module实现了很多方法，如函数children可以查看直接子module，函数module可以查看所有的子module（包括当前module）。与之相对应的还有函数named_childen和named_modules，其能够在返回module列表的同时返回它们的名字。 对于batchnorm、dropout、instancenorm等在训练和测试阶段行为差距巨大的层，如果在测试","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:5","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#nnfunctional和nnmodule"},{"categories":["深度学习"],"content":" nn.Functional和nn.Module nn.Functional nn中还有一个很常用的模块：nn.functional，nn中的大多数layer，在functional中都有一个与之相对应的函数。 nn.functional中的函数和nn.Module的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class layer(nn.Module)定义，会自动提取可学习的参数。而nn.functional中的函数更像是纯函数，由def function(input)定义。 import torch as t from torch import nn input = t.randn(2, 3) model = nn.Linear(3, 4) output1 = model(input) output2 = nn.functional.linear(input, model.weight, model.bias) output1 == output2 \"\"\" tensor([[ 1, 1, 1, 1], [ 1, 1, 1, 1]], dtype=torch.uint8) \"\"\" b = nn.functional.relu(input) b2 = nn.ReLU()(input) b == b2 \"\"\" tensor([[ 1, 1, 1], [ 1, 1, 1]], dtype=torch.uint8) \"\"\" 在实际应用的时候，如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用取决于个人的喜好。 如激活函数（ReLU、sigmoid、tanh），池化（MaxPool）等层由于没有可学习参数，则可以使用对应的functional函数代替，而对于卷积、全连接等具有可学习参数的网络建议使用nn.Module。 虽然dropout操作也没有可学习操作，但建议还是使用nn.Dropout而不是nn.functional.dropout，因为dropout在训练和测试两个阶段的行为有所差别，使用nn.Module对象能够通过model.eval操作加以区分。 import torch as t from torch import nn from torch.nn import functional as F # 实例1 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.pool(F.relu(self.conv1(x)), 2) x = F.pool(F.relu(self.conv2(x)), 2) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # 实例2 class MyLinear(nn.Module): def __init__(self): super(MyLinear, self).__init__() self.weight = nn.Parameter(t.randn(3, 4)) self.bias = nn.Parameter(t.zeros(3)) def forward(self): return F.linear(input, weight, bias) nn.Modulenn.Module基类的构造函数： def __init__(self): self._parameters = OrderedDict() self._modules = OrderedDict() self._buffers = OrderedDict() self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self.training = True 其中每个属性的解释如下： _parameters：字典，保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为param1，value为对应parameter的item。而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。 _modules：子module，通过self.submodel = nn.Linear(3, 4)指定的子module会保存于此。 _buffers：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。 _backward_hooks与_forward_hooks：钩子技术，用来提取中间变量，类似variable的hook。 training：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值来决定前向传播策略。 上述几个属性中，_parameters、_modules和_buffers这三个字典中的键值，都可以通过self.key方式获得，效果等价于self._parameters['key']. import torch as t from torch import nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 等价与self.register_parameter('param1' ,nn.Parameter(t.randn(3, 3))) self.param1 = nn.Parameter(t.rand(3, 3)) self.submodel1 = nn.Linear(3, 4) def forward(self, input): x = self.param1.mm(input) x = self.submodel1(x) return x net = Net() net \"\"\" Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) \"\"\" # 1.查看父属性 net._modules # OrderedDict([('submodel1', Linear(in_features=3, out_features=4, bias=True))]) net._parameters \"\"\" OrderedDict([('param1', Parameter containing: tensor([[ 0.3398, 0.5239, 0.7981], [ 0.7718, 0.0112, 0.8100], [ 0.6397, 0.9743, 0.8300]]))]) \"\"\" # 2.查看子属性param1 net.param1 # 等价于net._parameters['param1'] \"\"\" Parameter containing: tensor([[ 0.3398, 0.5239, 0.7981], [ 0.7718, 0.0112, 0.8100], [ 0.6397, 0.9743, 0.8300]]) \"\"\" # 3.查看所有的参数 for name, param in net.named_parameters(): print(name, param.size()) \"\"\" param1 torch.Size([3, 3]) submodel1.weight torch.Size([4, 3]) submodel1.bias torch.Size([4]) \"\"\" for name, submodel in net.named_modules(): print(name, submodel) \"\"\" Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) submodel1 Linear(in_features=3, out_features=4, bias=True) \"\"\" nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为方便用户访问各个子module，nn.Module实现了很多方法，如函数children可以查看直接子module，函数module可以查看所有的子module（包括当前module）。与之相对应的还有函数named_childen和named_modules，其能够在返回module列表的同时返回它们的名字。 对于batchnorm、dropout、instancenorm等在训练和测试阶段行为差距巨大的层，如果在测试","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:5","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#nnfunctional"},{"categories":["深度学习"],"content":" nn.Functional和nn.Module nn.Functional nn中还有一个很常用的模块：nn.functional，nn中的大多数layer，在functional中都有一个与之相对应的函数。 nn.functional中的函数和nn.Module的主要区别在于，用nn.Module实现的layers是一个特殊的类，都是由class layer(nn.Module)定义，会自动提取可学习的参数。而nn.functional中的函数更像是纯函数，由def function(input)定义。 import torch as t from torch import nn input = t.randn(2, 3) model = nn.Linear(3, 4) output1 = model(input) output2 = nn.functional.linear(input, model.weight, model.bias) output1 == output2 \"\"\" tensor([[ 1, 1, 1, 1], [ 1, 1, 1, 1]], dtype=torch.uint8) \"\"\" b = nn.functional.relu(input) b2 = nn.ReLU()(input) b == b2 \"\"\" tensor([[ 1, 1, 1], [ 1, 1, 1]], dtype=torch.uint8) \"\"\" 在实际应用的时候，如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用取决于个人的喜好。 如激活函数（ReLU、sigmoid、tanh），池化（MaxPool）等层由于没有可学习参数，则可以使用对应的functional函数代替，而对于卷积、全连接等具有可学习参数的网络建议使用nn.Module。 虽然dropout操作也没有可学习操作，但建议还是使用nn.Dropout而不是nn.functional.dropout，因为dropout在训练和测试两个阶段的行为有所差别，使用nn.Module对象能够通过model.eval操作加以区分。 import torch as t from torch import nn from torch.nn import functional as F # 实例1 class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = F.pool(F.relu(self.conv1(x)), 2) x = F.pool(F.relu(self.conv2(x)), 2) x = x.view(-1, 16 * 5 * 5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x # 实例2 class MyLinear(nn.Module): def __init__(self): super(MyLinear, self).__init__() self.weight = nn.Parameter(t.randn(3, 4)) self.bias = nn.Parameter(t.zeros(3)) def forward(self): return F.linear(input, weight, bias) nn.Modulenn.Module基类的构造函数： def __init__(self): self._parameters = OrderedDict() self._modules = OrderedDict() self._buffers = OrderedDict() self._backward_hooks = OrderedDict() self._forward_hooks = OrderedDict() self.training = True 其中每个属性的解释如下： _parameters：字典，保存用户直接设置的parameter，self.param1 = nn.Parameter(t.randn(3, 3))会被检测到，在字典中加入一个key为param1，value为对应parameter的item。而self.submodule = nn.Linear(3, 4)中的parameter则不会存于此。 _modules：子module，通过self.submodel = nn.Linear(3, 4)指定的子module会保存于此。 _buffers：缓存。如batchnorm使用momentum机制，每次前向传播需用到上一次前向传播的结果。 _backward_hooks与_forward_hooks：钩子技术，用来提取中间变量，类似variable的hook。 training：BatchNorm与Dropout层在训练阶段和测试阶段中采取的策略不同，通过判断training值来决定前向传播策略。 上述几个属性中，_parameters、_modules和_buffers这三个字典中的键值，都可以通过self.key方式获得，效果等价于self._parameters['key']. import torch as t from torch import nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 等价与self.register_parameter('param1' ,nn.Parameter(t.randn(3, 3))) self.param1 = nn.Parameter(t.rand(3, 3)) self.submodel1 = nn.Linear(3, 4) def forward(self, input): x = self.param1.mm(input) x = self.submodel1(x) return x net = Net() net \"\"\" Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) \"\"\" # 1.查看父属性 net._modules # OrderedDict([('submodel1', Linear(in_features=3, out_features=4, bias=True))]) net._parameters \"\"\" OrderedDict([('param1', Parameter containing: tensor([[ 0.3398, 0.5239, 0.7981], [ 0.7718, 0.0112, 0.8100], [ 0.6397, 0.9743, 0.8300]]))]) \"\"\" # 2.查看子属性param1 net.param1 # 等价于net._parameters['param1'] \"\"\" Parameter containing: tensor([[ 0.3398, 0.5239, 0.7981], [ 0.7718, 0.0112, 0.8100], [ 0.6397, 0.9743, 0.8300]]) \"\"\" # 3.查看所有的参数 for name, param in net.named_parameters(): print(name, param.size()) \"\"\" param1 torch.Size([3, 3]) submodel1.weight torch.Size([4, 3]) submodel1.bias torch.Size([4]) \"\"\" for name, submodel in net.named_modules(): print(name, submodel) \"\"\" Net( (submodel1): Linear(in_features=3, out_features=4, bias=True) ) submodel1 Linear(in_features=3, out_features=4, bias=True) \"\"\" nn.Module在实际使用中可能层层嵌套，一个module包含若干个子module，每一个子module又包含了更多的子module。为方便用户访问各个子module，nn.Module实现了很多方法，如函数children可以查看直接子module，函数module可以查看所有的子module（包括当前module）。与之相对应的还有函数named_childen和named_modules，其能够在返回module列表的同时返回它们的名字。 对于batchnorm、dropout、instancenorm等在训练和测试阶段行为差距巨大的层，如果在测试","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:5","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#nnmodule"},{"categories":["深度学习"],"content":" Pytorch模型保存与加载 在PyTorch中保存模型十分简单，所有的Module对象都具有state_dict()函数，返回当前Module所有的状态数据。将这些状态数据保存后，下次使用模型时即可利用model.load_state_dict()函数将状态加载进来。优化器（optimizer）也有类似的机制，不过一般并不需要保存优化器的运行状态。 # 模型的保存与加载 import torch as t from torch import nn class Net(nn.Module): def __init__(self): super(Net, self).__init__() # 等价与self.register_parameter('param1' ,nn.Parameter(t.randn(3, 3))) self.param1 = nn.Parameter(t.rand(3, 3)) self.submodel1 = nn.Linear(3, 4) def forward(self, input): x = self.param1.mm(input) x = self.submodel1(x) return x net = Net() # 保存模型 t.save(net.state_dict(), 'net.pth') # 加载已保存的模型 net2 = Net() net2.load_state_dict(t.load('net.pth')) ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:6","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#pytorch模型保存与加载"},{"categories":["深度学习"],"content":" 在GPU上运行 将Module放在GPU上运行十分简单，只需两步： model = model.cuda()：将模型的所有参数转存到GPU input.cuda()：将输入数据也放置到GPU上 至于如何在多个GPU上并行计算，PyTorch也提供了两个函数，可实现简单高效的并行GPU计算 nn.parallel.data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None) class torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0) 可见二者的参数十分相似，通过device_ids参数可以指定在哪些GPU上进行优化，output_device指定输出到哪个GPU上。唯一的不同就在于前者直接利用多GPU并行计算得出结果，而后者则返回一个新的module，能够自动在多GPU上进行并行加速。 DataParallel并行的方式，是将输入一个batch的数据均分成多份，分别送到对应的GPU进行计算，各个GPU得到的梯度累加。与Module相关的所有数据也都会以浅复制的方式复制多份，在此需要注意，在module中属性应该是只读的。 # method 1 new_net = nn.DataParallel(net, device_ids=[0, 1]) output = new_net(input) # method 2 output = nn.parallel.data_parallel(new_net, input, device_ids=[0, 1]) ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:7","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#在gpu上运行"},{"categories":["深度学习"],"content":" nn与autograd nn.Module利用的也是autograd技术，其主要工作是实现前向传播。在forward函数中，nn.Module对输入的tensor进行的各种操作，本质上都是用到了autograd技术。这里需要对比autograd.Function和nn.Module之间的区别： autograd.Function利用了Tensor对autograd技术的扩展，为autograd实现了新的运算op，不仅要实现前向传播还要手动实现反向传播 nn.Module利用了autograd技术，对nn的功能进行扩展，实现了深度学习中更多的层。只需实现前向传播功能，autograd即会自动实现反向传播 nn.functional是一些autograd操作的集合，是经过封装的函数 作为两大类扩充PyTorch接口的方法，在实际使用中，如果某一个操作，在autograd中尚未支持，那么只能实现Function接口对应的前向传播和反向传播。如果某些时候利用autograd接口比较复杂，则可以利用Function将多个操作聚合，实现优化，而如果只是想在深度学习中增加某一层，使用nn.Module进行封装则更为简单高效。 ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:8","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#nn与autograd"},{"categories":["深度学习"],"content":" 参考 深度学习之Pytorch(陈云) ","date":"2019-01-15","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/:0:9","series":null,"tags":["python","深度学习","Pytorch"],"title":"PyTorch快速入门1","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A81/#参考"},{"categories":["python"],"content":" Pandas 是基于 NumPy 构建的库，在数据处理方面可以把它理解为 NumPy 加强版，同时 Pandas 也是一项开源项目。它基于 Cython，因此读取与处理数据非常快，并且还能轻松处理浮点数据中的缺失数据（表示为 NaN）以及非浮点数据。 pandas适合于许多不同类型的数据，包括： 具有异构类型列的表格数据，例如SQL表格或Excel数据 有序和无序（不一定是固定频率）时间序列数据。 具有行列标签的任意矩阵数据（均匀类型或不同类型） 任何其他形式的观测/统计数据集。 ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:0","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#"},{"categories":["python"],"content":" 核心数据结构 pandas最核心的就是Series和DataFrame两个数据结构。 DataFrame可以看做是Series的容器，即：一个DataFrame中可以包含若干个Series。 名称 维度 说明 Series 1维 带有标签的同构类型数组 DataFrame 2维 表格结构，带有标签，大小可变，且可以包含异构的数据列 Series Series的创建 pandas.Series( data, index, dtype, copy)。 编号 参数 描述 1 data 数据采取各种形式，如：ndarray，list，constants 2 index 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。 3 dtype dtype用于数据类型。如果没有，将推断数据类型 4 copy 复制数据，默认为false。 # series的创建 # 1.根据list创建 import pandas as pd obj = pd.Series([4,7,-5,3]) obj # 输出 \"\"\" 0 4 1 7 2 -5 3 3 dtype: int64 \"\"\" obj.index #RangeIndex(start=0, stop=4, step=1) obj.values #array([ 4, 7, -5, 3]) # 2.根据dict创建 sdata = {'Ohio':35000,'Texas':71000,'Oregon':16000,'Utah':5000} obj3 = pd.Series(sdata) obj3 # 输出 \"\"\" Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 \"\"\" Series基本功能 编号 属性或方法 描述 1 axes 返回行轴标签列表。 2 dtype 返回对象的数据类型(dtype)。 3 empty 如果系列为空，则返回True。 4 ndim 返回底层数据的维数，默认定义：1。 5 size 返回基础数据中的元素数。 6 values 将系列作为ndarray返回。 7 head() 返回前n行。 8 tail() 返回最后n行。 # Series基本功能代码 import pandas as pd import numpy as np #Create a series with 100 random numbers s = pd.Series(np.random.randn(4)) # 1.显示axes s.axes \"\"\" The axes are: [RangeIndex(start=0, stop=4, step=1)] \"\"\" # 2.显示empty s.empty \"\"\" False \"\"\" # 3.显示ndim s.ndim \"\"\" 1 \"\"\" # 4.显示size s.size \"\"\" 4 \"\"\" # 5.values实例 s.values \"\"\" [ 1.78737302 -0.60515881 0.18047664 -0.1409218 ] \"\"\" # 6.head默认显示前5个数据 # 7.tail默认显示后5个数据 Series其他方法 # 1.索引 obj2[2] #-5 obj2['a'] #-5 obj2[['a','b','d']] #输出 a -5 b 7 d 4 dtype: int64 # 2.切片 # 与利用下标进行切片不同，使用标签进行切片时，末端是包含的 obj['b':'c'] # 输出 b 1.0 c 2.0 dtype: float64 # 3.重建索引 obj2 = pd.Series([4,7,-5,3],index=['d','b','a','c']) obj3 = obj2.reindex(['a','b','c','d','e']) obj3 # 输出 a -5.0 b 7.0 c 3.0 d 4.0 e NaN dtype: NaN, # reindex出现NaN，使用fill_value属性对NaN填充 obj4 = obj2.reindex(['a','b','c','d','e'],fill_value=0) obj4 #输出 a -5 b 7 c 3 d 4 e 0 dtype: int64 # 4.数据运算 # 可以对Series进行numpy中的一些数组运算 np.exp(obj2) # Series在算术运算中会自动对齐不同索引的数据： obj3 + obj4 #输出 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 # 5.排序和排名 # sort_index根据索引排序 # sort_values根据值排序 obj = pd.Series(range(4),index=['d','a','b','c']) obj.sort_index() #输出： a 1 b 2 c 3 d 0 dtype: int64 obj.sort_values() #输出： d 0 a 1 b 2 c 3 dtype: int64 # 使用rank函数会增加一个排名值 obj = pd.Series([7,-5,7,4,2,0,4]) obj.rank() #输出： 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 obj.rank(method='first') #输出 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 # 6.汇总和计算描述统计 # sum、mean、max、count、median等等 # corr相关系数 # conv协方差 obj1 = pd.Series(np.arange(10),index = list('abcdefghij')) obj2 = pd.Series(np.arange(12),index = list('cdefghijklmn')) obj1.corr(obj2) #1.0 obj1.cov(obj2) #6.0 # 7.唯一数、值计数 obj = pd.Series(['c','a','d','a','a','b','b','c','c']) uniques = obj.unique() uniques #array(['c', 'a', 'd', 'b'], dtype=object) #value_counts()返回各数的计数 obj.value_counts() #输出 a 3 c 3 b 2 d 1 dtype: int64 # 8.处理缺失数据 # isnull判断NaN值 # fillna填充NaN值 # dropna舍弃NaN值 data = pd.Series([1,np.nan,3.5,np.nan,7]) data.fillna(0) #输出 0 1.0 1 0.0 2 3.5 3 0.0 4 7.0 dtype: float64 DataFrame DataFrame创建 pandas.DataFrame( data, index, columns, dtype, copy) 编号 参数 描述 1 data 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。 2 index 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。 3 columns 对于列标签，可选的默认语法是 - np.arange(n)。 这只有在没有索引传递的情况下才是这样。 4 dtype 每列的数据类型。 5 copy 如果默认值为False，则此命令(或任何它)用于复制数据。 # DataFrame创建 # 1.使用list创建 import pandas as pd data = [1,2,3,4,5] df = pd.DataFrame(data) df \"\"\" 0 0 1 1 2 2 3 3 4 4 5 \"\"\" data = [['Alex',10],['Bob',12],['Clarke',13]] df = pd.DataFrame(data,columns=['Name','Age']) df \"\"\" Name Age 0 Alex 10 1 Bob 12 2 Clarke 13 \"\"\" # 2.使用dict创建 data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} df = pd.DataFrame(data) df \"\"\" Age Name 0 28 Tom 1 34 Jack 2 29 Steve 3 42 Ricky \"\"\" d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) df \"\"\" one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4 \"\"\" DataFrame基本功能 编","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#核心数据结构"},{"categories":["python"],"content":" 核心数据结构 pandas最核心的就是Series和DataFrame两个数据结构。 DataFrame可以看做是Series的容器，即：一个DataFrame中可以包含若干个Series。 名称 维度 说明 Series 1维 带有标签的同构类型数组 DataFrame 2维 表格结构，带有标签，大小可变，且可以包含异构的数据列 Series Series的创建 pandas.Series( data, index, dtype, copy)。 编号 参数 描述 1 data 数据采取各种形式，如：ndarray，list，constants 2 index 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。 3 dtype dtype用于数据类型。如果没有，将推断数据类型 4 copy 复制数据，默认为false。 # series的创建 # 1.根据list创建 import pandas as pd obj = pd.Series([4,7,-5,3]) obj # 输出 \"\"\" 0 4 1 7 2 -5 3 3 dtype: int64 \"\"\" obj.index #RangeIndex(start=0, stop=4, step=1) obj.values #array([ 4, 7, -5, 3]) # 2.根据dict创建 sdata = {'Ohio':35000,'Texas':71000,'Oregon':16000,'Utah':5000} obj3 = pd.Series(sdata) obj3 # 输出 \"\"\" Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 \"\"\" Series基本功能 编号 属性或方法 描述 1 axes 返回行轴标签列表。 2 dtype 返回对象的数据类型(dtype)。 3 empty 如果系列为空，则返回True。 4 ndim 返回底层数据的维数，默认定义：1。 5 size 返回基础数据中的元素数。 6 values 将系列作为ndarray返回。 7 head() 返回前n行。 8 tail() 返回最后n行。 # Series基本功能代码 import pandas as pd import numpy as np #Create a series with 100 random numbers s = pd.Series(np.random.randn(4)) # 1.显示axes s.axes \"\"\" The axes are: [RangeIndex(start=0, stop=4, step=1)] \"\"\" # 2.显示empty s.empty \"\"\" False \"\"\" # 3.显示ndim s.ndim \"\"\" 1 \"\"\" # 4.显示size s.size \"\"\" 4 \"\"\" # 5.values实例 s.values \"\"\" [ 1.78737302 -0.60515881 0.18047664 -0.1409218 ] \"\"\" # 6.head默认显示前5个数据 # 7.tail默认显示后5个数据 Series其他方法 # 1.索引 obj2[2] #-5 obj2['a'] #-5 obj2[['a','b','d']] #输出 a -5 b 7 d 4 dtype: int64 # 2.切片 # 与利用下标进行切片不同，使用标签进行切片时，末端是包含的 obj['b':'c'] # 输出 b 1.0 c 2.0 dtype: float64 # 3.重建索引 obj2 = pd.Series([4,7,-5,3],index=['d','b','a','c']) obj3 = obj2.reindex(['a','b','c','d','e']) obj3 # 输出 a -5.0 b 7.0 c 3.0 d 4.0 e NaN dtype: NaN, # reindex出现NaN，使用fill_value属性对NaN填充 obj4 = obj2.reindex(['a','b','c','d','e'],fill_value=0) obj4 #输出 a -5 b 7 c 3 d 4 e 0 dtype: int64 # 4.数据运算 # 可以对Series进行numpy中的一些数组运算 np.exp(obj2) # Series在算术运算中会自动对齐不同索引的数据： obj3 + obj4 #输出 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 # 5.排序和排名 # sort_index根据索引排序 # sort_values根据值排序 obj = pd.Series(range(4),index=['d','a','b','c']) obj.sort_index() #输出： a 1 b 2 c 3 d 0 dtype: int64 obj.sort_values() #输出： d 0 a 1 b 2 c 3 dtype: int64 # 使用rank函数会增加一个排名值 obj = pd.Series([7,-5,7,4,2,0,4]) obj.rank() #输出： 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 obj.rank(method='first') #输出 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 # 6.汇总和计算描述统计 # sum、mean、max、count、median等等 # corr相关系数 # conv协方差 obj1 = pd.Series(np.arange(10),index = list('abcdefghij')) obj2 = pd.Series(np.arange(12),index = list('cdefghijklmn')) obj1.corr(obj2) #1.0 obj1.cov(obj2) #6.0 # 7.唯一数、值计数 obj = pd.Series(['c','a','d','a','a','b','b','c','c']) uniques = obj.unique() uniques #array(['c', 'a', 'd', 'b'], dtype=object) #value_counts()返回各数的计数 obj.value_counts() #输出 a 3 c 3 b 2 d 1 dtype: int64 # 8.处理缺失数据 # isnull判断NaN值 # fillna填充NaN值 # dropna舍弃NaN值 data = pd.Series([1,np.nan,3.5,np.nan,7]) data.fillna(0) #输出 0 1.0 1 0.0 2 3.5 3 0.0 4 7.0 dtype: float64 DataFrame DataFrame创建 pandas.DataFrame( data, index, columns, dtype, copy) 编号 参数 描述 1 data 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。 2 index 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。 3 columns 对于列标签，可选的默认语法是 - np.arange(n)。 这只有在没有索引传递的情况下才是这样。 4 dtype 每列的数据类型。 5 copy 如果默认值为False，则此命令(或任何它)用于复制数据。 # DataFrame创建 # 1.使用list创建 import pandas as pd data = [1,2,3,4,5] df = pd.DataFrame(data) df \"\"\" 0 0 1 1 2 2 3 3 4 4 5 \"\"\" data = [['Alex',10],['Bob',12],['Clarke',13]] df = pd.DataFrame(data,columns=['Name','Age']) df \"\"\" Name Age 0 Alex 10 1 Bob 12 2 Clarke 13 \"\"\" # 2.使用dict创建 data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} df = pd.DataFrame(data) df \"\"\" Age Name 0 28 Tom 1 34 Jack 2 29 Steve 3 42 Ricky \"\"\" d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) df \"\"\" one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4 \"\"\" DataFrame基本功能 编","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#series"},{"categories":["python"],"content":" 核心数据结构 pandas最核心的就是Series和DataFrame两个数据结构。 DataFrame可以看做是Series的容器，即：一个DataFrame中可以包含若干个Series。 名称 维度 说明 Series 1维 带有标签的同构类型数组 DataFrame 2维 表格结构，带有标签，大小可变，且可以包含异构的数据列 Series Series的创建 pandas.Series( data, index, dtype, copy)。 编号 参数 描述 1 data 数据采取各种形式，如：ndarray，list，constants 2 index 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。 3 dtype dtype用于数据类型。如果没有，将推断数据类型 4 copy 复制数据，默认为false。 # series的创建 # 1.根据list创建 import pandas as pd obj = pd.Series([4,7,-5,3]) obj # 输出 \"\"\" 0 4 1 7 2 -5 3 3 dtype: int64 \"\"\" obj.index #RangeIndex(start=0, stop=4, step=1) obj.values #array([ 4, 7, -5, 3]) # 2.根据dict创建 sdata = {'Ohio':35000,'Texas':71000,'Oregon':16000,'Utah':5000} obj3 = pd.Series(sdata) obj3 # 输出 \"\"\" Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 \"\"\" Series基本功能 编号 属性或方法 描述 1 axes 返回行轴标签列表。 2 dtype 返回对象的数据类型(dtype)。 3 empty 如果系列为空，则返回True。 4 ndim 返回底层数据的维数，默认定义：1。 5 size 返回基础数据中的元素数。 6 values 将系列作为ndarray返回。 7 head() 返回前n行。 8 tail() 返回最后n行。 # Series基本功能代码 import pandas as pd import numpy as np #Create a series with 100 random numbers s = pd.Series(np.random.randn(4)) # 1.显示axes s.axes \"\"\" The axes are: [RangeIndex(start=0, stop=4, step=1)] \"\"\" # 2.显示empty s.empty \"\"\" False \"\"\" # 3.显示ndim s.ndim \"\"\" 1 \"\"\" # 4.显示size s.size \"\"\" 4 \"\"\" # 5.values实例 s.values \"\"\" [ 1.78737302 -0.60515881 0.18047664 -0.1409218 ] \"\"\" # 6.head默认显示前5个数据 # 7.tail默认显示后5个数据 Series其他方法 # 1.索引 obj2[2] #-5 obj2['a'] #-5 obj2[['a','b','d']] #输出 a -5 b 7 d 4 dtype: int64 # 2.切片 # 与利用下标进行切片不同，使用标签进行切片时，末端是包含的 obj['b':'c'] # 输出 b 1.0 c 2.0 dtype: float64 # 3.重建索引 obj2 = pd.Series([4,7,-5,3],index=['d','b','a','c']) obj3 = obj2.reindex(['a','b','c','d','e']) obj3 # 输出 a -5.0 b 7.0 c 3.0 d 4.0 e NaN dtype: NaN, # reindex出现NaN，使用fill_value属性对NaN填充 obj4 = obj2.reindex(['a','b','c','d','e'],fill_value=0) obj4 #输出 a -5 b 7 c 3 d 4 e 0 dtype: int64 # 4.数据运算 # 可以对Series进行numpy中的一些数组运算 np.exp(obj2) # Series在算术运算中会自动对齐不同索引的数据： obj3 + obj4 #输出 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 # 5.排序和排名 # sort_index根据索引排序 # sort_values根据值排序 obj = pd.Series(range(4),index=['d','a','b','c']) obj.sort_index() #输出： a 1 b 2 c 3 d 0 dtype: int64 obj.sort_values() #输出： d 0 a 1 b 2 c 3 dtype: int64 # 使用rank函数会增加一个排名值 obj = pd.Series([7,-5,7,4,2,0,4]) obj.rank() #输出： 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 obj.rank(method='first') #输出 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 # 6.汇总和计算描述统计 # sum、mean、max、count、median等等 # corr相关系数 # conv协方差 obj1 = pd.Series(np.arange(10),index = list('abcdefghij')) obj2 = pd.Series(np.arange(12),index = list('cdefghijklmn')) obj1.corr(obj2) #1.0 obj1.cov(obj2) #6.0 # 7.唯一数、值计数 obj = pd.Series(['c','a','d','a','a','b','b','c','c']) uniques = obj.unique() uniques #array(['c', 'a', 'd', 'b'], dtype=object) #value_counts()返回各数的计数 obj.value_counts() #输出 a 3 c 3 b 2 d 1 dtype: int64 # 8.处理缺失数据 # isnull判断NaN值 # fillna填充NaN值 # dropna舍弃NaN值 data = pd.Series([1,np.nan,3.5,np.nan,7]) data.fillna(0) #输出 0 1.0 1 0.0 2 3.5 3 0.0 4 7.0 dtype: float64 DataFrame DataFrame创建 pandas.DataFrame( data, index, columns, dtype, copy) 编号 参数 描述 1 data 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。 2 index 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。 3 columns 对于列标签，可选的默认语法是 - np.arange(n)。 这只有在没有索引传递的情况下才是这样。 4 dtype 每列的数据类型。 5 copy 如果默认值为False，则此命令(或任何它)用于复制数据。 # DataFrame创建 # 1.使用list创建 import pandas as pd data = [1,2,3,4,5] df = pd.DataFrame(data) df \"\"\" 0 0 1 1 2 2 3 3 4 4 5 \"\"\" data = [['Alex',10],['Bob',12],['Clarke',13]] df = pd.DataFrame(data,columns=['Name','Age']) df \"\"\" Name Age 0 Alex 10 1 Bob 12 2 Clarke 13 \"\"\" # 2.使用dict创建 data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} df = pd.DataFrame(data) df \"\"\" Age Name 0 28 Tom 1 34 Jack 2 29 Steve 3 42 Ricky \"\"\" d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) df \"\"\" one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4 \"\"\" DataFrame基本功能 编","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#series的创建"},{"categories":["python"],"content":" 核心数据结构 pandas最核心的就是Series和DataFrame两个数据结构。 DataFrame可以看做是Series的容器，即：一个DataFrame中可以包含若干个Series。 名称 维度 说明 Series 1维 带有标签的同构类型数组 DataFrame 2维 表格结构，带有标签，大小可变，且可以包含异构的数据列 Series Series的创建 pandas.Series( data, index, dtype, copy)。 编号 参数 描述 1 data 数据采取各种形式，如：ndarray，list，constants 2 index 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。 3 dtype dtype用于数据类型。如果没有，将推断数据类型 4 copy 复制数据，默认为false。 # series的创建 # 1.根据list创建 import pandas as pd obj = pd.Series([4,7,-5,3]) obj # 输出 \"\"\" 0 4 1 7 2 -5 3 3 dtype: int64 \"\"\" obj.index #RangeIndex(start=0, stop=4, step=1) obj.values #array([ 4, 7, -5, 3]) # 2.根据dict创建 sdata = {'Ohio':35000,'Texas':71000,'Oregon':16000,'Utah':5000} obj3 = pd.Series(sdata) obj3 # 输出 \"\"\" Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 \"\"\" Series基本功能 编号 属性或方法 描述 1 axes 返回行轴标签列表。 2 dtype 返回对象的数据类型(dtype)。 3 empty 如果系列为空，则返回True。 4 ndim 返回底层数据的维数，默认定义：1。 5 size 返回基础数据中的元素数。 6 values 将系列作为ndarray返回。 7 head() 返回前n行。 8 tail() 返回最后n行。 # Series基本功能代码 import pandas as pd import numpy as np #Create a series with 100 random numbers s = pd.Series(np.random.randn(4)) # 1.显示axes s.axes \"\"\" The axes are: [RangeIndex(start=0, stop=4, step=1)] \"\"\" # 2.显示empty s.empty \"\"\" False \"\"\" # 3.显示ndim s.ndim \"\"\" 1 \"\"\" # 4.显示size s.size \"\"\" 4 \"\"\" # 5.values实例 s.values \"\"\" [ 1.78737302 -0.60515881 0.18047664 -0.1409218 ] \"\"\" # 6.head默认显示前5个数据 # 7.tail默认显示后5个数据 Series其他方法 # 1.索引 obj2[2] #-5 obj2['a'] #-5 obj2[['a','b','d']] #输出 a -5 b 7 d 4 dtype: int64 # 2.切片 # 与利用下标进行切片不同，使用标签进行切片时，末端是包含的 obj['b':'c'] # 输出 b 1.0 c 2.0 dtype: float64 # 3.重建索引 obj2 = pd.Series([4,7,-5,3],index=['d','b','a','c']) obj3 = obj2.reindex(['a','b','c','d','e']) obj3 # 输出 a -5.0 b 7.0 c 3.0 d 4.0 e NaN dtype: NaN, # reindex出现NaN，使用fill_value属性对NaN填充 obj4 = obj2.reindex(['a','b','c','d','e'],fill_value=0) obj4 #输出 a -5 b 7 c 3 d 4 e 0 dtype: int64 # 4.数据运算 # 可以对Series进行numpy中的一些数组运算 np.exp(obj2) # Series在算术运算中会自动对齐不同索引的数据： obj3 + obj4 #输出 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 # 5.排序和排名 # sort_index根据索引排序 # sort_values根据值排序 obj = pd.Series(range(4),index=['d','a','b','c']) obj.sort_index() #输出： a 1 b 2 c 3 d 0 dtype: int64 obj.sort_values() #输出： d 0 a 1 b 2 c 3 dtype: int64 # 使用rank函数会增加一个排名值 obj = pd.Series([7,-5,7,4,2,0,4]) obj.rank() #输出： 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 obj.rank(method='first') #输出 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 # 6.汇总和计算描述统计 # sum、mean、max、count、median等等 # corr相关系数 # conv协方差 obj1 = pd.Series(np.arange(10),index = list('abcdefghij')) obj2 = pd.Series(np.arange(12),index = list('cdefghijklmn')) obj1.corr(obj2) #1.0 obj1.cov(obj2) #6.0 # 7.唯一数、值计数 obj = pd.Series(['c','a','d','a','a','b','b','c','c']) uniques = obj.unique() uniques #array(['c', 'a', 'd', 'b'], dtype=object) #value_counts()返回各数的计数 obj.value_counts() #输出 a 3 c 3 b 2 d 1 dtype: int64 # 8.处理缺失数据 # isnull判断NaN值 # fillna填充NaN值 # dropna舍弃NaN值 data = pd.Series([1,np.nan,3.5,np.nan,7]) data.fillna(0) #输出 0 1.0 1 0.0 2 3.5 3 0.0 4 7.0 dtype: float64 DataFrame DataFrame创建 pandas.DataFrame( data, index, columns, dtype, copy) 编号 参数 描述 1 data 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。 2 index 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。 3 columns 对于列标签，可选的默认语法是 - np.arange(n)。 这只有在没有索引传递的情况下才是这样。 4 dtype 每列的数据类型。 5 copy 如果默认值为False，则此命令(或任何它)用于复制数据。 # DataFrame创建 # 1.使用list创建 import pandas as pd data = [1,2,3,4,5] df = pd.DataFrame(data) df \"\"\" 0 0 1 1 2 2 3 3 4 4 5 \"\"\" data = [['Alex',10],['Bob',12],['Clarke',13]] df = pd.DataFrame(data,columns=['Name','Age']) df \"\"\" Name Age 0 Alex 10 1 Bob 12 2 Clarke 13 \"\"\" # 2.使用dict创建 data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} df = pd.DataFrame(data) df \"\"\" Age Name 0 28 Tom 1 34 Jack 2 29 Steve 3 42 Ricky \"\"\" d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) df \"\"\" one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4 \"\"\" DataFrame基本功能 编","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#series基本功能"},{"categories":["python"],"content":" 核心数据结构 pandas最核心的就是Series和DataFrame两个数据结构。 DataFrame可以看做是Series的容器，即：一个DataFrame中可以包含若干个Series。 名称 维度 说明 Series 1维 带有标签的同构类型数组 DataFrame 2维 表格结构，带有标签，大小可变，且可以包含异构的数据列 Series Series的创建 pandas.Series( data, index, dtype, copy)。 编号 参数 描述 1 data 数据采取各种形式，如：ndarray，list，constants 2 index 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。 3 dtype dtype用于数据类型。如果没有，将推断数据类型 4 copy 复制数据，默认为false。 # series的创建 # 1.根据list创建 import pandas as pd obj = pd.Series([4,7,-5,3]) obj # 输出 \"\"\" 0 4 1 7 2 -5 3 3 dtype: int64 \"\"\" obj.index #RangeIndex(start=0, stop=4, step=1) obj.values #array([ 4, 7, -5, 3]) # 2.根据dict创建 sdata = {'Ohio':35000,'Texas':71000,'Oregon':16000,'Utah':5000} obj3 = pd.Series(sdata) obj3 # 输出 \"\"\" Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 \"\"\" Series基本功能 编号 属性或方法 描述 1 axes 返回行轴标签列表。 2 dtype 返回对象的数据类型(dtype)。 3 empty 如果系列为空，则返回True。 4 ndim 返回底层数据的维数，默认定义：1。 5 size 返回基础数据中的元素数。 6 values 将系列作为ndarray返回。 7 head() 返回前n行。 8 tail() 返回最后n行。 # Series基本功能代码 import pandas as pd import numpy as np #Create a series with 100 random numbers s = pd.Series(np.random.randn(4)) # 1.显示axes s.axes \"\"\" The axes are: [RangeIndex(start=0, stop=4, step=1)] \"\"\" # 2.显示empty s.empty \"\"\" False \"\"\" # 3.显示ndim s.ndim \"\"\" 1 \"\"\" # 4.显示size s.size \"\"\" 4 \"\"\" # 5.values实例 s.values \"\"\" [ 1.78737302 -0.60515881 0.18047664 -0.1409218 ] \"\"\" # 6.head默认显示前5个数据 # 7.tail默认显示后5个数据 Series其他方法 # 1.索引 obj2[2] #-5 obj2['a'] #-5 obj2[['a','b','d']] #输出 a -5 b 7 d 4 dtype: int64 # 2.切片 # 与利用下标进行切片不同，使用标签进行切片时，末端是包含的 obj['b':'c'] # 输出 b 1.0 c 2.0 dtype: float64 # 3.重建索引 obj2 = pd.Series([4,7,-5,3],index=['d','b','a','c']) obj3 = obj2.reindex(['a','b','c','d','e']) obj3 # 输出 a -5.0 b 7.0 c 3.0 d 4.0 e NaN dtype: NaN, # reindex出现NaN，使用fill_value属性对NaN填充 obj4 = obj2.reindex(['a','b','c','d','e'],fill_value=0) obj4 #输出 a -5 b 7 c 3 d 4 e 0 dtype: int64 # 4.数据运算 # 可以对Series进行numpy中的一些数组运算 np.exp(obj2) # Series在算术运算中会自动对齐不同索引的数据： obj3 + obj4 #输出 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 # 5.排序和排名 # sort_index根据索引排序 # sort_values根据值排序 obj = pd.Series(range(4),index=['d','a','b','c']) obj.sort_index() #输出： a 1 b 2 c 3 d 0 dtype: int64 obj.sort_values() #输出： d 0 a 1 b 2 c 3 dtype: int64 # 使用rank函数会增加一个排名值 obj = pd.Series([7,-5,7,4,2,0,4]) obj.rank() #输出： 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 obj.rank(method='first') #输出 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 # 6.汇总和计算描述统计 # sum、mean、max、count、median等等 # corr相关系数 # conv协方差 obj1 = pd.Series(np.arange(10),index = list('abcdefghij')) obj2 = pd.Series(np.arange(12),index = list('cdefghijklmn')) obj1.corr(obj2) #1.0 obj1.cov(obj2) #6.0 # 7.唯一数、值计数 obj = pd.Series(['c','a','d','a','a','b','b','c','c']) uniques = obj.unique() uniques #array(['c', 'a', 'd', 'b'], dtype=object) #value_counts()返回各数的计数 obj.value_counts() #输出 a 3 c 3 b 2 d 1 dtype: int64 # 8.处理缺失数据 # isnull判断NaN值 # fillna填充NaN值 # dropna舍弃NaN值 data = pd.Series([1,np.nan,3.5,np.nan,7]) data.fillna(0) #输出 0 1.0 1 0.0 2 3.5 3 0.0 4 7.0 dtype: float64 DataFrame DataFrame创建 pandas.DataFrame( data, index, columns, dtype, copy) 编号 参数 描述 1 data 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。 2 index 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。 3 columns 对于列标签，可选的默认语法是 - np.arange(n)。 这只有在没有索引传递的情况下才是这样。 4 dtype 每列的数据类型。 5 copy 如果默认值为False，则此命令(或任何它)用于复制数据。 # DataFrame创建 # 1.使用list创建 import pandas as pd data = [1,2,3,4,5] df = pd.DataFrame(data) df \"\"\" 0 0 1 1 2 2 3 3 4 4 5 \"\"\" data = [['Alex',10],['Bob',12],['Clarke',13]] df = pd.DataFrame(data,columns=['Name','Age']) df \"\"\" Name Age 0 Alex 10 1 Bob 12 2 Clarke 13 \"\"\" # 2.使用dict创建 data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} df = pd.DataFrame(data) df \"\"\" Age Name 0 28 Tom 1 34 Jack 2 29 Steve 3 42 Ricky \"\"\" d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) df \"\"\" one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4 \"\"\" DataFrame基本功能 编","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#series其他方法"},{"categories":["python"],"content":" 核心数据结构 pandas最核心的就是Series和DataFrame两个数据结构。 DataFrame可以看做是Series的容器，即：一个DataFrame中可以包含若干个Series。 名称 维度 说明 Series 1维 带有标签的同构类型数组 DataFrame 2维 表格结构，带有标签，大小可变，且可以包含异构的数据列 Series Series的创建 pandas.Series( data, index, dtype, copy)。 编号 参数 描述 1 data 数据采取各种形式，如：ndarray，list，constants 2 index 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。 3 dtype dtype用于数据类型。如果没有，将推断数据类型 4 copy 复制数据，默认为false。 # series的创建 # 1.根据list创建 import pandas as pd obj = pd.Series([4,7,-5,3]) obj # 输出 \"\"\" 0 4 1 7 2 -5 3 3 dtype: int64 \"\"\" obj.index #RangeIndex(start=0, stop=4, step=1) obj.values #array([ 4, 7, -5, 3]) # 2.根据dict创建 sdata = {'Ohio':35000,'Texas':71000,'Oregon':16000,'Utah':5000} obj3 = pd.Series(sdata) obj3 # 输出 \"\"\" Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 \"\"\" Series基本功能 编号 属性或方法 描述 1 axes 返回行轴标签列表。 2 dtype 返回对象的数据类型(dtype)。 3 empty 如果系列为空，则返回True。 4 ndim 返回底层数据的维数，默认定义：1。 5 size 返回基础数据中的元素数。 6 values 将系列作为ndarray返回。 7 head() 返回前n行。 8 tail() 返回最后n行。 # Series基本功能代码 import pandas as pd import numpy as np #Create a series with 100 random numbers s = pd.Series(np.random.randn(4)) # 1.显示axes s.axes \"\"\" The axes are: [RangeIndex(start=0, stop=4, step=1)] \"\"\" # 2.显示empty s.empty \"\"\" False \"\"\" # 3.显示ndim s.ndim \"\"\" 1 \"\"\" # 4.显示size s.size \"\"\" 4 \"\"\" # 5.values实例 s.values \"\"\" [ 1.78737302 -0.60515881 0.18047664 -0.1409218 ] \"\"\" # 6.head默认显示前5个数据 # 7.tail默认显示后5个数据 Series其他方法 # 1.索引 obj2[2] #-5 obj2['a'] #-5 obj2[['a','b','d']] #输出 a -5 b 7 d 4 dtype: int64 # 2.切片 # 与利用下标进行切片不同，使用标签进行切片时，末端是包含的 obj['b':'c'] # 输出 b 1.0 c 2.0 dtype: float64 # 3.重建索引 obj2 = pd.Series([4,7,-5,3],index=['d','b','a','c']) obj3 = obj2.reindex(['a','b','c','d','e']) obj3 # 输出 a -5.0 b 7.0 c 3.0 d 4.0 e NaN dtype: NaN, # reindex出现NaN，使用fill_value属性对NaN填充 obj4 = obj2.reindex(['a','b','c','d','e'],fill_value=0) obj4 #输出 a -5 b 7 c 3 d 4 e 0 dtype: int64 # 4.数据运算 # 可以对Series进行numpy中的一些数组运算 np.exp(obj2) # Series在算术运算中会自动对齐不同索引的数据： obj3 + obj4 #输出 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 # 5.排序和排名 # sort_index根据索引排序 # sort_values根据值排序 obj = pd.Series(range(4),index=['d','a','b','c']) obj.sort_index() #输出： a 1 b 2 c 3 d 0 dtype: int64 obj.sort_values() #输出： d 0 a 1 b 2 c 3 dtype: int64 # 使用rank函数会增加一个排名值 obj = pd.Series([7,-5,7,4,2,0,4]) obj.rank() #输出： 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 obj.rank(method='first') #输出 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 # 6.汇总和计算描述统计 # sum、mean、max、count、median等等 # corr相关系数 # conv协方差 obj1 = pd.Series(np.arange(10),index = list('abcdefghij')) obj2 = pd.Series(np.arange(12),index = list('cdefghijklmn')) obj1.corr(obj2) #1.0 obj1.cov(obj2) #6.0 # 7.唯一数、值计数 obj = pd.Series(['c','a','d','a','a','b','b','c','c']) uniques = obj.unique() uniques #array(['c', 'a', 'd', 'b'], dtype=object) #value_counts()返回各数的计数 obj.value_counts() #输出 a 3 c 3 b 2 d 1 dtype: int64 # 8.处理缺失数据 # isnull判断NaN值 # fillna填充NaN值 # dropna舍弃NaN值 data = pd.Series([1,np.nan,3.5,np.nan,7]) data.fillna(0) #输出 0 1.0 1 0.0 2 3.5 3 0.0 4 7.0 dtype: float64 DataFrame DataFrame创建 pandas.DataFrame( data, index, columns, dtype, copy) 编号 参数 描述 1 data 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。 2 index 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。 3 columns 对于列标签，可选的默认语法是 - np.arange(n)。 这只有在没有索引传递的情况下才是这样。 4 dtype 每列的数据类型。 5 copy 如果默认值为False，则此命令(或任何它)用于复制数据。 # DataFrame创建 # 1.使用list创建 import pandas as pd data = [1,2,3,4,5] df = pd.DataFrame(data) df \"\"\" 0 0 1 1 2 2 3 3 4 4 5 \"\"\" data = [['Alex',10],['Bob',12],['Clarke',13]] df = pd.DataFrame(data,columns=['Name','Age']) df \"\"\" Name Age 0 Alex 10 1 Bob 12 2 Clarke 13 \"\"\" # 2.使用dict创建 data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} df = pd.DataFrame(data) df \"\"\" Age Name 0 28 Tom 1 34 Jack 2 29 Steve 3 42 Ricky \"\"\" d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) df \"\"\" one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4 \"\"\" DataFrame基本功能 编","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#dataframe"},{"categories":["python"],"content":" 核心数据结构 pandas最核心的就是Series和DataFrame两个数据结构。 DataFrame可以看做是Series的容器，即：一个DataFrame中可以包含若干个Series。 名称 维度 说明 Series 1维 带有标签的同构类型数组 DataFrame 2维 表格结构，带有标签，大小可变，且可以包含异构的数据列 Series Series的创建 pandas.Series( data, index, dtype, copy)。 编号 参数 描述 1 data 数据采取各种形式，如：ndarray，list，constants 2 index 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。 3 dtype dtype用于数据类型。如果没有，将推断数据类型 4 copy 复制数据，默认为false。 # series的创建 # 1.根据list创建 import pandas as pd obj = pd.Series([4,7,-5,3]) obj # 输出 \"\"\" 0 4 1 7 2 -5 3 3 dtype: int64 \"\"\" obj.index #RangeIndex(start=0, stop=4, step=1) obj.values #array([ 4, 7, -5, 3]) # 2.根据dict创建 sdata = {'Ohio':35000,'Texas':71000,'Oregon':16000,'Utah':5000} obj3 = pd.Series(sdata) obj3 # 输出 \"\"\" Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 \"\"\" Series基本功能 编号 属性或方法 描述 1 axes 返回行轴标签列表。 2 dtype 返回对象的数据类型(dtype)。 3 empty 如果系列为空，则返回True。 4 ndim 返回底层数据的维数，默认定义：1。 5 size 返回基础数据中的元素数。 6 values 将系列作为ndarray返回。 7 head() 返回前n行。 8 tail() 返回最后n行。 # Series基本功能代码 import pandas as pd import numpy as np #Create a series with 100 random numbers s = pd.Series(np.random.randn(4)) # 1.显示axes s.axes \"\"\" The axes are: [RangeIndex(start=0, stop=4, step=1)] \"\"\" # 2.显示empty s.empty \"\"\" False \"\"\" # 3.显示ndim s.ndim \"\"\" 1 \"\"\" # 4.显示size s.size \"\"\" 4 \"\"\" # 5.values实例 s.values \"\"\" [ 1.78737302 -0.60515881 0.18047664 -0.1409218 ] \"\"\" # 6.head默认显示前5个数据 # 7.tail默认显示后5个数据 Series其他方法 # 1.索引 obj2[2] #-5 obj2['a'] #-5 obj2[['a','b','d']] #输出 a -5 b 7 d 4 dtype: int64 # 2.切片 # 与利用下标进行切片不同，使用标签进行切片时，末端是包含的 obj['b':'c'] # 输出 b 1.0 c 2.0 dtype: float64 # 3.重建索引 obj2 = pd.Series([4,7,-5,3],index=['d','b','a','c']) obj3 = obj2.reindex(['a','b','c','d','e']) obj3 # 输出 a -5.0 b 7.0 c 3.0 d 4.0 e NaN dtype: NaN, # reindex出现NaN，使用fill_value属性对NaN填充 obj4 = obj2.reindex(['a','b','c','d','e'],fill_value=0) obj4 #输出 a -5 b 7 c 3 d 4 e 0 dtype: int64 # 4.数据运算 # 可以对Series进行numpy中的一些数组运算 np.exp(obj2) # Series在算术运算中会自动对齐不同索引的数据： obj3 + obj4 #输出 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 # 5.排序和排名 # sort_index根据索引排序 # sort_values根据值排序 obj = pd.Series(range(4),index=['d','a','b','c']) obj.sort_index() #输出： a 1 b 2 c 3 d 0 dtype: int64 obj.sort_values() #输出： d 0 a 1 b 2 c 3 dtype: int64 # 使用rank函数会增加一个排名值 obj = pd.Series([7,-5,7,4,2,0,4]) obj.rank() #输出： 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 obj.rank(method='first') #输出 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 # 6.汇总和计算描述统计 # sum、mean、max、count、median等等 # corr相关系数 # conv协方差 obj1 = pd.Series(np.arange(10),index = list('abcdefghij')) obj2 = pd.Series(np.arange(12),index = list('cdefghijklmn')) obj1.corr(obj2) #1.0 obj1.cov(obj2) #6.0 # 7.唯一数、值计数 obj = pd.Series(['c','a','d','a','a','b','b','c','c']) uniques = obj.unique() uniques #array(['c', 'a', 'd', 'b'], dtype=object) #value_counts()返回各数的计数 obj.value_counts() #输出 a 3 c 3 b 2 d 1 dtype: int64 # 8.处理缺失数据 # isnull判断NaN值 # fillna填充NaN值 # dropna舍弃NaN值 data = pd.Series([1,np.nan,3.5,np.nan,7]) data.fillna(0) #输出 0 1.0 1 0.0 2 3.5 3 0.0 4 7.0 dtype: float64 DataFrame DataFrame创建 pandas.DataFrame( data, index, columns, dtype, copy) 编号 参数 描述 1 data 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。 2 index 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。 3 columns 对于列标签，可选的默认语法是 - np.arange(n)。 这只有在没有索引传递的情况下才是这样。 4 dtype 每列的数据类型。 5 copy 如果默认值为False，则此命令(或任何它)用于复制数据。 # DataFrame创建 # 1.使用list创建 import pandas as pd data = [1,2,3,4,5] df = pd.DataFrame(data) df \"\"\" 0 0 1 1 2 2 3 3 4 4 5 \"\"\" data = [['Alex',10],['Bob',12],['Clarke',13]] df = pd.DataFrame(data,columns=['Name','Age']) df \"\"\" Name Age 0 Alex 10 1 Bob 12 2 Clarke 13 \"\"\" # 2.使用dict创建 data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} df = pd.DataFrame(data) df \"\"\" Age Name 0 28 Tom 1 34 Jack 2 29 Steve 3 42 Ricky \"\"\" d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) df \"\"\" one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4 \"\"\" DataFrame基本功能 编","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#dataframe创建"},{"categories":["python"],"content":" 核心数据结构 pandas最核心的就是Series和DataFrame两个数据结构。 DataFrame可以看做是Series的容器，即：一个DataFrame中可以包含若干个Series。 名称 维度 说明 Series 1维 带有标签的同构类型数组 DataFrame 2维 表格结构，带有标签，大小可变，且可以包含异构的数据列 Series Series的创建 pandas.Series( data, index, dtype, copy)。 编号 参数 描述 1 data 数据采取各种形式，如：ndarray，list，constants 2 index 索引值必须是唯一的和散列的，与数据的长度相同。 默认np.arange(n)如果没有索引被传递。 3 dtype dtype用于数据类型。如果没有，将推断数据类型 4 copy 复制数据，默认为false。 # series的创建 # 1.根据list创建 import pandas as pd obj = pd.Series([4,7,-5,3]) obj # 输出 \"\"\" 0 4 1 7 2 -5 3 3 dtype: int64 \"\"\" obj.index #RangeIndex(start=0, stop=4, step=1) obj.values #array([ 4, 7, -5, 3]) # 2.根据dict创建 sdata = {'Ohio':35000,'Texas':71000,'Oregon':16000,'Utah':5000} obj3 = pd.Series(sdata) obj3 # 输出 \"\"\" Ohio 35000 Oregon 16000 Texas 71000 Utah 5000 dtype: int64 \"\"\" Series基本功能 编号 属性或方法 描述 1 axes 返回行轴标签列表。 2 dtype 返回对象的数据类型(dtype)。 3 empty 如果系列为空，则返回True。 4 ndim 返回底层数据的维数，默认定义：1。 5 size 返回基础数据中的元素数。 6 values 将系列作为ndarray返回。 7 head() 返回前n行。 8 tail() 返回最后n行。 # Series基本功能代码 import pandas as pd import numpy as np #Create a series with 100 random numbers s = pd.Series(np.random.randn(4)) # 1.显示axes s.axes \"\"\" The axes are: [RangeIndex(start=0, stop=4, step=1)] \"\"\" # 2.显示empty s.empty \"\"\" False \"\"\" # 3.显示ndim s.ndim \"\"\" 1 \"\"\" # 4.显示size s.size \"\"\" 4 \"\"\" # 5.values实例 s.values \"\"\" [ 1.78737302 -0.60515881 0.18047664 -0.1409218 ] \"\"\" # 6.head默认显示前5个数据 # 7.tail默认显示后5个数据 Series其他方法 # 1.索引 obj2[2] #-5 obj2['a'] #-5 obj2[['a','b','d']] #输出 a -5 b 7 d 4 dtype: int64 # 2.切片 # 与利用下标进行切片不同，使用标签进行切片时，末端是包含的 obj['b':'c'] # 输出 b 1.0 c 2.0 dtype: float64 # 3.重建索引 obj2 = pd.Series([4,7,-5,3],index=['d','b','a','c']) obj3 = obj2.reindex(['a','b','c','d','e']) obj3 # 输出 a -5.0 b 7.0 c 3.0 d 4.0 e NaN dtype: NaN, # reindex出现NaN，使用fill_value属性对NaN填充 obj4 = obj2.reindex(['a','b','c','d','e'],fill_value=0) obj4 #输出 a -5 b 7 c 3 d 4 e 0 dtype: int64 # 4.数据运算 # 可以对Series进行numpy中的一些数组运算 np.exp(obj2) # Series在算术运算中会自动对齐不同索引的数据： obj3 + obj4 #输出 California NaN Ohio 70000.0 Oregon 32000.0 Texas 142000.0 Utah NaN dtype: float64 # 5.排序和排名 # sort_index根据索引排序 # sort_values根据值排序 obj = pd.Series(range(4),index=['d','a','b','c']) obj.sort_index() #输出： a 1 b 2 c 3 d 0 dtype: int64 obj.sort_values() #输出： d 0 a 1 b 2 c 3 dtype: int64 # 使用rank函数会增加一个排名值 obj = pd.Series([7,-5,7,4,2,0,4]) obj.rank() #输出： 0 6.5 1 1.0 2 6.5 3 4.5 4 3.0 5 2.0 6 4.5 dtype: float64 obj.rank(method='first') #输出 0 6.0 1 1.0 2 7.0 3 4.0 4 3.0 5 2.0 6 5.0 dtype: float64 # 6.汇总和计算描述统计 # sum、mean、max、count、median等等 # corr相关系数 # conv协方差 obj1 = pd.Series(np.arange(10),index = list('abcdefghij')) obj2 = pd.Series(np.arange(12),index = list('cdefghijklmn')) obj1.corr(obj2) #1.0 obj1.cov(obj2) #6.0 # 7.唯一数、值计数 obj = pd.Series(['c','a','d','a','a','b','b','c','c']) uniques = obj.unique() uniques #array(['c', 'a', 'd', 'b'], dtype=object) #value_counts()返回各数的计数 obj.value_counts() #输出 a 3 c 3 b 2 d 1 dtype: int64 # 8.处理缺失数据 # isnull判断NaN值 # fillna填充NaN值 # dropna舍弃NaN值 data = pd.Series([1,np.nan,3.5,np.nan,7]) data.fillna(0) #输出 0 1.0 1 0.0 2 3.5 3 0.0 4 7.0 dtype: float64 DataFrame DataFrame创建 pandas.DataFrame( data, index, columns, dtype, copy) 编号 参数 描述 1 data 数据采取各种形式，如:ndarray，series，map，lists，dict，constant和另一个DataFrame。 2 index 对于行标签，要用于结果帧的索引是可选缺省值np.arrange(n)，如果没有传递索引值。 3 columns 对于列标签，可选的默认语法是 - np.arange(n)。 这只有在没有索引传递的情况下才是这样。 4 dtype 每列的数据类型。 5 copy 如果默认值为False，则此命令(或任何它)用于复制数据。 # DataFrame创建 # 1.使用list创建 import pandas as pd data = [1,2,3,4,5] df = pd.DataFrame(data) df \"\"\" 0 0 1 1 2 2 3 3 4 4 5 \"\"\" data = [['Alex',10],['Bob',12],['Clarke',13]] df = pd.DataFrame(data,columns=['Name','Age']) df \"\"\" Name Age 0 Alex 10 1 Bob 12 2 Clarke 13 \"\"\" # 2.使用dict创建 data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]} df = pd.DataFrame(data) df \"\"\" Age Name 0 28 Tom 1 34 Jack 2 29 Steve 3 42 Ricky \"\"\" d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])} df = pd.DataFrame(d) df \"\"\" one two a 1.0 1 b 2.0 2 c 3.0 3 d NaN 4 \"\"\" DataFrame基本功能 编","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#dataframe基本功能"},{"categories":["python"],"content":" Pandas常用函数 统计函数 编号 函数 描述 1 count() 非空观测数量 2 sum() 所有值之和 3 mean() 所有值的平均值 4 median() 所有值的中位数 5 mode() 值的模值 6 std() 值的标准偏差 7 min() 所有值中的最小值 8 max() 所有值中的最大值 9 abs() 绝对值 10 prod() 数组元素的乘积 11 cumsum() 累计总和 12 cumprod() 累计乘积 13 pct_change() 计算变化百分比 14 conv() 计算协方差 15 corr() 相关性计算 16 rank() 数据排名 import pandas as pd import numpy as np #Create a Dictionary of series d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Minsu','Jack', 'Lee','David','Gasper','Betina','Andres']), 'Age':pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]), 'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])} #Create a DataFrame df = pd.DataFrame(d) df \"\"\" Age Name Rating 0 25 Tom 4.23 1 26 James 3.24 2 25 Ricky 3.98 3 23 Vin 2.56 4 30 Steve 3.20 5 29 Minsu 4.60 6 23 Jack 3.80 7 34 Lee 3.78 8 40 David 2.98 9 30 Gasper 4.80 10 51 Betina 4.10 11 46 Andres 3.65 \"\"\" # 1.sum(),默认axis=0 df.sum() \"\"\" Age 382 Name TomJamesRickyVinSteveMinsuJackLeeDavidGasperBe... Rating 44.92 dtype: object \"\"\" # 2.mean() df.mean() \"\"\" Age 31.833333 Rating 3.743333 dtype: float64 \"\"\" # 3.std() df.std() \"\"\" Age 9.232682 Rating 0.661628 dtype: float64 \"\"\" # 汇总函数 describe() df.describe() \"\"\" Age Rating count 12.000000 12.000000 mean 31.833333 3.743333 std 9.232682 0.661628 min 23.000000 2.560000 25% 25.000000 3.230000 50% 29.500000 3.790000 75% 35.500000 4.132500 max 51.000000 4.800000 \"\"\" 应用函数 要将自定义或其他库的函数应用于Pandas对象，有三个重要的方法，下面来讨论如何使用这些方法。使用适当的方法取决于函数是否期望在整个DataFrame，行或列或元素上进行操作。 表合理函数应用：pipe() 行或列函数应用：apply() 元素函数应用：applymap() # 1.表格函数应用 import pandas as pd import numpy as np def adder(ele1,ele2): return ele1+ele2 df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.pipe(adder,2) \"\"\" col1 col2 col3 0 2.176704 2.219691 1.509360 1 2.222378 2.422167 3.953921 2 2.241096 1.135424 2.696432 3 2.355763 0.376672 1.182570 4 2.308743 2.714767 2.130288 \"\"\" # 2.行列应用函数 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.apply(np.mean) \"\"\" col1 col2 col3 0 0.343569 -1.013287 1.131245 1 0.508922 -0.949778 -1.600569 2 -1.182331 -0.420703 -1.725400 3 0.860265 2.069038 -0.537648 4 0.876758 -0.238051 0.473992 \"\"\" # 3.元素函数应用 import pandas as pd import numpy as np # My custom function df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.applymap(lambda x:x*100) \"\"\" col1 col2 col3 0 17.670426 21.969052 -49.064031 1 22.237846 42.216693 195.392124 2 24.109576 -86.457646 69.643171 3 35.576312 -162.332803 -81.743023 4 30.874333 71.476717 13.028751 \"\"\" 迭代函数 Pandas对象之间的基本迭代的行为取决于类型。当迭代一个系列时，它被视为数组式，基本迭代产生这些值。其他数据结构，如：DataFrame，遵循类似惯例迭代对象的键。 要遍历数据帧(DataFrame)中的行，可以使用以下函数 - iteritems() - 迭代(key，value)对 iterrows() - 将行迭代为(索引，系列)对 itertuples() - 以namedtuples的形式迭代行 # 1.iteritems,迭代的计算列 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns=['col1','col2','col3']) for key,value in df.iteritems(): print (key,value) \"\"\" col1 0 0.802390 1 0.324060 2 0.256811 3 0.839186 Name: col1, dtype: float64 col2 0 1.624313 1 -1.033582 2 1.796663 3 1.856277 Name: col2, dtype: float64 col3 0 -0.022142 1 -0.230820 2 1.160691 3 -0.830279 Name: col3, dtype: float64 \"\"\" # 2.iterrows 迭代计算行 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns = ['col1','col2','col3']) for row_index,row in df.iterrows(): print (row_index,row) \"\"\" 0 col1 1.529759 col2 0.762811 col3 -0.634691 Name: 0, dtype: float64 1 col1 -0.944087 col2 1.420919 col3 -0.507895 Name: 1, dtype: float64 2 col1 -0.077287 col2 -0.858556 col3 -0.663385 Name: 2, dtype: float64 3 col1 -1.638578 col2 0.059866 col3 0.493482 Name: 3, dtype: float64 \"\"\" 字符与文本处理函数 编号 函数 描述 1 lower() 将Series/Index中的字符串转换为小写。 2 upper() 将Series/Index中的字符串转换为大写。 3 len() 计算字符串长度。 4 strip() 帮助从两侧的系列/索引中的每个字符串中删除空格(包括换行符)。 5 split(' ') 用给定的模式拆分每个字符串。 6 cat(sep=' ') 使用给定的分隔符连接系列/索引元素。 7 get_dummies() 返回具有单热编码值的数据帧(DataFrame)。 8 contains(pattern) 如果元素中包含子字符串，则返回每个元素的布尔值True，否则为False。 9 replace(a,b) 将值a替换为值b。 10 repeat","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas常用函数"},{"categories":["python"],"content":" Pandas常用函数 统计函数 编号 函数 描述 1 count() 非空观测数量 2 sum() 所有值之和 3 mean() 所有值的平均值 4 median() 所有值的中位数 5 mode() 值的模值 6 std() 值的标准偏差 7 min() 所有值中的最小值 8 max() 所有值中的最大值 9 abs() 绝对值 10 prod() 数组元素的乘积 11 cumsum() 累计总和 12 cumprod() 累计乘积 13 pct_change() 计算变化百分比 14 conv() 计算协方差 15 corr() 相关性计算 16 rank() 数据排名 import pandas as pd import numpy as np #Create a Dictionary of series d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Minsu','Jack', 'Lee','David','Gasper','Betina','Andres']), 'Age':pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]), 'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])} #Create a DataFrame df = pd.DataFrame(d) df \"\"\" Age Name Rating 0 25 Tom 4.23 1 26 James 3.24 2 25 Ricky 3.98 3 23 Vin 2.56 4 30 Steve 3.20 5 29 Minsu 4.60 6 23 Jack 3.80 7 34 Lee 3.78 8 40 David 2.98 9 30 Gasper 4.80 10 51 Betina 4.10 11 46 Andres 3.65 \"\"\" # 1.sum(),默认axis=0 df.sum() \"\"\" Age 382 Name TomJamesRickyVinSteveMinsuJackLeeDavidGasperBe... Rating 44.92 dtype: object \"\"\" # 2.mean() df.mean() \"\"\" Age 31.833333 Rating 3.743333 dtype: float64 \"\"\" # 3.std() df.std() \"\"\" Age 9.232682 Rating 0.661628 dtype: float64 \"\"\" # 汇总函数 describe() df.describe() \"\"\" Age Rating count 12.000000 12.000000 mean 31.833333 3.743333 std 9.232682 0.661628 min 23.000000 2.560000 25% 25.000000 3.230000 50% 29.500000 3.790000 75% 35.500000 4.132500 max 51.000000 4.800000 \"\"\" 应用函数 要将自定义或其他库的函数应用于Pandas对象，有三个重要的方法，下面来讨论如何使用这些方法。使用适当的方法取决于函数是否期望在整个DataFrame，行或列或元素上进行操作。 表合理函数应用：pipe() 行或列函数应用：apply() 元素函数应用：applymap() # 1.表格函数应用 import pandas as pd import numpy as np def adder(ele1,ele2): return ele1+ele2 df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.pipe(adder,2) \"\"\" col1 col2 col3 0 2.176704 2.219691 1.509360 1 2.222378 2.422167 3.953921 2 2.241096 1.135424 2.696432 3 2.355763 0.376672 1.182570 4 2.308743 2.714767 2.130288 \"\"\" # 2.行列应用函数 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.apply(np.mean) \"\"\" col1 col2 col3 0 0.343569 -1.013287 1.131245 1 0.508922 -0.949778 -1.600569 2 -1.182331 -0.420703 -1.725400 3 0.860265 2.069038 -0.537648 4 0.876758 -0.238051 0.473992 \"\"\" # 3.元素函数应用 import pandas as pd import numpy as np # My custom function df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.applymap(lambda x:x*100) \"\"\" col1 col2 col3 0 17.670426 21.969052 -49.064031 1 22.237846 42.216693 195.392124 2 24.109576 -86.457646 69.643171 3 35.576312 -162.332803 -81.743023 4 30.874333 71.476717 13.028751 \"\"\" 迭代函数 Pandas对象之间的基本迭代的行为取决于类型。当迭代一个系列时，它被视为数组式，基本迭代产生这些值。其他数据结构，如：DataFrame，遵循类似惯例迭代对象的键。 要遍历数据帧(DataFrame)中的行，可以使用以下函数 - iteritems() - 迭代(key，value)对 iterrows() - 将行迭代为(索引，系列)对 itertuples() - 以namedtuples的形式迭代行 # 1.iteritems,迭代的计算列 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns=['col1','col2','col3']) for key,value in df.iteritems(): print (key,value) \"\"\" col1 0 0.802390 1 0.324060 2 0.256811 3 0.839186 Name: col1, dtype: float64 col2 0 1.624313 1 -1.033582 2 1.796663 3 1.856277 Name: col2, dtype: float64 col3 0 -0.022142 1 -0.230820 2 1.160691 3 -0.830279 Name: col3, dtype: float64 \"\"\" # 2.iterrows 迭代计算行 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns = ['col1','col2','col3']) for row_index,row in df.iterrows(): print (row_index,row) \"\"\" 0 col1 1.529759 col2 0.762811 col3 -0.634691 Name: 0, dtype: float64 1 col1 -0.944087 col2 1.420919 col3 -0.507895 Name: 1, dtype: float64 2 col1 -0.077287 col2 -0.858556 col3 -0.663385 Name: 2, dtype: float64 3 col1 -1.638578 col2 0.059866 col3 0.493482 Name: 3, dtype: float64 \"\"\" 字符与文本处理函数 编号 函数 描述 1 lower() 将Series/Index中的字符串转换为小写。 2 upper() 将Series/Index中的字符串转换为大写。 3 len() 计算字符串长度。 4 strip() 帮助从两侧的系列/索引中的每个字符串中删除空格(包括换行符)。 5 split(' ') 用给定的模式拆分每个字符串。 6 cat(sep=' ') 使用给定的分隔符连接系列/索引元素。 7 get_dummies() 返回具有单热编码值的数据帧(DataFrame)。 8 contains(pattern) 如果元素中包含子字符串，则返回每个元素的布尔值True，否则为False。 9 replace(a,b) 将值a替换为值b。 10 repeat","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#统计函数"},{"categories":["python"],"content":" Pandas常用函数 统计函数 编号 函数 描述 1 count() 非空观测数量 2 sum() 所有值之和 3 mean() 所有值的平均值 4 median() 所有值的中位数 5 mode() 值的模值 6 std() 值的标准偏差 7 min() 所有值中的最小值 8 max() 所有值中的最大值 9 abs() 绝对值 10 prod() 数组元素的乘积 11 cumsum() 累计总和 12 cumprod() 累计乘积 13 pct_change() 计算变化百分比 14 conv() 计算协方差 15 corr() 相关性计算 16 rank() 数据排名 import pandas as pd import numpy as np #Create a Dictionary of series d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Minsu','Jack', 'Lee','David','Gasper','Betina','Andres']), 'Age':pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]), 'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])} #Create a DataFrame df = pd.DataFrame(d) df \"\"\" Age Name Rating 0 25 Tom 4.23 1 26 James 3.24 2 25 Ricky 3.98 3 23 Vin 2.56 4 30 Steve 3.20 5 29 Minsu 4.60 6 23 Jack 3.80 7 34 Lee 3.78 8 40 David 2.98 9 30 Gasper 4.80 10 51 Betina 4.10 11 46 Andres 3.65 \"\"\" # 1.sum(),默认axis=0 df.sum() \"\"\" Age 382 Name TomJamesRickyVinSteveMinsuJackLeeDavidGasperBe... Rating 44.92 dtype: object \"\"\" # 2.mean() df.mean() \"\"\" Age 31.833333 Rating 3.743333 dtype: float64 \"\"\" # 3.std() df.std() \"\"\" Age 9.232682 Rating 0.661628 dtype: float64 \"\"\" # 汇总函数 describe() df.describe() \"\"\" Age Rating count 12.000000 12.000000 mean 31.833333 3.743333 std 9.232682 0.661628 min 23.000000 2.560000 25% 25.000000 3.230000 50% 29.500000 3.790000 75% 35.500000 4.132500 max 51.000000 4.800000 \"\"\" 应用函数 要将自定义或其他库的函数应用于Pandas对象，有三个重要的方法，下面来讨论如何使用这些方法。使用适当的方法取决于函数是否期望在整个DataFrame，行或列或元素上进行操作。 表合理函数应用：pipe() 行或列函数应用：apply() 元素函数应用：applymap() # 1.表格函数应用 import pandas as pd import numpy as np def adder(ele1,ele2): return ele1+ele2 df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.pipe(adder,2) \"\"\" col1 col2 col3 0 2.176704 2.219691 1.509360 1 2.222378 2.422167 3.953921 2 2.241096 1.135424 2.696432 3 2.355763 0.376672 1.182570 4 2.308743 2.714767 2.130288 \"\"\" # 2.行列应用函数 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.apply(np.mean) \"\"\" col1 col2 col3 0 0.343569 -1.013287 1.131245 1 0.508922 -0.949778 -1.600569 2 -1.182331 -0.420703 -1.725400 3 0.860265 2.069038 -0.537648 4 0.876758 -0.238051 0.473992 \"\"\" # 3.元素函数应用 import pandas as pd import numpy as np # My custom function df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.applymap(lambda x:x*100) \"\"\" col1 col2 col3 0 17.670426 21.969052 -49.064031 1 22.237846 42.216693 195.392124 2 24.109576 -86.457646 69.643171 3 35.576312 -162.332803 -81.743023 4 30.874333 71.476717 13.028751 \"\"\" 迭代函数 Pandas对象之间的基本迭代的行为取决于类型。当迭代一个系列时，它被视为数组式，基本迭代产生这些值。其他数据结构，如：DataFrame，遵循类似惯例迭代对象的键。 要遍历数据帧(DataFrame)中的行，可以使用以下函数 - iteritems() - 迭代(key，value)对 iterrows() - 将行迭代为(索引，系列)对 itertuples() - 以namedtuples的形式迭代行 # 1.iteritems,迭代的计算列 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns=['col1','col2','col3']) for key,value in df.iteritems(): print (key,value) \"\"\" col1 0 0.802390 1 0.324060 2 0.256811 3 0.839186 Name: col1, dtype: float64 col2 0 1.624313 1 -1.033582 2 1.796663 3 1.856277 Name: col2, dtype: float64 col3 0 -0.022142 1 -0.230820 2 1.160691 3 -0.830279 Name: col3, dtype: float64 \"\"\" # 2.iterrows 迭代计算行 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns = ['col1','col2','col3']) for row_index,row in df.iterrows(): print (row_index,row) \"\"\" 0 col1 1.529759 col2 0.762811 col3 -0.634691 Name: 0, dtype: float64 1 col1 -0.944087 col2 1.420919 col3 -0.507895 Name: 1, dtype: float64 2 col1 -0.077287 col2 -0.858556 col3 -0.663385 Name: 2, dtype: float64 3 col1 -1.638578 col2 0.059866 col3 0.493482 Name: 3, dtype: float64 \"\"\" 字符与文本处理函数 编号 函数 描述 1 lower() 将Series/Index中的字符串转换为小写。 2 upper() 将Series/Index中的字符串转换为大写。 3 len() 计算字符串长度。 4 strip() 帮助从两侧的系列/索引中的每个字符串中删除空格(包括换行符)。 5 split(' ') 用给定的模式拆分每个字符串。 6 cat(sep=' ') 使用给定的分隔符连接系列/索引元素。 7 get_dummies() 返回具有单热编码值的数据帧(DataFrame)。 8 contains(pattern) 如果元素中包含子字符串，则返回每个元素的布尔值True，否则为False。 9 replace(a,b) 将值a替换为值b。 10 repeat","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#应用函数"},{"categories":["python"],"content":" Pandas常用函数 统计函数 编号 函数 描述 1 count() 非空观测数量 2 sum() 所有值之和 3 mean() 所有值的平均值 4 median() 所有值的中位数 5 mode() 值的模值 6 std() 值的标准偏差 7 min() 所有值中的最小值 8 max() 所有值中的最大值 9 abs() 绝对值 10 prod() 数组元素的乘积 11 cumsum() 累计总和 12 cumprod() 累计乘积 13 pct_change() 计算变化百分比 14 conv() 计算协方差 15 corr() 相关性计算 16 rank() 数据排名 import pandas as pd import numpy as np #Create a Dictionary of series d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Minsu','Jack', 'Lee','David','Gasper','Betina','Andres']), 'Age':pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]), 'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])} #Create a DataFrame df = pd.DataFrame(d) df \"\"\" Age Name Rating 0 25 Tom 4.23 1 26 James 3.24 2 25 Ricky 3.98 3 23 Vin 2.56 4 30 Steve 3.20 5 29 Minsu 4.60 6 23 Jack 3.80 7 34 Lee 3.78 8 40 David 2.98 9 30 Gasper 4.80 10 51 Betina 4.10 11 46 Andres 3.65 \"\"\" # 1.sum(),默认axis=0 df.sum() \"\"\" Age 382 Name TomJamesRickyVinSteveMinsuJackLeeDavidGasperBe... Rating 44.92 dtype: object \"\"\" # 2.mean() df.mean() \"\"\" Age 31.833333 Rating 3.743333 dtype: float64 \"\"\" # 3.std() df.std() \"\"\" Age 9.232682 Rating 0.661628 dtype: float64 \"\"\" # 汇总函数 describe() df.describe() \"\"\" Age Rating count 12.000000 12.000000 mean 31.833333 3.743333 std 9.232682 0.661628 min 23.000000 2.560000 25% 25.000000 3.230000 50% 29.500000 3.790000 75% 35.500000 4.132500 max 51.000000 4.800000 \"\"\" 应用函数 要将自定义或其他库的函数应用于Pandas对象，有三个重要的方法，下面来讨论如何使用这些方法。使用适当的方法取决于函数是否期望在整个DataFrame，行或列或元素上进行操作。 表合理函数应用：pipe() 行或列函数应用：apply() 元素函数应用：applymap() # 1.表格函数应用 import pandas as pd import numpy as np def adder(ele1,ele2): return ele1+ele2 df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.pipe(adder,2) \"\"\" col1 col2 col3 0 2.176704 2.219691 1.509360 1 2.222378 2.422167 3.953921 2 2.241096 1.135424 2.696432 3 2.355763 0.376672 1.182570 4 2.308743 2.714767 2.130288 \"\"\" # 2.行列应用函数 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.apply(np.mean) \"\"\" col1 col2 col3 0 0.343569 -1.013287 1.131245 1 0.508922 -0.949778 -1.600569 2 -1.182331 -0.420703 -1.725400 3 0.860265 2.069038 -0.537648 4 0.876758 -0.238051 0.473992 \"\"\" # 3.元素函数应用 import pandas as pd import numpy as np # My custom function df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.applymap(lambda x:x*100) \"\"\" col1 col2 col3 0 17.670426 21.969052 -49.064031 1 22.237846 42.216693 195.392124 2 24.109576 -86.457646 69.643171 3 35.576312 -162.332803 -81.743023 4 30.874333 71.476717 13.028751 \"\"\" 迭代函数 Pandas对象之间的基本迭代的行为取决于类型。当迭代一个系列时，它被视为数组式，基本迭代产生这些值。其他数据结构，如：DataFrame，遵循类似惯例迭代对象的键。 要遍历数据帧(DataFrame)中的行，可以使用以下函数 - iteritems() - 迭代(key，value)对 iterrows() - 将行迭代为(索引，系列)对 itertuples() - 以namedtuples的形式迭代行 # 1.iteritems,迭代的计算列 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns=['col1','col2','col3']) for key,value in df.iteritems(): print (key,value) \"\"\" col1 0 0.802390 1 0.324060 2 0.256811 3 0.839186 Name: col1, dtype: float64 col2 0 1.624313 1 -1.033582 2 1.796663 3 1.856277 Name: col2, dtype: float64 col3 0 -0.022142 1 -0.230820 2 1.160691 3 -0.830279 Name: col3, dtype: float64 \"\"\" # 2.iterrows 迭代计算行 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns = ['col1','col2','col3']) for row_index,row in df.iterrows(): print (row_index,row) \"\"\" 0 col1 1.529759 col2 0.762811 col3 -0.634691 Name: 0, dtype: float64 1 col1 -0.944087 col2 1.420919 col3 -0.507895 Name: 1, dtype: float64 2 col1 -0.077287 col2 -0.858556 col3 -0.663385 Name: 2, dtype: float64 3 col1 -1.638578 col2 0.059866 col3 0.493482 Name: 3, dtype: float64 \"\"\" 字符与文本处理函数 编号 函数 描述 1 lower() 将Series/Index中的字符串转换为小写。 2 upper() 将Series/Index中的字符串转换为大写。 3 len() 计算字符串长度。 4 strip() 帮助从两侧的系列/索引中的每个字符串中删除空格(包括换行符)。 5 split(' ') 用给定的模式拆分每个字符串。 6 cat(sep=' ') 使用给定的分隔符连接系列/索引元素。 7 get_dummies() 返回具有单热编码值的数据帧(DataFrame)。 8 contains(pattern) 如果元素中包含子字符串，则返回每个元素的布尔值True，否则为False。 9 replace(a,b) 将值a替换为值b。 10 repeat","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#迭代函数"},{"categories":["python"],"content":" Pandas常用函数 统计函数 编号 函数 描述 1 count() 非空观测数量 2 sum() 所有值之和 3 mean() 所有值的平均值 4 median() 所有值的中位数 5 mode() 值的模值 6 std() 值的标准偏差 7 min() 所有值中的最小值 8 max() 所有值中的最大值 9 abs() 绝对值 10 prod() 数组元素的乘积 11 cumsum() 累计总和 12 cumprod() 累计乘积 13 pct_change() 计算变化百分比 14 conv() 计算协方差 15 corr() 相关性计算 16 rank() 数据排名 import pandas as pd import numpy as np #Create a Dictionary of series d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Minsu','Jack', 'Lee','David','Gasper','Betina','Andres']), 'Age':pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]), 'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])} #Create a DataFrame df = pd.DataFrame(d) df \"\"\" Age Name Rating 0 25 Tom 4.23 1 26 James 3.24 2 25 Ricky 3.98 3 23 Vin 2.56 4 30 Steve 3.20 5 29 Minsu 4.60 6 23 Jack 3.80 7 34 Lee 3.78 8 40 David 2.98 9 30 Gasper 4.80 10 51 Betina 4.10 11 46 Andres 3.65 \"\"\" # 1.sum(),默认axis=0 df.sum() \"\"\" Age 382 Name TomJamesRickyVinSteveMinsuJackLeeDavidGasperBe... Rating 44.92 dtype: object \"\"\" # 2.mean() df.mean() \"\"\" Age 31.833333 Rating 3.743333 dtype: float64 \"\"\" # 3.std() df.std() \"\"\" Age 9.232682 Rating 0.661628 dtype: float64 \"\"\" # 汇总函数 describe() df.describe() \"\"\" Age Rating count 12.000000 12.000000 mean 31.833333 3.743333 std 9.232682 0.661628 min 23.000000 2.560000 25% 25.000000 3.230000 50% 29.500000 3.790000 75% 35.500000 4.132500 max 51.000000 4.800000 \"\"\" 应用函数 要将自定义或其他库的函数应用于Pandas对象，有三个重要的方法，下面来讨论如何使用这些方法。使用适当的方法取决于函数是否期望在整个DataFrame，行或列或元素上进行操作。 表合理函数应用：pipe() 行或列函数应用：apply() 元素函数应用：applymap() # 1.表格函数应用 import pandas as pd import numpy as np def adder(ele1,ele2): return ele1+ele2 df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.pipe(adder,2) \"\"\" col1 col2 col3 0 2.176704 2.219691 1.509360 1 2.222378 2.422167 3.953921 2 2.241096 1.135424 2.696432 3 2.355763 0.376672 1.182570 4 2.308743 2.714767 2.130288 \"\"\" # 2.行列应用函数 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.apply(np.mean) \"\"\" col1 col2 col3 0 0.343569 -1.013287 1.131245 1 0.508922 -0.949778 -1.600569 2 -1.182331 -0.420703 -1.725400 3 0.860265 2.069038 -0.537648 4 0.876758 -0.238051 0.473992 \"\"\" # 3.元素函数应用 import pandas as pd import numpy as np # My custom function df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.applymap(lambda x:x*100) \"\"\" col1 col2 col3 0 17.670426 21.969052 -49.064031 1 22.237846 42.216693 195.392124 2 24.109576 -86.457646 69.643171 3 35.576312 -162.332803 -81.743023 4 30.874333 71.476717 13.028751 \"\"\" 迭代函数 Pandas对象之间的基本迭代的行为取决于类型。当迭代一个系列时，它被视为数组式，基本迭代产生这些值。其他数据结构，如：DataFrame，遵循类似惯例迭代对象的键。 要遍历数据帧(DataFrame)中的行，可以使用以下函数 - iteritems() - 迭代(key，value)对 iterrows() - 将行迭代为(索引，系列)对 itertuples() - 以namedtuples的形式迭代行 # 1.iteritems,迭代的计算列 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns=['col1','col2','col3']) for key,value in df.iteritems(): print (key,value) \"\"\" col1 0 0.802390 1 0.324060 2 0.256811 3 0.839186 Name: col1, dtype: float64 col2 0 1.624313 1 -1.033582 2 1.796663 3 1.856277 Name: col2, dtype: float64 col3 0 -0.022142 1 -0.230820 2 1.160691 3 -0.830279 Name: col3, dtype: float64 \"\"\" # 2.iterrows 迭代计算行 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns = ['col1','col2','col3']) for row_index,row in df.iterrows(): print (row_index,row) \"\"\" 0 col1 1.529759 col2 0.762811 col3 -0.634691 Name: 0, dtype: float64 1 col1 -0.944087 col2 1.420919 col3 -0.507895 Name: 1, dtype: float64 2 col1 -0.077287 col2 -0.858556 col3 -0.663385 Name: 2, dtype: float64 3 col1 -1.638578 col2 0.059866 col3 0.493482 Name: 3, dtype: float64 \"\"\" 字符与文本处理函数 编号 函数 描述 1 lower() 将Series/Index中的字符串转换为小写。 2 upper() 将Series/Index中的字符串转换为大写。 3 len() 计算字符串长度。 4 strip() 帮助从两侧的系列/索引中的每个字符串中删除空格(包括换行符)。 5 split(' ') 用给定的模式拆分每个字符串。 6 cat(sep=' ') 使用给定的分隔符连接系列/索引元素。 7 get_dummies() 返回具有单热编码值的数据帧(DataFrame)。 8 contains(pattern) 如果元素中包含子字符串，则返回每个元素的布尔值True，否则为False。 9 replace(a,b) 将值a替换为值b。 10 repeat","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#字符与文本处理函数"},{"categories":["python"],"content":" Pandas常用函数 统计函数 编号 函数 描述 1 count() 非空观测数量 2 sum() 所有值之和 3 mean() 所有值的平均值 4 median() 所有值的中位数 5 mode() 值的模值 6 std() 值的标准偏差 7 min() 所有值中的最小值 8 max() 所有值中的最大值 9 abs() 绝对值 10 prod() 数组元素的乘积 11 cumsum() 累计总和 12 cumprod() 累计乘积 13 pct_change() 计算变化百分比 14 conv() 计算协方差 15 corr() 相关性计算 16 rank() 数据排名 import pandas as pd import numpy as np #Create a Dictionary of series d = {'Name':pd.Series(['Tom','James','Ricky','Vin','Steve','Minsu','Jack', 'Lee','David','Gasper','Betina','Andres']), 'Age':pd.Series([25,26,25,23,30,29,23,34,40,30,51,46]), 'Rating':pd.Series([4.23,3.24,3.98,2.56,3.20,4.6,3.8,3.78,2.98,4.80,4.10,3.65])} #Create a DataFrame df = pd.DataFrame(d) df \"\"\" Age Name Rating 0 25 Tom 4.23 1 26 James 3.24 2 25 Ricky 3.98 3 23 Vin 2.56 4 30 Steve 3.20 5 29 Minsu 4.60 6 23 Jack 3.80 7 34 Lee 3.78 8 40 David 2.98 9 30 Gasper 4.80 10 51 Betina 4.10 11 46 Andres 3.65 \"\"\" # 1.sum(),默认axis=0 df.sum() \"\"\" Age 382 Name TomJamesRickyVinSteveMinsuJackLeeDavidGasperBe... Rating 44.92 dtype: object \"\"\" # 2.mean() df.mean() \"\"\" Age 31.833333 Rating 3.743333 dtype: float64 \"\"\" # 3.std() df.std() \"\"\" Age 9.232682 Rating 0.661628 dtype: float64 \"\"\" # 汇总函数 describe() df.describe() \"\"\" Age Rating count 12.000000 12.000000 mean 31.833333 3.743333 std 9.232682 0.661628 min 23.000000 2.560000 25% 25.000000 3.230000 50% 29.500000 3.790000 75% 35.500000 4.132500 max 51.000000 4.800000 \"\"\" 应用函数 要将自定义或其他库的函数应用于Pandas对象，有三个重要的方法，下面来讨论如何使用这些方法。使用适当的方法取决于函数是否期望在整个DataFrame，行或列或元素上进行操作。 表合理函数应用：pipe() 行或列函数应用：apply() 元素函数应用：applymap() # 1.表格函数应用 import pandas as pd import numpy as np def adder(ele1,ele2): return ele1+ele2 df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.pipe(adder,2) \"\"\" col1 col2 col3 0 2.176704 2.219691 1.509360 1 2.222378 2.422167 3.953921 2 2.241096 1.135424 2.696432 3 2.355763 0.376672 1.182570 4 2.308743 2.714767 2.130288 \"\"\" # 2.行列应用函数 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.apply(np.mean) \"\"\" col1 col2 col3 0 0.343569 -1.013287 1.131245 1 0.508922 -0.949778 -1.600569 2 -1.182331 -0.420703 -1.725400 3 0.860265 2.069038 -0.537648 4 0.876758 -0.238051 0.473992 \"\"\" # 3.元素函数应用 import pandas as pd import numpy as np # My custom function df = pd.DataFrame(np.random.randn(5,3),columns=['col1','col2','col3']) df.applymap(lambda x:x*100) \"\"\" col1 col2 col3 0 17.670426 21.969052 -49.064031 1 22.237846 42.216693 195.392124 2 24.109576 -86.457646 69.643171 3 35.576312 -162.332803 -81.743023 4 30.874333 71.476717 13.028751 \"\"\" 迭代函数 Pandas对象之间的基本迭代的行为取决于类型。当迭代一个系列时，它被视为数组式，基本迭代产生这些值。其他数据结构，如：DataFrame，遵循类似惯例迭代对象的键。 要遍历数据帧(DataFrame)中的行，可以使用以下函数 - iteritems() - 迭代(key，value)对 iterrows() - 将行迭代为(索引，系列)对 itertuples() - 以namedtuples的形式迭代行 # 1.iteritems,迭代的计算列 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns=['col1','col2','col3']) for key,value in df.iteritems(): print (key,value) \"\"\" col1 0 0.802390 1 0.324060 2 0.256811 3 0.839186 Name: col1, dtype: float64 col2 0 1.624313 1 -1.033582 2 1.796663 3 1.856277 Name: col2, dtype: float64 col3 0 -0.022142 1 -0.230820 2 1.160691 3 -0.830279 Name: col3, dtype: float64 \"\"\" # 2.iterrows 迭代计算行 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(4,3),columns = ['col1','col2','col3']) for row_index,row in df.iterrows(): print (row_index,row) \"\"\" 0 col1 1.529759 col2 0.762811 col3 -0.634691 Name: 0, dtype: float64 1 col1 -0.944087 col2 1.420919 col3 -0.507895 Name: 1, dtype: float64 2 col1 -0.077287 col2 -0.858556 col3 -0.663385 Name: 2, dtype: float64 3 col1 -1.638578 col2 0.059866 col3 0.493482 Name: 3, dtype: float64 \"\"\" 字符与文本处理函数 编号 函数 描述 1 lower() 将Series/Index中的字符串转换为小写。 2 upper() 将Series/Index中的字符串转换为大写。 3 len() 计算字符串长度。 4 strip() 帮助从两侧的系列/索引中的每个字符串中删除空格(包括换行符)。 5 split(' ') 用给定的模式拆分每个字符串。 6 cat(sep=' ') 使用给定的分隔符连接系列/索引元素。 7 get_dummies() 返回具有单热编码值的数据帧(DataFrame)。 8 contains(pattern) 如果元素中包含子字符串，则返回每个元素的布尔值True，否则为False。 9 replace(a,b) 将值a替换为值b。 10 repeat","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#窗口函数"},{"categories":["python"],"content":" Pandas索引与选择 Python和NumPy索引运算符\"[]\"和属性运算符\".\"。 可以在广泛的用例中快速轻松地访问Pandas数据结构。然而，由于要访问的数据类型不是预先知道的，所以直接使用标准运算符具有一些优化限制。 *Pandas*现在支持三种类型的多轴索引： 编号 索引 描述 1 .loc() 基于标签 2 .iloc() 基于整数 3 .ix() 基于标签和整数 # 1 .loc() # loc需要两个单/列表/范围运算符，用\",\"分隔。第一个表示行，第二个表示列。 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(8, 4), index = ['a','b','c','d','e','f','g','h'], columns = ['A', 'B', 'C', 'D']) df.loc[:,['A','C']] \"\"\" A C a -0.529735 -1.067299 b -2.230089 -1.798575 c 0.685852 0.333387 d 1.061853 0.131853 e 0.990459 0.189966 f 0.057314 -0.370055 g 0.453960 -0.624419 h 0.666668 -0.433971 \"\"\" # 注意，loc行参数是index df.loc[['a','b','f','h'],['A','C']] \"\"\" A C a -1.959731 0.720956 b 1.318976 0.199987 f -1.117735 -0.181116 h -0.147029 0.027369 \"\"\" df.loc['a'] \u003e 0 \"\"\" A False B True C False D True Name: a, dtype: bool \"\"\" # 2 .iloc() # iloc使用纯整数索引 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(8, 4), columns = ['A', 'B', 'C', 'D']) df.iloc[:4] \"\"\" A B C D 0 0.277146 0.274234 0.860555 -1.312323 1 -1.064776 2.082030 0.695930 2.409340 2 0.033953 -1.155217 0.113045 -0.028330 3 0.241075 -2.156415 0.939586 -1.670171 \"\"\" df.iloc[1:5, 2:4] \"\"\" C D 1 0.893615 0.659946 2 0.869331 -1.443731 3 -0.483688 -1.167312 4 1.566395 -1.292206 \"\"\" df.iloc[1:3, :] \"\"\" A B C D 1 -0.133711 0.081257 -0.031869 0.009109 2 0.895576 -0.513450 -0.048573 0.698965 \"\"\" # 3 .ix() # ix可以使用标签和整数的混合运算 import pandas as pd import numpy as np df = pd.DataFrame(np.random.randn(8, 4), columns = ['A', 'B', 'C', 'D']) df.ix[:,'A'] \"\"\" 0 1.539915 1 1.359477 2 0.239694 3 0.563254 4 2.123950 5 0.341554 6 -0.075717 7 -0.606742 Name: A, dtype: float64 \"\"\" ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:3","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas索引与选择"},{"categories":["python"],"content":" Pandas分组聚合 任何分组(groupby)操作都涉及原始对象的以下操作之一。它们是 ： 分割对象 应用一个函数 结合的结果 在许多情况下，我们将数据分成多个集合，并在每个子集上应用一些函数。在应用函数中，可以执行以下操作 ： 聚合 - 计算汇总统计 转换 - 执行一些特定于组的操作 过滤 - 在某些情况下丢弃数据 Pandas对象可以分成任何对象。有多种方式来拆分对象 obj.groupby(‘key’) obj.groupby([‘key1’,’key2’]) obj.groupby(key,axis=1) # 首先生成一个DataFrame import pandas as pd ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings', 'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'], 'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2], 'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017], 'Points':[876,789,863,673,741,812,756,788,694,701,804,690]} df = pd.DataFrame(ipl_data) df \"\"\" Points Rank Team Year 0 876 1 Riders 2014 1 789 2 Riders 2015 2 863 2 Devils 2014 3 673 3 Devils 2015 4 741 3 Kings 2014 5 812 4 kings 2015 6 756 1 Kings 2016 7 788 1 Kings 2017 8 694 2 Riders 2016 9 701 4 Royals 2014 10 804 1 Royals 2015 11 690 2 Riders 2017 \"\"\" # 1.分组 df.groupby('Team') # \u003cpandas.core.groupby.DataFrameGroupBy object at 0x00000245D60AD518\u003e # 2.分组内容 df.groupby('Team').groups \"\"\" { 'Devils': Int64Index([2, 3], dtype='int64'), 'Kings': Int64Index([4, 6, 7], dtype='int64'), 'Riders': Int64Index([0, 1, 8, 11], dtype='int64'), 'Royals': Int64Index([9, 10], dtype='int64'), 'kings': Int64Index([5], dtype='int64') } \"\"\" # 3.多键分组 df.groupby(['Team','Year']).groups \"\"\" { ('Devils', 2014): Int64Index([2], dtype='int64'), ('Devils', 2015): Int64Index([3], dtype='int64'), ('Kings', 2014): Int64Index([4], dtype='int64'), ('Kings', 2016): Int64Index([6], dtype='int64'), ('Kings', 2017): Int64Index([7], dtype='int64'), ('Riders', 2014): Int64Index([0], dtype='int64'), ('Riders', 2015): Int64Index([1], dtype='int64'), ('Riders', 2016): Int64Index([8], dtype='int64'), ('Riders', 2017): Int64Index([11], dtype='int64'), ('Royals', 2014): Int64Index([9], dtype='int64'), ('Royals', 2015): Int64Index([10], dtype='int64'), ('kings', 2015): Int64Index([5], dtype='int64') } \"\"\" # 4.迭代输出分组 grouped = df.groupby('Year') for name,group in grouped: print (name) print (group) \"\"\" 2014 Points Rank Team Year 0 876 1 Riders 2014 2 863 2 Devils 2014 4 741 3 Kings 2014 9 701 4 Royals 2014 2015 Points Rank Team Year 1 789 2 Riders 2015 3 673 3 Devils 2015 5 812 4 kings 2015 10 804 1 Royals 2015 2016 Points Rank Team Year 6 756 1 Kings 2016 8 694 2 Riders 2016 2017 Points Rank Team Year 7 788 1 Kings 2017 11 690 2 Riders 2017 \"\"\" # 5.获取分组 grouped = df.groupby(\"Year\") grouped.get_group(2014) \"\"\" Points Rank Team Year 0 876 1 Riders 2014 2 863 2 Devils 2014 4 741 3 Kings 2014 9 701 4 Royals 2014 \"\"\" 聚合函数为每个组返回单个聚合值。当创建了分组(group by)对象，就可以对分组数据执行多个聚合操作。 一个常用的方法是agg 分组或列上的转换返回索引大小与被分组的索引相同的对象。因此，转换应该返回与组块大小相同的结果。 转换的方法是transform 过滤根据定义的标准过滤数据并返回数据的子集。 过滤用的方法是filter # 首先生成一个DataFrame import pandas as pd ipl_data = {'Team': ['Riders', 'Riders', 'Devils', 'Devils', 'Kings', 'kings', 'Kings', 'Kings', 'Riders', 'Royals', 'Royals', 'Riders'], 'Rank': [1, 2, 2, 3, 3,4 ,1 ,1,2 , 4,1,2], 'Year': [2014,2015,2014,2015,2014,2015,2016,2017,2016,2014,2015,2017], 'Points':[876,789,863,673,741,812,756,788,694,701,804,690]} df = pd.DataFrame(ipl_data) # 1.聚合函数 grouped['Points'].agg(np.mean) \"\"\" Year 2014 795.25 2015 769.50 2016 725.00 2017 739.00 Name: Points, dtype: float64 \"\"\" # 2.多个聚合函数 grouped = df.groupby('Team') agg = grouped['Points'].agg([np.sum, np.mean, np.std]) \"\"\" sum mean std Team Devils 1536 768.000000 134.350288 Kings 2285 761.666667 24.006943 Riders 3049 762.250000 88.567771 Royals 1505 752.500000 72.831998 kings 812 812.000000 NaN \"\"\" # 3.转换函数 grouped = df.groupby('Team') score = lambda x: (x - x.mean()) / x.std()*10 grouped.transform(score) \"\"\" Points Rank Year 0 12.843272 -15.000000 -11.618950 1 3.020286 5.000000 -3.872983 2 7.071068 -7.071068 -7.071068 3 -7.071068 7.071068 7.071068 4 -8.608621 11.547005 -10.910895 5 NaN NaN NaN 6 -2.360428 -5.773503 2.182179 7 10.969049 -5.773503 8.728716 8 -7.705963 5.000000 3.872983 9 -7.071068 7.071068 -7.071068 10 7.071068 -7.071068 7.071068 11 -8.157595 5.000000 11.618950 \"\"\" ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:4","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas分组聚合"},{"categories":["python"],"content":" Pandas合并与级联 Pandas合并 Pandas提供了一个单独的merge()函数来进行连接。 pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True)1 left - 一个DataFrame对象。 right - 另一个DataFrame对象。 on - 列(名称)连接，必须在左和右DataFrame对象中存在(找到)。 left_on - 左侧DataFrame中的列用作键，可以是列名或长度等于DataFrame长度的数组。 right_on - 来自右的DataFrame的列作为键，可以是列名或长度等于DataFrame长度的数组。 left_index - 如果为True，则使用左侧DataFrame中的索引(行标签)作为其连接键。 在具有MultiIndex(分层)的DataFrame的情况下，级别的数量必须与来自右DataFrame的连接键的数量相匹配。 right_index - 与右DataFrame的left_index具有相同的用法。 how - 它是left, right, outer以及inner之中的一个，默认为内inner。 下面将介绍每种方法的用法。 sort - 按照字典顺序通过连接键对结果DataFrame进行排序。默认为True，设置为False时，在很多情况下大大提高性能。 # 连接实例 import pandas as pd left = pd.DataFrame({ 'id':[1,2,3,4,5], 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5']}) right = pd.DataFrame( {'id':[1,2,3,4,5], 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5']}) print (left) print(\"========================================\") print (right) \"\"\" Name id subject_id 0 Alex 1 sub1 1 Amy 2 sub2 2 Allen 3 sub4 3 Alice 4 sub6 4 Ayoung 5 sub5 ======================================== Name id subject_id 0 Billy 1 sub2 1 Brian 2 sub4 2 Bran 3 sub3 3 Bryce 4 sub6 4 Betty 5 sub5 \"\"\" rs = pd.merge(left,right,on='id') print(rs) \"\"\" Name_x id subject_id_x Name_y subject_id_y 0 Alex 1 sub1 Billy sub2 1 Amy 2 sub2 Brian sub4 2 Allen 3 sub4 Bran sub3 3 Alice 4 sub6 Bryce sub6 4 Ayoung 5 sub5 Betty sub5 \"\"\" how参数与sql: 合并方法 SQL等效 描述 left LEFT OUTER JOIN 使用左侧对象的键 right RIGHT OUTER JOIN 使用右侧对象的键 outer FULL OUTER JOIN 使用键的联合 inner INNER JOIN 使用键的交集 rs = pd.merge(left, right, on='subject_id', how='left') \"\"\" Name_x id_x subject_id Name_y id_y 0 Alex 1 sub1 NaN NaN 1 Amy 2 sub2 Billy 1.0 2 Allen 3 sub4 Brian 2.0 3 Alice 4 sub6 Bryce 4.0 4 Ayoung 5 sub5 Betty 5.0 \"\"\" rs = pd.merge(left, right, on='subject_id', how='right') \"\"\" Name_x id_x subject_id Name_y id_y 0 Amy 2.0 sub2 Billy 1 1 Allen 3.0 sub4 Brian 2 2 Alice 4.0 sub6 Bryce 4 3 Ayoung 5.0 sub5 Betty 5 4 NaN NaN sub3 Bran 3 \"\"\" Pandas级联 pd.concat(objs,axis=0,join='outer',join_axes=None,ignore_index=False) objs - 这是Series，DataFrame或Panel对象的序列或映射。 axis - {0，1，...}，默认为0，这是连接的轴。 join - {'inner', 'outer'}，默认inner。如何处理其他轴上的索引。联合的外部和交叉的内部。 ignore_index − 布尔值，默认为False。如果指定为True，则不要使用连接轴上的索引值。结果轴将被标记为：0，...，n-1。 join_axes - 这是Index对象的列表。用于其他(n-1)轴的特定索引，而不是执行内部/外部集逻辑。 import pandas as pd one = pd.DataFrame({ 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5'], 'Marks_scored':[98,90,87,69,78]}, index=[1,2,3,4,5]) two = pd.DataFrame({ 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5'], 'Marks_scored':[89,80,79,97,88]}, index=[1,2,3,4,5]) rs = pd.concat([one,two]) print(rs) \"\"\" Marks_scored Name subject_id 1 98 Alex sub1 2 90 Amy sub2 3 87 Allen sub4 4 69 Alice sub6 5 78 Ayoung sub5 1 89 Billy sub2 2 80 Brian sub4 3 79 Bran sub3 4 97 Bryce sub6 5 88 Betty sub5 \"\"\" rs = pd.concat([one,two],keys=['x','y'],ignore_index=True) print(rs) \"\"\" Marks_scored Name subject_id 0 98 Alex sub1 1 90 Amy sub2 2 87 Allen sub4 3 69 Alice sub6 4 78 Ayoung sub5 5 89 Billy sub2 6 80 Brian sub4 7 79 Bran sub3 8 97 Bryce sub6 9 88 Betty sub5 \"\"\" # 附加连接 append import pandas as pd one = pd.DataFrame({ 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5'], 'Marks_scored':[98,90,87,69,78]}, index=[1,2,3,4,5]) two = pd.DataFrame({ 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5'], 'Marks_scored':[89,80,79,97,88]}, index=[1,2,3,4,5]) rs = one.append(two) print(rs) \"\"\" Marks_scored Name subject_id 1 98 Alex sub1 2 90 Amy sub2 3 87 Allen sub4 4 69 Alice sub6 5 78 Ayoung sub5 1 89 Billy sub2 2 80 Brian sub4 3 79 Bran sub3 4 97 Bryce sub6 5 88 Betty sub5 \"\"\" ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:5","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas合并与级联"},{"categories":["python"],"content":" Pandas合并与级联 Pandas合并 Pandas提供了一个单独的merge()函数来进行连接。 pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True)1 left - 一个DataFrame对象。 right - 另一个DataFrame对象。 on - 列(名称)连接，必须在左和右DataFrame对象中存在(找到)。 left_on - 左侧DataFrame中的列用作键，可以是列名或长度等于DataFrame长度的数组。 right_on - 来自右的DataFrame的列作为键，可以是列名或长度等于DataFrame长度的数组。 left_index - 如果为True，则使用左侧DataFrame中的索引(行标签)作为其连接键。 在具有MultiIndex(分层)的DataFrame的情况下，级别的数量必须与来自右DataFrame的连接键的数量相匹配。 right_index - 与右DataFrame的left_index具有相同的用法。 how - 它是left, right, outer以及inner之中的一个，默认为内inner。 下面将介绍每种方法的用法。 sort - 按照字典顺序通过连接键对结果DataFrame进行排序。默认为True，设置为False时，在很多情况下大大提高性能。 # 连接实例 import pandas as pd left = pd.DataFrame({ 'id':[1,2,3,4,5], 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5']}) right = pd.DataFrame( {'id':[1,2,3,4,5], 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5']}) print (left) print(\"========================================\") print (right) \"\"\" Name id subject_id 0 Alex 1 sub1 1 Amy 2 sub2 2 Allen 3 sub4 3 Alice 4 sub6 4 Ayoung 5 sub5 ======================================== Name id subject_id 0 Billy 1 sub2 1 Brian 2 sub4 2 Bran 3 sub3 3 Bryce 4 sub6 4 Betty 5 sub5 \"\"\" rs = pd.merge(left,right,on='id') print(rs) \"\"\" Name_x id subject_id_x Name_y subject_id_y 0 Alex 1 sub1 Billy sub2 1 Amy 2 sub2 Brian sub4 2 Allen 3 sub4 Bran sub3 3 Alice 4 sub6 Bryce sub6 4 Ayoung 5 sub5 Betty sub5 \"\"\" how参数与sql: 合并方法 SQL等效 描述 left LEFT OUTER JOIN 使用左侧对象的键 right RIGHT OUTER JOIN 使用右侧对象的键 outer FULL OUTER JOIN 使用键的联合 inner INNER JOIN 使用键的交集 rs = pd.merge(left, right, on='subject_id', how='left') \"\"\" Name_x id_x subject_id Name_y id_y 0 Alex 1 sub1 NaN NaN 1 Amy 2 sub2 Billy 1.0 2 Allen 3 sub4 Brian 2.0 3 Alice 4 sub6 Bryce 4.0 4 Ayoung 5 sub5 Betty 5.0 \"\"\" rs = pd.merge(left, right, on='subject_id', how='right') \"\"\" Name_x id_x subject_id Name_y id_y 0 Amy 2.0 sub2 Billy 1 1 Allen 3.0 sub4 Brian 2 2 Alice 4.0 sub6 Bryce 4 3 Ayoung 5.0 sub5 Betty 5 4 NaN NaN sub3 Bran 3 \"\"\" Pandas级联 pd.concat(objs,axis=0,join='outer',join_axes=None,ignore_index=False) objs - 这是Series，DataFrame或Panel对象的序列或映射。 axis - {0，1，...}，默认为0，这是连接的轴。 join - {'inner', 'outer'}，默认inner。如何处理其他轴上的索引。联合的外部和交叉的内部。 ignore_index − 布尔值，默认为False。如果指定为True，则不要使用连接轴上的索引值。结果轴将被标记为：0，...，n-1。 join_axes - 这是Index对象的列表。用于其他(n-1)轴的特定索引，而不是执行内部/外部集逻辑。 import pandas as pd one = pd.DataFrame({ 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5'], 'Marks_scored':[98,90,87,69,78]}, index=[1,2,3,4,5]) two = pd.DataFrame({ 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5'], 'Marks_scored':[89,80,79,97,88]}, index=[1,2,3,4,5]) rs = pd.concat([one,two]) print(rs) \"\"\" Marks_scored Name subject_id 1 98 Alex sub1 2 90 Amy sub2 3 87 Allen sub4 4 69 Alice sub6 5 78 Ayoung sub5 1 89 Billy sub2 2 80 Brian sub4 3 79 Bran sub3 4 97 Bryce sub6 5 88 Betty sub5 \"\"\" rs = pd.concat([one,two],keys=['x','y'],ignore_index=True) print(rs) \"\"\" Marks_scored Name subject_id 0 98 Alex sub1 1 90 Amy sub2 2 87 Allen sub4 3 69 Alice sub6 4 78 Ayoung sub5 5 89 Billy sub2 6 80 Brian sub4 7 79 Bran sub3 8 97 Bryce sub6 9 88 Betty sub5 \"\"\" # 附加连接 append import pandas as pd one = pd.DataFrame({ 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5'], 'Marks_scored':[98,90,87,69,78]}, index=[1,2,3,4,5]) two = pd.DataFrame({ 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5'], 'Marks_scored':[89,80,79,97,88]}, index=[1,2,3,4,5]) rs = one.append(two) print(rs) \"\"\" Marks_scored Name subject_id 1 98 Alex sub1 2 90 Amy sub2 3 87 Allen sub4 4 69 Alice sub6 5 78 Ayoung sub5 1 89 Billy sub2 2 80 Brian sub4 3 79 Bran sub3 4 97 Bryce sub6 5 88 Betty sub5 \"\"\" ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:5","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas合并"},{"categories":["python"],"content":" Pandas合并与级联 Pandas合并 Pandas提供了一个单独的merge()函数来进行连接。 pd.merge(left, right, how='inner', on=None, left_on=None, right_on=None,left_index=False, right_index=False, sort=True)1 left - 一个DataFrame对象。 right - 另一个DataFrame对象。 on - 列(名称)连接，必须在左和右DataFrame对象中存在(找到)。 left_on - 左侧DataFrame中的列用作键，可以是列名或长度等于DataFrame长度的数组。 right_on - 来自右的DataFrame的列作为键，可以是列名或长度等于DataFrame长度的数组。 left_index - 如果为True，则使用左侧DataFrame中的索引(行标签)作为其连接键。 在具有MultiIndex(分层)的DataFrame的情况下，级别的数量必须与来自右DataFrame的连接键的数量相匹配。 right_index - 与右DataFrame的left_index具有相同的用法。 how - 它是left, right, outer以及inner之中的一个，默认为内inner。 下面将介绍每种方法的用法。 sort - 按照字典顺序通过连接键对结果DataFrame进行排序。默认为True，设置为False时，在很多情况下大大提高性能。 # 连接实例 import pandas as pd left = pd.DataFrame({ 'id':[1,2,3,4,5], 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5']}) right = pd.DataFrame( {'id':[1,2,3,4,5], 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5']}) print (left) print(\"========================================\") print (right) \"\"\" Name id subject_id 0 Alex 1 sub1 1 Amy 2 sub2 2 Allen 3 sub4 3 Alice 4 sub6 4 Ayoung 5 sub5 ======================================== Name id subject_id 0 Billy 1 sub2 1 Brian 2 sub4 2 Bran 3 sub3 3 Bryce 4 sub6 4 Betty 5 sub5 \"\"\" rs = pd.merge(left,right,on='id') print(rs) \"\"\" Name_x id subject_id_x Name_y subject_id_y 0 Alex 1 sub1 Billy sub2 1 Amy 2 sub2 Brian sub4 2 Allen 3 sub4 Bran sub3 3 Alice 4 sub6 Bryce sub6 4 Ayoung 5 sub5 Betty sub5 \"\"\" how参数与sql: 合并方法 SQL等效 描述 left LEFT OUTER JOIN 使用左侧对象的键 right RIGHT OUTER JOIN 使用右侧对象的键 outer FULL OUTER JOIN 使用键的联合 inner INNER JOIN 使用键的交集 rs = pd.merge(left, right, on='subject_id', how='left') \"\"\" Name_x id_x subject_id Name_y id_y 0 Alex 1 sub1 NaN NaN 1 Amy 2 sub2 Billy 1.0 2 Allen 3 sub4 Brian 2.0 3 Alice 4 sub6 Bryce 4.0 4 Ayoung 5 sub5 Betty 5.0 \"\"\" rs = pd.merge(left, right, on='subject_id', how='right') \"\"\" Name_x id_x subject_id Name_y id_y 0 Amy 2.0 sub2 Billy 1 1 Allen 3.0 sub4 Brian 2 2 Alice 4.0 sub6 Bryce 4 3 Ayoung 5.0 sub5 Betty 5 4 NaN NaN sub3 Bran 3 \"\"\" Pandas级联 pd.concat(objs,axis=0,join='outer',join_axes=None,ignore_index=False) objs - 这是Series，DataFrame或Panel对象的序列或映射。 axis - {0，1，...}，默认为0，这是连接的轴。 join - {'inner', 'outer'}，默认inner。如何处理其他轴上的索引。联合的外部和交叉的内部。 ignore_index − 布尔值，默认为False。如果指定为True，则不要使用连接轴上的索引值。结果轴将被标记为：0，...，n-1。 join_axes - 这是Index对象的列表。用于其他(n-1)轴的特定索引，而不是执行内部/外部集逻辑。 import pandas as pd one = pd.DataFrame({ 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5'], 'Marks_scored':[98,90,87,69,78]}, index=[1,2,3,4,5]) two = pd.DataFrame({ 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5'], 'Marks_scored':[89,80,79,97,88]}, index=[1,2,3,4,5]) rs = pd.concat([one,two]) print(rs) \"\"\" Marks_scored Name subject_id 1 98 Alex sub1 2 90 Amy sub2 3 87 Allen sub4 4 69 Alice sub6 5 78 Ayoung sub5 1 89 Billy sub2 2 80 Brian sub4 3 79 Bran sub3 4 97 Bryce sub6 5 88 Betty sub5 \"\"\" rs = pd.concat([one,two],keys=['x','y'],ignore_index=True) print(rs) \"\"\" Marks_scored Name subject_id 0 98 Alex sub1 1 90 Amy sub2 2 87 Allen sub4 3 69 Alice sub6 4 78 Ayoung sub5 5 89 Billy sub2 6 80 Brian sub4 7 79 Bran sub3 8 97 Bryce sub6 9 88 Betty sub5 \"\"\" # 附加连接 append import pandas as pd one = pd.DataFrame({ 'Name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], 'subject_id':['sub1','sub2','sub4','sub6','sub5'], 'Marks_scored':[98,90,87,69,78]}, index=[1,2,3,4,5]) two = pd.DataFrame({ 'Name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], 'subject_id':['sub2','sub4','sub3','sub6','sub5'], 'Marks_scored':[89,80,79,97,88]}, index=[1,2,3,4,5]) rs = one.append(two) print(rs) \"\"\" Marks_scored Name subject_id 1 98 Alex sub1 2 90 Amy sub2 3 87 Allen sub4 4 69 Alice sub6 5 78 Ayoung sub5 1 89 Billy sub2 2 80 Brian sub4 3 79 Bran sub3 4 97 Bryce sub6 5 88 Betty sub5 \"\"\" ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:5","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas级联"},{"categories":["python"],"content":" Pandas时间工具 # 时间工具实例 import pandas as pd # 1.获取当前时间 pd.datetime.now() # datetime.datetime(2019, 1, 10, 10, 56, 6, 388470) # 2.创建时间戳 time = pd.Timestamp('2019-11-01') time # 2019-11-01 00:00:00 # 3.创建时间范围 time = pd.date_range(\"12:00\", \"23:59\", freq=\"30min\").time time \"\"\" [datetime.time(12, 0) datetime.time(12, 30) datetime.time(13, 0) datetime.time(13, 30) datetime.time(14, 0) datetime.time(14, 30) datetime.time(15, 0) datetime.time(15, 30) datetime.time(16, 0) datetime.time(16, 30) datetime.time(17, 0) datetime.time(17, 30) datetime.time(18, 0) datetime.time(18, 30) datetime.time(19, 0) datetime.time(19, 30) datetime.time(20, 0) datetime.time(20, 30) datetime.time(21, 0) datetime.time(21, 30) datetime.time(22, 0) datetime.time(22, 30) datetime.time(23, 0) datetime.time(23, 30)] \"\"\" # 4.改变时间频率 \"\"\" [datetime.time(12, 0) datetime.time(13, 0) datetime.time(14, 0) datetime.time(15, 0) datetime.time(16, 0) datetime.time(17, 0) datetime.time(18, 0) datetime.time(19, 0) datetime.time(20, 0) datetime.time(21, 0) datetime.time(22, 0) datetime.time(23, 0)] \"\"\" # 5.转换时间戳 # 要转换类似日期的对象(例如字符串，时代或混合)的序列或类似列表的对象，可以使用to_datetime函数。 # 当传递时将返回一个Series(具有相同的索引)，而类似列表被转换为DatetimeIndex time = pd.to_datetime(pd.Series(['Jul 31, 2009','2019-10-10', None])) time \"\"\" 0 2009-07-31 1 2019-10-10 2 NaT dtype: datetime64[ns] \"\"\" # 6.时间差函数 Timedelta timediff = pd.Timedelta(6,unit='h') # 0 days 06:00:00 timediff = pd.Timedelta(days=2) # 2 days 00:00:00 ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:6","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas时间工具"},{"categories":["python"],"content":" Pandas类别数据 通常实时的数据包括重复的文本列。例如：性别，国家和代码等特征总是重复的。这些是分类数据的例子。 分类变量只能采用有限的数量，而且通常是固定的数量。除了固定长度，分类数据可能有顺序，但不能执行数字操作。 分类是*Pandas*数据类型。 分类数据类型在以下情况下非常有用 : 一个字符串变量，只包含几个不同的值。将这样的字符串变量转换为分类变量将会节省一些内存。 变量的词汇顺序与逻辑顺序(\"one\"，\"two\"，\"three\")不同。 通过转换为分类并指定类别上的顺序，排序和最小/最大将使用逻辑顺序，而不是词法顺序。 作为其他python库的一个信号，这个列应该被当作一个分类变量(例如，使用合适的统计方法或plot类型)。 分类对象可以通过多种方式创建。 pandas对象创建中将dtype指定为category pd.Categorical import pandas as pd # 1.第一种，指定类型 s = pd.Series([\"a\",\"b\",\"c\",\"a\"], dtype=\"category\") s \"\"\" 0 a 1 b 2 c 3 a dtype: category Categories (3, object): [a, b, c] \"\"\" # 2.第二种，使用Categorical cat = pd.Categorical(['a', 'b', 'c', 'a', 'b', 'c']) cat \"\"\" [a, b, c, a, b, c] Categories (3, object): [a, b, c] \"\"\" cat = pd.Categorical(['a','b','c','a','b','c','d'], ['c', 'b', 'a'],ordered=True) cat \"\"\" [a, b, c, a, b, c, NaN] Categories (3, object): [c \u003c b \u003c a] \"\"\" ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:7","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas类别数据"},{"categories":["python"],"content":" Pandas IO工具 Pandas I/O API是一套pd.read_xxx()，返回Pandas对象的顶级读取器函数。 例如： read_csv read_excel read_table read_sql 等等 pandas.read_csv(filepath_or_buffer, sep=',', delimiter=None, header='infer',names=None, index_col=None, usecols=None) ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:8","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas-io工具"},{"categories":["python"],"content":" Pandas自定义设置 API由五个相关函数组成。它们分别是： get_option() set_option() reset_option() describe_option() option_context() 编号 参数 描述 1 display.max_rows 要显示的最大行数 2 display.max_columns 要显示的最大列数 3 display.expand_frame_repr 显示数据帧以拉伸页面 4 display.max_colwidth 显示最大列宽 5 display.precision 显示十进制数的精度 import pandas as pd # 1.get_option print (\"display.max_rows = \", pd.get_option(\"display.max_rows\")) # display.max_rows = 60 # 2.set_option pd.set_option(\"display.max_rows\",80) print (\"after set display.max_rows = \", pd.get_option(\"display.max_rows\")) # display.max_rows = 80 ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:9","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas自定义设置"},{"categories":["python"],"content":" Pandas运用实例 import pandas as pd import numpy as np # 读入数据或者创建数据 # 1.读入csv文件 df = pd.DataFrame(pd.read_csv('city.csv',header=1)) # 2.或者使用pandas创建df df = pd.DataFrame({\"id\":[1,2,3,4,5,6], \"date\":pd.date_range('20130102', periods=6), \"city\":['Beijing ', 'SH', ' guangzhou ', 'Shenzhen', 'shanghai', 'BEIJING '], \"age\":[23,44,54,32,34,32], \"category\":['100-A','100-B','110-A','110-C','210-A','130-F'], \"price\":[1200,np.nan,2133,5433,np.nan,4432]}, columns =['id','date','city','category','age','price']) df1=pd.DataFrame({\"id\":[1,2,3,4,5,6,7,8], \"gender\"['male','female','male','female','male','female','male','female'], \"pay\":['Y','N','Y','Y','N','Y','N','Y',], \"m-point\":[10,12,20,40,40,40,30,20]}) # 基本信息查看 # 3.查看shape维度 # (6, 6) # 4.查看info df.info() \"\"\" \u003cclass 'pandas.core.frame.DataFrame'\u003e RangeIndex: 6 entries, 0 to 5 Data columns (total 6 columns): id 6 non-null int64 date 6 non-null datetime64[ns] city 6 non-null object category 6 non-null object age 6 non-null int64 price 4 non-null float64 dtypes: datetime64[ns](1), float64(1), int64(2), object(2) memory usage: 368.0+ bytes \"\"\" # 5.查看数据类型 df.dtypes \"\"\" id int64 date datetime64[ns] city object category object age int64 price float64 dtype: object \"\"\" # 6.查看空值 df.isnull() \"\"\" id date city category age price 0 False False False False False False 1 False False False False False True 2 False False False False False False 3 False False False False False False 4 False False False False False True 5 False False False False False False \"\"\" # 7.查看列名 df.columns \"\"\" Index(['id', 'date', 'city', 'category', 'age', 'price'], dtype='object') \"\"\" ## 数据清洗 # 8.填充缺失值 df.fillna(value=0) # 使用均值填充缺失值 df['prince'].fillna(df['prince'].mean()) # 9.大小写转换 df['city']=df['city'].str.lower() # 10.更改列名 df.rename(columns={'category': 'category-size'}) # 11.更改数据类型 df['price'].astype('int') # 12.去除重复值 df['city'].drop_duplicates(keep='last') # 13.数据替换 df['city'].replace('sh', 'shanghai') # 14.合并数据集 df_inner=pd.merge(df,df1,how='inner') # 匹配合并，交集 # 15.设置索引 df_inner.set_index('id') # 16.排序 df_inner.sort_values(by=['age']) # 17.根据条件处理新的一列 df_inner['group'] = np.where(df_inner['price'] \u003e 3000,'high','low') # 18.判断city列是否存在某个值 df_inner['city'].isin(['beijing']) # 19.数据筛选 df_inner.loc[(df_inner['age'] \u003e 25) \u0026 (df_inner['city'] == 'beijing'), ['id','city','age','category','gender']] df_inner.query('city == [\"beijing\", \"shanghai\"]') ### 数据汇总统计 # 20.对city统计 df_inner.groupby('city').count() # 21.根据city分组，并计算price的均值，总和，个数 df_inner.groupby('city')['price'].agg([len,np.sum, np.mean]) # 22.样本抽样,放回采样 df_inner.sample(n=6, replace=True) # 23.计算两个字段的方差 df_inner['price'].cov(df_inner['m-point']) # 24.计算所有字段之间的相关系数 df_inner.corr() # 25.最后输出到csv df_inner.to_csv('excel_to_python.csv') ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:10","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas运用实例"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#pandas参考函数api"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#构造函数"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#属性和数据"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#类型转换"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#索引和迭代"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#二元运算"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#函数应用分组窗口"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#描述统计学"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#从新索引选取标签操作"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#处理缺失值"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#从新定型排序转变形态"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#combining-joiningmerging"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#时间序列"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#作图"},{"categories":["python"],"content":" Pandas参考函数API 构造函数 方法 描述 DataFrame([data, index, columns, dtype, copy]) 构造数据框 属性和数据 方法 描述 Axes index: row labels；columns: column labels DataFrame.as_matrix([columns]) 转换为矩阵 DataFrame.dtypes 返回数据的类型 DataFrame.ftypes Return the ftypes (indication of sparse/dense and dtype) in this object. DataFrame.get_dtype_counts() 返回数据框数据类型的个数 DataFrame.get_ftype_counts() Return the counts of ftypes in this object. DataFrame.select_dtypes([include, exclude]) 根据数据类型选取子数据框 DataFrame.values Numpy的展示方式 DataFrame.axes 返回横纵坐标的标签名 DataFrame.ndim 返回数据框的纬度 DataFrame.size 返回数据框元素的个数 DataFrame.shape 返回数据框的形状 DataFrame.memory_usage([index, deep]) Memory usage of DataFrame columns. 类型转换 方法 描述 DataFrame.astype(dtype[, copy, errors]) 转换数据类型 DataFrame.copy([deep]) 复制数据框 DataFrame.isnull() 以布尔的方式返回空值 DataFrame.notnull() 以布尔的方式返回非空值 索引和迭代 方法 描述 DataFrame.head([n]) 返回前n行数据 DataFrame.at 快速标签常量访问器 DataFrame.iat 快速整型常量访问器 DataFrame.loc 标签定位 DataFrame.iloc 整型定位 DataFrame.insert(loc, column, value[, …]) 在特殊地点插入行 DataFrame.iter() Iterate over infor axis DataFrame.iteritems() 返回列名和序列的迭代器 DataFrame.iterrows() 返回索引和序列的迭代器 DataFrame.itertuples([index, name]) Iterate over DataFrame rows as namedtuples, with index value as first element of the tuple. DataFrame.lookup(row_labels, col_labels) Label-based “fancy indexing” function for DataFrame. DataFrame.pop(item) 返回删除的项目 DataFrame.tail([n]) 返回最后n行 DataFrame.xs(key[, axis, level, drop_level]) Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. DataFrame.isin(values) 是否包含数据框中的元素 DataFrame.where(cond[, other, inplace, …]) 条件筛选 DataFrame.mask(cond[, other, inplace, axis, …]) Return an object of same shape as self and whose corresponding entries are from self where cond is False and otherwise are from other. DataFrame.query(expr[, inplace]) Query the columns of a frame with a boolean expression. 二元运算 方法 描述 DataFrame.add(other[, axis, level, fill_value]) 加法，元素指向 DataFrame.sub(other[, axis, level, fill_value]) 减法，元素指向 DataFrame.mul(other[, axis, level, fill_value]) 乘法，元素指向 DataFrame.div(other[, axis, level, fill_value]) 小数除法，元素指向 DataFrame.truediv(other[, axis, level, …]) 真除法，元素指向 DataFrame.floordiv(other[, axis, level, …]) 向下取整除法，元素指向 DataFrame.mod(other[, axis, level, fill_value]) 模运算，元素指向 DataFrame.pow(other[, axis, level, fill_value]) 幂运算，元素指向 DataFrame.radd(other[, axis, level, fill_value]) 右侧加法，元素指向 DataFrame.rsub(other[, axis, level, fill_value]) 右侧减法，元素指向 DataFrame.rmul(other[, axis, level, fill_value]) 右侧乘法，元素指向 DataFrame.rdiv(other[, axis, level, fill_value]) 右侧小数除法，元素指向 DataFrame.rtruediv(other[, axis, level, …]) 右侧真除法，元素指向 DataFrame.rfloordiv(other[, axis, level, …]) 右侧向下取整除法，元素指向 DataFrame.rmod(other[, axis, level, fill_value]) 右侧模运算，元素指向 DataFrame.rpow(other[, axis, level, fill_value]) 右侧幂运算，元素指向 DataFrame.lt(other[, axis, level]) 类似Array.lt DataFrame.gt(other[, axis, level]) 类似Array.gt DataFrame.le(other[, axis, level]) 类似Array.le DataFrame.ge(other[, axis, level]) 类似Array.ge DataFrame.ne(other[, axis, level]) 类似Array.ne DataFrame.eq(other[, axis, level]) 类似Array.eq DataFrame.combine(other, func[, fill_value, …]) Add two DataFrame objects and do not propagate NaN values, so if for a DataFrame.combine_first(other) Combine two DataFrame objects and default to non-null values in frame calling the method. 函数应用\u0026分组\u0026窗口 方法 描述 [DataFrame.apply(func, axis, broadcast, …]) 应用函数 DataFrame.applymap(func) Apply a function to a DataFrame that is intended to operate elementwise, i.e. DataFrame.aggregate(func[, axis]) Aggregate using callable, string, dict, or list of string/callables DataFrame.transform(func, *args, **kwargs) Call function producing a like-indexed NDFrame DataFrame.groupby([by, axis, level, …]) 分组 DataFrame.rolling(window[, min_periods, …]) 滚动窗口 DataFrame.expanding([min_periods, freq, …]) 拓展窗口 DataFrame.ewm([com, span, halflife, alpha, …]) 指数权重窗口 描述统计学 方法 描述 DataFrame.abs() 返回绝对值 DataFrame.all([axis, bool_only, skipna, level]) Return whether all elements are True over requested axis DataFrame.a","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:11","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#转换为其他格式"},{"categories":["python"],"content":" 参考链接 csdn 基本函数 易百教程 ","date":"2019-01-10","objectID":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/:0:12","series":null,"tags":["python","Pandas"],"title":"python之Pandas笔记","uri":"/python%E4%B9%8Bpandas%E7%AC%94%E8%AE%B0/#参考链接"},{"categories":["机器学习"],"content":" 传统文本分类方法文本分类问题算是自然语言处理领域中一个非常经典的问题了，相关研究最早可以追溯到上世纪50年代，当时是通过专家规则（Pattern）进行分类，甚至在80年代初一度发展到利用知识工程建立专家系统，这样做的好处是短平快的解决top问题，但显然天花板非常低，不仅费时费力，覆盖的范围和准确率都非常有限。 后来伴随着统计学习方法的发展，特别是90年代后互联网在线文本数量增长和机器学习学科的兴起，逐渐形成了一套解决大规模文本分类问题的经典玩法，这个阶段的主要套路是人工特征工程+浅层分类模型。训练文本分类器过程见下图： 整个文本分类问题就拆分成了特征工程和分类器两部分。 1.1 特征工程 特征工程在机器学习中往往是最耗时耗力的，但却极其的重要。抽象来讲，机器学习问题是把数据转换成信息再提炼到知识的过程，特征是“数据–\u003e信息”的过程，决定了结果的上限，而分类器是“信息–\u003e知识”的过程，则是去逼近这个上限。然而特征工程不同于分类器模型，不具备很强的通用性，往往需要结合对特征任务的理解。 文本分类问题所在的自然语言领域自然也有其特有的特征处理逻辑，传统分本分类任务大部分工作也在此处。文本特征工程分位文本预处理、特征提取、文本表示三个部分，最终目的是把文本转换成计算机可理解的格式，并封装足够用于分类的信息，即很强的特征表达能力。 1）文本预处理 文本预处理过程是在文本中提取关键词表示文本的过程，中文文本处理中主要包括文本分词和去停用词两个阶段。之所以进行分词，是因为很多研究表明特征粒度为词粒度远好于字粒度，其实很好理解，因为大部分分类算法不考虑词序信息，基于字粒度显然损失了过多“n-gram”信息。 具体到中文分词，不同于英文有天然的空格间隔，需要设计复杂的分词算法。传统算法主要有基于字符串匹配的正向/逆向/双向最大匹配；基于理解的句法和语义分析消歧；基于统计的互信息/CRF方法。近年来随着深度学习的应用，WordEmbedding + Bi-LSTM+CRF方法逐渐成为主流，本文重点在文本分类，就不展开了。而停止词是文本中一些高频的代词连词介词等对文本分类无意义的词，通常维护一个停用词表，特征提取过程中删除停用表中出现的词，本质上属于特征选择的一部分。 经过文本分词和去停止词之后，一个句子就变成了下图“ / ”分割的一个个关键词的形式： 夏装 / 雪纺 / 条纹 / 短袖 / t恤 / 女 / 春 / 半袖 / 衣服 / 夏天 / 中长款 / 大码 / 胖mm / 显瘦 / 上衣 / 夏 2）文本表示和特征提取 文本表示： 文本表示的目的是把文本预处理后的转换成计算机可理解的方式，是决定文本分类质量最重要的部分。传统做法常用词袋模型（BOW, Bag Of Words）或向量空间模型（Vector Space Model），最大的不足是忽略文本上下文关系，每个词之间彼此独立，并且无法表征语义信息。词袋模型的示例如下： ( 0, 0, 0, 0, .... , 1, ... 0, 0, 0, 0) 一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高纬度、高稀疏性。词袋模型是向量空间模型的基础，因此向量空间模型通过特征项选择降低维度，通过特征权重计算增加稠密性。 特征提取： 向量空间模型的文本表示方法的特征提取对应特征项的选择和特征权重计算两部分。特征选择的基本思路是根据某个评价指标独立的对原始特征项（词项）进行评分排序，从中选择得分最高的一些特征项，过滤掉其余的特征项。常用的评价有文档频率、互信息、信息增益、χ²统计量等。 特征权重主要是经典的TF-IDF方法及其扩展方法，主要思路是一个词的重要度与在类别内的词频成正比，与所有类别出现的次数成反比。 3）基于语义的文本表示 传统做法在文本表示方面除了向量空间模型，还有基于语义的文本表示方法，比如LDA主题模型、LSI/PLSI概率潜在语义索引等方法，一般认为这些方法得到的文本表示可以认为文档的深层表示，而word embedding文本分布式表示方法则是深度学习方法的重要基础，下文会展现。 1.2 分类器 分类器基本都是统计分类方法了，基本上大部分机器学习方法都在文本分类领域有所应用，比如朴素贝叶斯分类算法（Naïve Bayes）、KNN、SVM、最大熵和神经网络等等。 ","date":"2019-01-07","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/:1:0","series":null,"tags":["机器学习","深度学习","NLP"],"title":"机器学习案例之NLP文本分类","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#传统文本分类方法"},{"categories":["机器学习"],"content":" **深度学习 **文本分类方法上面介绍了传统的文本分类做法，传统做法主要问题的文本表示是高纬度高稀疏的，特征表达能力很弱，而且神经网络很不擅长对此类数据的处理；此外需要人工进行特征工程，成本很高。而深度学习最初在之所以图像和语音取得巨大成功，一个很重要的原因是图像和语音原始数据是连续和稠密的，有局部相关性，。应用深度学习解决大规模文本分类问题最重要的是解决文本表示，再利用CNN/RNN等网络结构自动获取特征表达能力，去掉繁杂的人工特征工程，端到端的解决问题。接下来会分别介绍： 2.1 文本的分布式表示：词向量（word embedding） 分布式表示（Distributed Representation）其实Hinton 最早在1986年就提出了，基本思想是将每个词表达成 n 维稠密、连续的实数向量，与之相对的one-hot encoding向量空间只有一个维度是1，其余都是0。分布式表示最大的优点是具备非常powerful的特征表达能力，比如 n 维向量每维 k 个值，可以表征 个概念。事实上，不管是神经网络的隐层，还是多个潜在变量的概率主题模型，都是应用分布式表示。下图是03年Bengio在 A Neural Probabilistic Language Model的网络结构： 这篇文章提出的神经网络语言模型（NNLM，Neural Probabilistic Language Model）采用的是文本分布式表示，即每个词表示为稠密的实数向量。NNLM模型的目标是构建语言模型： 词的分布式表示即词向量（word embedding）是训练语言模型的一个附加产物，即图中的Matrix C。 尽管Hinton 86年就提出了词的分布式表示，Bengio 03年便提出了NNLM，词向量真正火起来是google Mikolov 13年发表的两篇word2vec的文章 Efficient Estimation of Word Representations in Vector Space 和 Distributed Representations of Words and Phrases and their Compositionality 更重要的是发布了简单好用的 word2vec工具包 在语义维度上得到了很好的验证，极大的推进了文本分析的进程。下图是文中提出的CBOW 和 Skip-Gram两个模型的结构，基本类似于NNLM，不同的是模型去掉了非线性隐层，预测目标不同，CBOW是上下文词预测当前词，Skip-Gram则相反。 除此之外，提出了Hierarchical Softmax 和 Negative Sample两个方法，很好的解决了计算有效性，事实上这两个方法都没有严格的理论证明，有些trick之处，非常的实用主义。实际上word2vec学习的向量和真正语义还有差距，更多学到的是具备相似上下文的词，比如“good”“bad”相似度也很高，反而是文本分类任务输入有监督的语义能够学到更好的语义表示，有机会后续系统分享下。 至此，文本的表示通过词向量的表示方式，把文本数据从高纬度高稀疏的神经网络难处理的方式，变成了类似图像、语音的的连续稠密数据。深度学习算法本身有很强的数据迁移性，很多之前在图像领域很适用的深度学习算法比如CNN等也可以很好的迁移到文本领域了。 2.2 深度学习文本分类模型 词向量解决了文本表示的问题，文本分类模型则是利用CNN/RNN等深度学习网络及其变体解决自动特征提取（即特征表达）的问题。 1）fastText fastText 是上文提到的 word2vec 作者 Mikolov 转战 Facebook 后16年7月刚发表的一篇论文 Bag of Tricks for Efficient Text Classification。 fastText 它极致简单，模型图见下： 原理是把句子中所有的词向量进行平均（某种意义上可以理解为只有一个avg pooling特殊CNN），然后直接接 softmax 层。其实文章也加入了一些 n-gram 特征的 trick 来捕获局部序列信息。文章倒没太多信息量，算是“水文”吧，带来的思考是文本分类问题是有一些“线性”问题的部分，也就是说不必做过多的非线性转换、特征组合即可捕获很多分类信息，因此有些任务即便简单的模型便可以搞定了。 2）TextCNN fastText 中的网络结果是完全没有考虑词序信息的，而它用的 n-gram 特征 trick 恰恰说明了局部序列信息的重要意义。卷积神经网络（CNN Convolutional Neural Network）最初在图像领域取得了巨大成功，核心点在于可以捕捉局部相关性，具体到文本分类任务中可以利用CNN来提取句子中类似 n-gram 的关键信息。 TextCNN的详细过程原理图见下： TextCNN详细过程：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5，这个可以类比为图像中的原始像素点了。然后经过有 filter_size=(2,3,4) 的一维卷积层，每个filter_size 有两个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。 特征：这里的特征就是词向量，有静态（static）和非静态（non-static）方式。static方式采用比如word2vec预训练的词向量，训练过程不更新词向量，实质上属于迁移学习了，特别是数据量比较小的情况下，采用静态的词向量往往效果不错。non-static则是在训练过程中更新词向量。推荐的方式是 non-static 中的 fine-tunning方式，它是以预训练（pre-train）的word2vec向量初始化词向量，训练过程中调整词向量，能加速收敛，当然如果有充足的训练数据和资源，直接随机初始化词向量效果也是可以的。 通道（Channels）：图像中可以利用 (R, G, B) 作为不同channel，而文本的输入的channel通常是不同方式的embedding方式（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法。 一维卷积（conv-1d）：图像是二维数据，经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。一维卷积带来的问题是需要设计通过不同 filter_size 的 filter 获取不同宽度的视野。 Pooling层：将 pooling 改成 (dynamic) k-max pooling ，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。比如在情感分析场景，举个例子： “ 我觉得这个地方景色还不错，但是人也实在太多了 ” 虽然前半部分体现情感是正向的，全局文本表达的是偏负面的情感，利用 k-max pooling能够很好捕捉这类信息。 3）TextRNN 尽管TextCNN能够在很多任务里面能有不错的表现，但CNN有个最大问题是固定 filter_size 的视野，一方面无法建模更长的序列信息，另一方面 filter_size 的超参调节也很繁琐。CNN本质是做文本的特征表达工作，而自然语言处理中更常用的是递归神经网络（RNN, Recurrent Neural Network），能够更好的表达上下文信息。具体在文本分类任务中，Bi-directional RNN（实际使用的是双向LSTM）从某种意义上可以理解为可以捕获变长且双向的的 “n-gram” 信息。 RNN算是在自然语言处理领域非常一个标配网络了，在序列标注/命名体识别/seq2seq模型等很多场景都有应用，[，下图LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出了。 4）TextRNN + Attention CNN和RNN用在文本分类任务中尽管效果显著，但都有一个不足的地方就是不够直观，可解释性不好，特别是在分析badcase时候感受尤其深刻。而注意力（Attention）机制是自然语言处理领域一个常用的建模长时间记忆机制，能够很直观的给出每个词对结果的贡献，基本成了Seq2Seq模型的标配了。实际上文本分类从某种意义上也可以理解为一种特殊的Seq2Seq，所以考虑把Attention机制引入近来。 Attention机制介绍： 以机器翻译为例简单介绍下，下图中 $x_t$ 是源语言的一个词，$y_t$ 是目标语言的一个词，机器翻译的任务就是给定源序列得到目标序列。翻译 $y_t$ 的过程产生取决于上一个词 $y_{t - 1}$和源语言的词的表示 $h_j$（$x_j$ 的 bi-RNN 模型的表示），而每个词所占的权重是不一样的。比如源语言是中文 “我 / 是 / 中国人” 目标语言 “i / am / Chinese”，翻译出“Chinese”时候显然取决于“中国人”，而与“我 / 是”基本无关。下图公式$\\alpha_{ij}$ 则是翻译英文第 $i$ 个词时，中文第 $j$ 个词的贡献，也就是注意力。显然在翻译“Chinese”时，“中国人”的注意力值非常大。 Attention的核心point是在翻译每个目标词（或 预测商品标题文本所属类别）所用的上下文是不同的，这样的考虑显然是更合理的。 TextRNN + Attention 模型： 下图是模型的网络结构图，它一方面用层次化的结构保留了文档的","date":"2019-01-07","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/:2:0","series":null,"tags":["机器学习","深度学习","NLP"],"title":"机器学习案例之NLP文本分类","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#深度学习-文本分类方法"},{"categories":["机器学习"],"content":" 实验报告说明 在本次实验中，我们爬取了旅游评论数据集，总共有35706条数据，正样本为好评，负样本为差评，其中正样本有32460条，负样本有3246条。对于文本表示，我们使用VSM和word2vec模型。一共做了四种测试： VSM + RandomForestClassifier VSM + BernoulliNB word2vec + TextRNN word2vec + TextRNN + Attention 1.VSM + RandomForestClassifier 实验采用6-fold验证，accuracy,precision,recall如下所示： 2.VSM + BernoulliNB 实验采用6-fold验证，accuracy,precision,recall如下所示： 3.word2vec + TextRNN 双向GRU结构，模型结构参数如下： 实验结果如下所示： 4.word2vec + TextRNN + Attention LSTM加Attention模型，模型参数如下： 实验结果如下所示： 从上面四个实验，我们可以看出，传统的文本分类模型，效果还是不错的，其中RandomForestClassifier的效果要比BernoulliNB效果更好一些。深度学习的方法，acc大约也在0.9左右，训练集和测试集的acc相差不大。 ","date":"2019-01-07","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/:3:0","series":null,"tags":["机器学习","深度学习","NLP"],"title":"机器学习案例之NLP文本分类","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#实验报告说明"},{"categories":["机器学习"],"content":" 项目链接GitHub ","date":"2019-01-07","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/:4:0","series":null,"tags":["机器学习","深度学习","NLP"],"title":"机器学习案例之NLP文本分类","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#项目链接"},{"categories":["机器学习"],"content":" 参考深度学习（CNN RNN Attention）解决大规模文本分类问题 ","date":"2019-01-07","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/:5:0","series":null,"tags":["机器学习","深度学习","NLP"],"title":"机器学习案例之NLP文本分类","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Bnlp%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/#参考"},{"categories":["机器学习"],"content":" Titanic是kaggle上的一道just for fun的题，没有奖金，但是数据整洁，适合用来练手，进行数据分析和机器学习。 这道题给的数据是泰坦尼克号上的乘客的信息，预测乘客是否幸存。这是个二元分类的机器学习问题，但是由于数据样本相对较少，还是具有一定的挑战性。 ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:0:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#"},{"categories":["机器学习"],"content":" 导入相关的库 import numpy as np import pandas as pd from sklearn.preprocessing import StandardScaler from sklearn.linear_model import LogisticRegression from sklearn.model_selection import train_test_split from sklearn.model_selection import KFold from sklearn.metrics import make_scorer, confusion_matrix from sklearn.model_selection import learning_curve import matplotlib.pyplot as plt import seaborn as sns from sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier from sklearn.neighbors import KNeighborsClassifier from sklearn.naive_bayes import GaussianNB from sklearn.neural_network import MLPClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.svm import SVC, LinearSVC from sklearn.metrics import accuracy_score, roc_curve, auc from scipy import interp sns.set_style('whitegrid') ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:1:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#导入相关的库"},{"categories":["机器学习"],"content":" 读入数据 trainDF = pd.read_csv('./Tantic_Data/train.csv') testDF = pd.read_csv('./Tantic_Data/test.csv') ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:2:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#读入数据"},{"categories":["机器学习"],"content":" 数据的一般信息首先让我们查看一下数据的分布 ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:3:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#数据的一般信息"},{"categories":["机器学习"],"content":" Training Data trainDF.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e RangeIndex: 891 entries, 0 to 890 Data columns (total 12 columns): PassengerId 891 non-null int64 Survived 891 non-null int64 Pclass 891 non-null int64 Name 891 non-null object Sex 891 non-null object Age 714 non-null float64 SibSp 891 non-null int64 Parch 891 non-null int64 Ticket 891 non-null object Fare 891 non-null float64 Cabin 204 non-null object Embarked 889 non-null object dtypes: float64(2), int64(5), object(5) memory usage: 83.6+ KB ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:3:1","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#training-data"},{"categories":["机器学习"],"content":" Testing Data testDF.info() \u003cclass 'pandas.core.frame.DataFrame'\u003e RangeIndex: 418 entries, 0 to 417 Data columns (total 11 columns): PassengerId 418 non-null int64 Pclass 418 non-null int64 Name 418 non-null object Sex 418 non-null object Age 332 non-null float64 SibSp 418 non-null int64 Parch 418 non-null int64 Ticket 418 non-null object Fare 417 non-null float64 Cabin 91 non-null object Embarked 418 non-null object dtypes: float64(2), int64(4), object(5) memory usage: 36.0+ KB 在上面，来自训练集的survived 列是taret / dependent / response变量。 得分为1表示乘客幸存，得分为0表示乘客死亡。 还有描述每位乘客的各种features (variables)： PassengerID：在船上分配给旅行者的ID Pclass：乘客的等级，1,2或3 Name：乘客姓名 Sex：乘客的性别 Age：乘客的年龄 SibSp：与乘客一起旅行的兄弟姐妹/配偶的数量 Parch：与乘客一起旅行的父母/子女的数量 Ticket：乘客的机票号码 Fare：乘客的票价 Cabin：乘客的客舱号码 Embarked：登船港口，S，C或Q（C = Cherbourg，Q = Queenstown，S = Southhampton） ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:3:2","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#testing-data"},{"categories":["机器学习"],"content":" 数据探索首先，让我们看看幸存下来的人数. sns.countplot(x='Survived',data=trainDF) 大多数人都没有活下来。 让我们通过观察按性别存活的人数来进一步研究这个问题。 sns.countplot(x='Survived', hue='Sex', data=trainDF) 在这里我们可以看到，男性死亡的人数多于女性，而且大多数女性幸免于难。 现在让我们按Pclass看看生存计数。 sns.countplot(x='Survived',hue='Pclass', data=trainDF) 在这里我们可以看到，第一类的大多数幸存下来，而第三类的大多数人都死了。 让我们来看看fare分配。 plt.hist(x='Fare',data=trainDF,bins=40) 在这里，我们可以看到大多数人支付的费用低于50，但有一些异常值就像500美元范围内的人一样。 这可以通过每个班级中人数的差异来解释。 最低级别，3，人数最多，最高级别最少。 最低级别支付最低票价，因此此类别中有更多人。 最后，让我们使用热图查看缺失数据的数量。 fig, ax = plt.subplots(figsize=(12,8)) sns.heatmap(trainDF.isnull(), cmap='coolwarm', yticklabels=False, cbar=False, ax=ax) 让我们用test做同样的事情。 fig, ax = plt.subplots(figsize=(12,5)) sns.heatmap(testDF.isnull(), cmap='coolwarm', yticklabels=False, cbar=False, ax=ax) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:4:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#数据探索"},{"categories":["机器学习"],"content":" 数据清洗现在让我们清理数据，以便它可以与scikit-learn模型一起使用。 ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:5:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#数据清洗"},{"categories":["机器学习"],"content":" 缺失数据 Embarked Nulls首先，让我们在数据集中处理NaN,我们首先从Embarked中的NaN开始。 让我们根据登船港口来看待生存机会。 fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5)) # Plot the number of occurances for each embarked location sns.countplot(x='Embarked', data=trainDF, ax=ax1) # Plot the number of people that survived by embarked location sns.countplot(x='Survived', hue = 'Embarked', data=trainDF, ax=ax2, order=[1,0]) # Group by Embarked, and get the mean for survived passengers for each # embarked location embark_pct = trainDF[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean() # Plot the above mean sns.barplot(x='Embarked',y='Survived', data=embark_pct, order=['S','C','Q'], ax=ax3) 在这里，我们可以看到大多数人从S出发，因此大多数幸存下来的人都是S.但是，当我们查看幸存人数的平均值与登机位置登记的总人数时， S的存活率最低。 这不足以确定上述人员登上哪个港口。 让我们看看其他可能表明乘客登船的变量 没有其他用户共享相同的票号。 让我们寻找支付类似票价的同一班级的人。 trainDF[(trainDF['Pclass'] == 1) \u0026 (trainDF['Fare'] \u003e 75) \u0026 (trainDF['Fare'] \u003c 85)].groupby('Embarked')['PassengerId'].count() Embarked C 16 S 13 Name: PassengerId, dtype: int64 在拥有相同级别并支付相同票价的人中，有16人从C出发，13人从S出发。 现在，由于支付类似票价的同一班级的大多数人来自C，而从C开始的人拥有最高的生存率，我们认为这些人可能从C开始。我们现在将他们的登船价值改为 C。 # Set Value trainDF = trainDF.set_value(trainDF['Embarked'].isnull(), 'Embarked','C') /Users/apple/Soft/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead Fare nulls现在来处理Fare列. 让我们想象一下从南安普敦出发的三等乘客支付的票价的直方图。 fig,ax = plt.subplots(figsize=(8,5)) testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].hist(bins=100, ax=ax) plt.xlabel('Fare') plt.ylabel('Frequency') plt.title('Histogram of Fare for Pclass = 3, Embarked = S' print (\"The top 5 most common fares:\") testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].value_counts().head() The top 5 most common fares: 8.0500 17 7.8958 10 7.7750 10 8.6625 8 7.8542 8 Name: Fare, dtype: int64 使用众数填充Fare列的缺失值, $8.05. # Fill value testDF.set_value(testDF.Fare.isnull(), 'Fare', 8.05) # Verify Age nulls现在让我们在训练和测试集中填写缺失的年龄数据。 填充的一种方法是用柱的方式填充NaN。 通过逐类查看平均年龄，我们可以使这个填充过程更加智能化。 plt.figure(figsize=(12,7)) sns.boxplot(x='Pclass', y='Age', data=trainDF) trainDF.groupby('Pclass')['Age'].mean() Pclass 1 38.233441 2 29.877630 3 25.140620 Name: Age, dtype: float64 我们看到等级越高，平均年龄就越高。 然后我们可以使用上述方法编写一个函数来填充NaN年龄值。 def fixNaNAge(age, pclass): if age == age: return age if pclass == 1: return 38 elif pclass == 2: return 30 else: return 25 现在，我们将在训练和测试数据框中填写年龄NaN，并验证它们是否正确填充。 trainDF['Age'] = trainDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']),axis=1) testDF['Age'] = testDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']), axis=1 fig = plt.figure(figsize=(15,5)) trainDF['Age'].astype(int).hist(bins=70) testDF['Age'].astype(int).hist(bins=70) facet = sns.FacetGrid(trainDF, hue='Survived', aspect=4) facet.map(sns.kdeplot, 'Age', shade=True) facet.set(xlim=(0, trainDF['Age'].max())) facet.add_legend() fig, ax = plt.subplots(1,1,figsize=(18,4)) age_mean = trainDF[['Age','Survived']].groupby(['Age'],as_index=False).mean() sns.barplot(x='Age', y='Survived', data=age_mean) Cabin nulls最后，对于机舱列，我们缺少太多信息来正确填充它，因此我们可以完全放弃该特征。 trainDF.drop('Cabin', axis=1,inplace=True) testDF.drop('Cabin', axis=1, inplace=True) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:5:1","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#缺失数据"},{"categories":["机器学习"],"content":" 缺失数据 Embarked Nulls首先，让我们在数据集中处理NaN,我们首先从Embarked中的NaN开始。 让我们根据登船港口来看待生存机会。 fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5)) # Plot the number of occurances for each embarked location sns.countplot(x='Embarked', data=trainDF, ax=ax1) # Plot the number of people that survived by embarked location sns.countplot(x='Survived', hue = 'Embarked', data=trainDF, ax=ax2, order=[1,0]) # Group by Embarked, and get the mean for survived passengers for each # embarked location embark_pct = trainDF[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean() # Plot the above mean sns.barplot(x='Embarked',y='Survived', data=embark_pct, order=['S','C','Q'], ax=ax3) 在这里，我们可以看到大多数人从S出发，因此大多数幸存下来的人都是S.但是，当我们查看幸存人数的平均值与登机位置登记的总人数时， S的存活率最低。 这不足以确定上述人员登上哪个港口。 让我们看看其他可能表明乘客登船的变量 没有其他用户共享相同的票号。 让我们寻找支付类似票价的同一班级的人。 trainDF[(trainDF['Pclass'] == 1) \u0026 (trainDF['Fare'] \u003e 75) \u0026 (trainDF['Fare'] \u003c 85)].groupby('Embarked')['PassengerId'].count() Embarked C 16 S 13 Name: PassengerId, dtype: int64 在拥有相同级别并支付相同票价的人中，有16人从C出发，13人从S出发。 现在，由于支付类似票价的同一班级的大多数人来自C，而从C开始的人拥有最高的生存率，我们认为这些人可能从C开始。我们现在将他们的登船价值改为 C。 # Set Value trainDF = trainDF.set_value(trainDF['Embarked'].isnull(), 'Embarked','C') /Users/apple/Soft/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead Fare nulls现在来处理Fare列. 让我们想象一下从南安普敦出发的三等乘客支付的票价的直方图。 fig,ax = plt.subplots(figsize=(8,5)) testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].hist(bins=100, ax=ax) plt.xlabel('Fare') plt.ylabel('Frequency') plt.title('Histogram of Fare for Pclass = 3, Embarked = S' print (\"The top 5 most common fares:\") testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].value_counts().head() The top 5 most common fares: 8.0500 17 7.8958 10 7.7750 10 8.6625 8 7.8542 8 Name: Fare, dtype: int64 使用众数填充Fare列的缺失值, $8.05. # Fill value testDF.set_value(testDF.Fare.isnull(), 'Fare', 8.05) # Verify Age nulls现在让我们在训练和测试集中填写缺失的年龄数据。 填充的一种方法是用柱的方式填充NaN。 通过逐类查看平均年龄，我们可以使这个填充过程更加智能化。 plt.figure(figsize=(12,7)) sns.boxplot(x='Pclass', y='Age', data=trainDF) trainDF.groupby('Pclass')['Age'].mean() Pclass 1 38.233441 2 29.877630 3 25.140620 Name: Age, dtype: float64 我们看到等级越高，平均年龄就越高。 然后我们可以使用上述方法编写一个函数来填充NaN年龄值。 def fixNaNAge(age, pclass): if age == age: return age if pclass == 1: return 38 elif pclass == 2: return 30 else: return 25 现在，我们将在训练和测试数据框中填写年龄NaN，并验证它们是否正确填充。 trainDF['Age'] = trainDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']),axis=1) testDF['Age'] = testDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']), axis=1 fig = plt.figure(figsize=(15,5)) trainDF['Age'].astype(int).hist(bins=70) testDF['Age'].astype(int).hist(bins=70) facet = sns.FacetGrid(trainDF, hue='Survived', aspect=4) facet.map(sns.kdeplot, 'Age', shade=True) facet.set(xlim=(0, trainDF['Age'].max())) facet.add_legend() fig, ax = plt.subplots(1,1,figsize=(18,4)) age_mean = trainDF[['Age','Survived']].groupby(['Age'],as_index=False).mean() sns.barplot(x='Age', y='Survived', data=age_mean) Cabin nulls最后，对于机舱列，我们缺少太多信息来正确填充它，因此我们可以完全放弃该特征。 trainDF.drop('Cabin', axis=1,inplace=True) testDF.drop('Cabin', axis=1, inplace=True) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:5:1","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#embarked-nulls"},{"categories":["机器学习"],"content":" 缺失数据 Embarked Nulls首先，让我们在数据集中处理NaN,我们首先从Embarked中的NaN开始。 让我们根据登船港口来看待生存机会。 fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5)) # Plot the number of occurances for each embarked location sns.countplot(x='Embarked', data=trainDF, ax=ax1) # Plot the number of people that survived by embarked location sns.countplot(x='Survived', hue = 'Embarked', data=trainDF, ax=ax2, order=[1,0]) # Group by Embarked, and get the mean for survived passengers for each # embarked location embark_pct = trainDF[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean() # Plot the above mean sns.barplot(x='Embarked',y='Survived', data=embark_pct, order=['S','C','Q'], ax=ax3) 在这里，我们可以看到大多数人从S出发，因此大多数幸存下来的人都是S.但是，当我们查看幸存人数的平均值与登机位置登记的总人数时， S的存活率最低。 这不足以确定上述人员登上哪个港口。 让我们看看其他可能表明乘客登船的变量 没有其他用户共享相同的票号。 让我们寻找支付类似票价的同一班级的人。 trainDF[(trainDF['Pclass'] == 1) \u0026 (trainDF['Fare'] \u003e 75) \u0026 (trainDF['Fare'] \u003c 85)].groupby('Embarked')['PassengerId'].count() Embarked C 16 S 13 Name: PassengerId, dtype: int64 在拥有相同级别并支付相同票价的人中，有16人从C出发，13人从S出发。 现在，由于支付类似票价的同一班级的大多数人来自C，而从C开始的人拥有最高的生存率，我们认为这些人可能从C开始。我们现在将他们的登船价值改为 C。 # Set Value trainDF = trainDF.set_value(trainDF['Embarked'].isnull(), 'Embarked','C') /Users/apple/Soft/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead Fare nulls现在来处理Fare列. 让我们想象一下从南安普敦出发的三等乘客支付的票价的直方图。 fig,ax = plt.subplots(figsize=(8,5)) testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].hist(bins=100, ax=ax) plt.xlabel('Fare') plt.ylabel('Frequency') plt.title('Histogram of Fare for Pclass = 3, Embarked = S' print (\"The top 5 most common fares:\") testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].value_counts().head() The top 5 most common fares: 8.0500 17 7.8958 10 7.7750 10 8.6625 8 7.8542 8 Name: Fare, dtype: int64 使用众数填充Fare列的缺失值, $8.05. # Fill value testDF.set_value(testDF.Fare.isnull(), 'Fare', 8.05) # Verify Age nulls现在让我们在训练和测试集中填写缺失的年龄数据。 填充的一种方法是用柱的方式填充NaN。 通过逐类查看平均年龄，我们可以使这个填充过程更加智能化。 plt.figure(figsize=(12,7)) sns.boxplot(x='Pclass', y='Age', data=trainDF) trainDF.groupby('Pclass')['Age'].mean() Pclass 1 38.233441 2 29.877630 3 25.140620 Name: Age, dtype: float64 我们看到等级越高，平均年龄就越高。 然后我们可以使用上述方法编写一个函数来填充NaN年龄值。 def fixNaNAge(age, pclass): if age == age: return age if pclass == 1: return 38 elif pclass == 2: return 30 else: return 25 现在，我们将在训练和测试数据框中填写年龄NaN，并验证它们是否正确填充。 trainDF['Age'] = trainDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']),axis=1) testDF['Age'] = testDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']), axis=1 fig = plt.figure(figsize=(15,5)) trainDF['Age'].astype(int).hist(bins=70) testDF['Age'].astype(int).hist(bins=70) facet = sns.FacetGrid(trainDF, hue='Survived', aspect=4) facet.map(sns.kdeplot, 'Age', shade=True) facet.set(xlim=(0, trainDF['Age'].max())) facet.add_legend() fig, ax = plt.subplots(1,1,figsize=(18,4)) age_mean = trainDF[['Age','Survived']].groupby(['Age'],as_index=False).mean() sns.barplot(x='Age', y='Survived', data=age_mean) Cabin nulls最后，对于机舱列，我们缺少太多信息来正确填充它，因此我们可以完全放弃该特征。 trainDF.drop('Cabin', axis=1,inplace=True) testDF.drop('Cabin', axis=1, inplace=True) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:5:1","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#fare-nulls"},{"categories":["机器学习"],"content":" 缺失数据 Embarked Nulls首先，让我们在数据集中处理NaN,我们首先从Embarked中的NaN开始。 让我们根据登船港口来看待生存机会。 fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5)) # Plot the number of occurances for each embarked location sns.countplot(x='Embarked', data=trainDF, ax=ax1) # Plot the number of people that survived by embarked location sns.countplot(x='Survived', hue = 'Embarked', data=trainDF, ax=ax2, order=[1,0]) # Group by Embarked, and get the mean for survived passengers for each # embarked location embark_pct = trainDF[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean() # Plot the above mean sns.barplot(x='Embarked',y='Survived', data=embark_pct, order=['S','C','Q'], ax=ax3) 在这里，我们可以看到大多数人从S出发，因此大多数幸存下来的人都是S.但是，当我们查看幸存人数的平均值与登机位置登记的总人数时， S的存活率最低。 这不足以确定上述人员登上哪个港口。 让我们看看其他可能表明乘客登船的变量 没有其他用户共享相同的票号。 让我们寻找支付类似票价的同一班级的人。 trainDF[(trainDF['Pclass'] == 1) \u0026 (trainDF['Fare'] \u003e 75) \u0026 (trainDF['Fare'] \u003c 85)].groupby('Embarked')['PassengerId'].count() Embarked C 16 S 13 Name: PassengerId, dtype: int64 在拥有相同级别并支付相同票价的人中，有16人从C出发，13人从S出发。 现在，由于支付类似票价的同一班级的大多数人来自C，而从C开始的人拥有最高的生存率，我们认为这些人可能从C开始。我们现在将他们的登船价值改为 C。 # Set Value trainDF = trainDF.set_value(trainDF['Embarked'].isnull(), 'Embarked','C') /Users/apple/Soft/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead Fare nulls现在来处理Fare列. 让我们想象一下从南安普敦出发的三等乘客支付的票价的直方图。 fig,ax = plt.subplots(figsize=(8,5)) testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].hist(bins=100, ax=ax) plt.xlabel('Fare') plt.ylabel('Frequency') plt.title('Histogram of Fare for Pclass = 3, Embarked = S' print (\"The top 5 most common fares:\") testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].value_counts().head() The top 5 most common fares: 8.0500 17 7.8958 10 7.7750 10 8.6625 8 7.8542 8 Name: Fare, dtype: int64 使用众数填充Fare列的缺失值, $8.05. # Fill value testDF.set_value(testDF.Fare.isnull(), 'Fare', 8.05) # Verify Age nulls现在让我们在训练和测试集中填写缺失的年龄数据。 填充的一种方法是用柱的方式填充NaN。 通过逐类查看平均年龄，我们可以使这个填充过程更加智能化。 plt.figure(figsize=(12,7)) sns.boxplot(x='Pclass', y='Age', data=trainDF) trainDF.groupby('Pclass')['Age'].mean() Pclass 1 38.233441 2 29.877630 3 25.140620 Name: Age, dtype: float64 我们看到等级越高，平均年龄就越高。 然后我们可以使用上述方法编写一个函数来填充NaN年龄值。 def fixNaNAge(age, pclass): if age == age: return age if pclass == 1: return 38 elif pclass == 2: return 30 else: return 25 现在，我们将在训练和测试数据框中填写年龄NaN，并验证它们是否正确填充。 trainDF['Age'] = trainDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']),axis=1) testDF['Age'] = testDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']), axis=1 fig = plt.figure(figsize=(15,5)) trainDF['Age'].astype(int).hist(bins=70) testDF['Age'].astype(int).hist(bins=70) facet = sns.FacetGrid(trainDF, hue='Survived', aspect=4) facet.map(sns.kdeplot, 'Age', shade=True) facet.set(xlim=(0, trainDF['Age'].max())) facet.add_legend() fig, ax = plt.subplots(1,1,figsize=(18,4)) age_mean = trainDF[['Age','Survived']].groupby(['Age'],as_index=False).mean() sns.barplot(x='Age', y='Survived', data=age_mean) Cabin nulls最后，对于机舱列，我们缺少太多信息来正确填充它，因此我们可以完全放弃该特征。 trainDF.drop('Cabin', axis=1,inplace=True) testDF.drop('Cabin', axis=1, inplace=True) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:5:1","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#age-nulls"},{"categories":["机器学习"],"content":" 缺失数据 Embarked Nulls首先，让我们在数据集中处理NaN,我们首先从Embarked中的NaN开始。 让我们根据登船港口来看待生存机会。 fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(15,5)) # Plot the number of occurances for each embarked location sns.countplot(x='Embarked', data=trainDF, ax=ax1) # Plot the number of people that survived by embarked location sns.countplot(x='Survived', hue = 'Embarked', data=trainDF, ax=ax2, order=[1,0]) # Group by Embarked, and get the mean for survived passengers for each # embarked location embark_pct = trainDF[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean() # Plot the above mean sns.barplot(x='Embarked',y='Survived', data=embark_pct, order=['S','C','Q'], ax=ax3) 在这里，我们可以看到大多数人从S出发，因此大多数幸存下来的人都是S.但是，当我们查看幸存人数的平均值与登机位置登记的总人数时， S的存活率最低。 这不足以确定上述人员登上哪个港口。 让我们看看其他可能表明乘客登船的变量 没有其他用户共享相同的票号。 让我们寻找支付类似票价的同一班级的人。 trainDF[(trainDF['Pclass'] == 1) \u0026 (trainDF['Fare'] \u003e 75) \u0026 (trainDF['Fare'] \u003c 85)].groupby('Embarked')['PassengerId'].count() Embarked C 16 S 13 Name: PassengerId, dtype: int64 在拥有相同级别并支付相同票价的人中，有16人从C出发，13人从S出发。 现在，由于支付类似票价的同一班级的大多数人来自C，而从C开始的人拥有最高的生存率，我们认为这些人可能从C开始。我们现在将他们的登船价值改为 C。 # Set Value trainDF = trainDF.set_value(trainDF['Embarked'].isnull(), 'Embarked','C') /Users/apple/Soft/miniconda3/lib/python3.5/site-packages/ipykernel_launcher.py:2: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead Fare nulls现在来处理Fare列. 让我们想象一下从南安普敦出发的三等乘客支付的票价的直方图。 fig,ax = plt.subplots(figsize=(8,5)) testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].hist(bins=100, ax=ax) plt.xlabel('Fare') plt.ylabel('Frequency') plt.title('Histogram of Fare for Pclass = 3, Embarked = S' print (\"The top 5 most common fares:\") testDF[(testDF['Pclass'] == 3) \u0026 (testDF['Embarked'] == 'S')]['Fare'].value_counts().head() The top 5 most common fares: 8.0500 17 7.8958 10 7.7750 10 8.6625 8 7.8542 8 Name: Fare, dtype: int64 使用众数填充Fare列的缺失值, $8.05. # Fill value testDF.set_value(testDF.Fare.isnull(), 'Fare', 8.05) # Verify Age nulls现在让我们在训练和测试集中填写缺失的年龄数据。 填充的一种方法是用柱的方式填充NaN。 通过逐类查看平均年龄，我们可以使这个填充过程更加智能化。 plt.figure(figsize=(12,7)) sns.boxplot(x='Pclass', y='Age', data=trainDF) trainDF.groupby('Pclass')['Age'].mean() Pclass 1 38.233441 2 29.877630 3 25.140620 Name: Age, dtype: float64 我们看到等级越高，平均年龄就越高。 然后我们可以使用上述方法编写一个函数来填充NaN年龄值。 def fixNaNAge(age, pclass): if age == age: return age if pclass == 1: return 38 elif pclass == 2: return 30 else: return 25 现在，我们将在训练和测试数据框中填写年龄NaN，并验证它们是否正确填充。 trainDF['Age'] = trainDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']),axis=1) testDF['Age'] = testDF.apply(lambda row: fixNaNAge(row['Age'],row['Pclass']), axis=1 fig = plt.figure(figsize=(15,5)) trainDF['Age'].astype(int).hist(bins=70) testDF['Age'].astype(int).hist(bins=70) facet = sns.FacetGrid(trainDF, hue='Survived', aspect=4) facet.map(sns.kdeplot, 'Age', shade=True) facet.set(xlim=(0, trainDF['Age'].max())) facet.add_legend() fig, ax = plt.subplots(1,1,figsize=(18,4)) age_mean = trainDF[['Age','Survived']].groupby(['Age'],as_index=False).mean() sns.barplot(x='Age', y='Survived', data=age_mean) Cabin nulls最后，对于机舱列，我们缺少太多信息来正确填充它，因此我们可以完全放弃该特征。 trainDF.drop('Cabin', axis=1,inplace=True) testDF.drop('Cabin', axis=1, inplace=True) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:5:1","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#cabin-nulls"},{"categories":["机器学习"],"content":" Adding features这些名称的前缀在某些情况下表明了社会地位，这可能是事故幸存的重要因素。 Braund, Mr. Owen Harris Heikkinen, Miss. Laina Oliva y Ocana, Dona. Fermina Peter, Master. Michael J 提取乘客头衔并在名为Title的附加栏中引导他们 Title_Dictionary = { \"Capt\": \"Officer\", \"Col\": \"Officer\", \"Major\": \"Officer\", \"Jonkheer\": \"Nobel\", \"Don\": \"Nobel\", \"Sir\" : \"Nobel\", \"Dr\": \"Officer\", \"Rev\": \"Officer\", \"the Countess\": \"Nobel\", \"Dona\": \"Nobel\", \"Mme\": \"Mrs\", \"Mlle\": \"Miss\", \"Ms\": \"Mrs\", \"Mr\" : \"Mr\", \"Mrs\" : \"Mrs\", \"Miss\" : \"Miss\", \"Master\" : \"Master\", \"Lady\" : \"Nobel\" } trainDF['Title'] = trainDF['Name'].apply(lambda x: Title_Dictionary[x.split(',')[1].split('.')[0].strip()]) testDF['Title'] = testDF['Name'].apply(lambda x: Title_Dictionary[x.split(',')[1].split('.')[0].strip()]) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:5:2","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#adding-features"},{"categories":["机器学习"],"content":" Aggregating Features让我们添加一个字段FamilySize，它聚合字段中的信息，表明存在合作伙伴（Parch）或亲戚（Sibsp）。 trainDF['FamilySize'] = trainDF['SibSp'] + trainDF['Parch'] testDF['FamilySize'] = testDF['SibSp'] + testDF['Parch'] 乘客的性别是事故幸存的重要因素。 乘客的年龄也是如此（例如，给予妇女和儿童的优惠待遇）。 让我们介绍一个新功能，以考虑到乘客的性别和年龄。 def getPerson(passenger): age, sex = passenger return 'child' if age \u003c 16 else sex trainDF['Person'] = trainDF[['Age', 'Sex']].apply(getPerson, axis=1) testDF['Person'] = testDF[['Age', 'Sex']].apply(getPerson, axis=1) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:6:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#aggregating-features"},{"categories":["机器学习"],"content":" Dropping Useless Features现在让我们放弃那些不再感兴趣的特征，因为它们没有显示任何不可理解的特征，或者它们已经聚合到另一个特征中。 我们将要删除的特征是PassengerID, Name, Sex, Ticket, SibSp, Parch. features_to_drop = ['PassengerId','Name','Sex','Ticket','SibSp','Parch'] trainDF.drop(labels=features_to_drop, axis=1, inplace=True) testDF.drop(labels=features_to_drop, axis=1, inplace=True) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:7:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#dropping-useless-features"},{"categories":["机器学习"],"content":" Convert Categorical Variables分类变量需要转换为数值，因为scikit-learn仅将数值作为numpy数组中的输入。 我们可以使用数字表示分类值，但是这种编码意味着类别中值之间的有序关系。 为避免这种情况，我们可以使用虚拟变量对分类值进行编码。 我们拥有的四个分类特征是Pclass，Embarked，Title和Person。 在这4个中，可能没有Person，Title，Embarked的有序关系，所以我们将对这些进行热编码，同时对Pclass进行数字编码。 我们通过为每个要素类别创建一个特征来进行热编码。 如果原始要素属于该类别，则类别列的值将为1。 只有一个分类要素列可以具有1.删除一列也很常见，因为剩下的列隐含了它的值。 # Create dummy features for each categorical feature dummies_person_train = pd.get_dummies(trainDF['Person'], prefix='Person') dummies_embarked_train = pd.get_dummies(trainDF['Embarked'], prefix='Embarked') dummies_title_train = pd.get_dummies(trainDF['Title'], prefix='Title') # Add the new features to the dataframe via concating tempDF = pd.concat([trainDF, dummies_person_train, dummies_embarked_train, dummies_title_train], axis=1) # Drop the original categorical feature columns tempDF = tempDF.drop(['Person','Embarked','Title'],axis=1) # Drop one of each of the dummy variables because its value is implied # by the other dummy variable columns # E.g. if Person_male = 0, and Person_female = 0, then the person # is a child trainDF = tempDF.drop(['Person_child','Embarked_C','Title_Master'],axis=1) # Create dummy features for each categorical feature dummies_person_test = pd.get_dummies(testDF['Person'], prefix='Person') dummies_embarked_test = pd.get_dummies(testDF['Embarked'], prefix='Embarked') dummies_title_test = pd.get_dummies(testDF['Title'], prefix='Title') # Add the new features to the dataframe via concating tempDF = pd.concat([testDF, dummies_person_test, dummies_embarked_test, dummies_title_test], axis=1) # Drop the original categorical feature columns tempDF = tempDF.drop(['Person','Embarked','Title'],axis=1) # Drop one of each of the dummy variables because its value is implied # by the other dummy variable columns # E.g. if Person_male = 0, and Person_female = 0, then the person # is a child testDF = tempDF.drop(['Person_child','Embarked_C','Title_Master'],axis=1) ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:8:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#convert-categorical-variables"},{"categories":["机器学习"],"content":" 机器学习模型 现在在本节中，我们将训练几个模型并调整它们，看看我们是否可以提高性能。 调整模型的方法不止一种。 它通常是一种非常迭代的方法。 由于它是迭代的，我们通常需要单个数字评估指标来优化并在2个不同模型之间做出决策。 通常会有一个您正在优化的度量标准，然后是对其他度量标准的约束。 一些可能用到的评价指标: maximizing accuracy minimizing error maximizing roc 可能包含的约束: model size \u003c= 100 MB FPR \u003e= 75% inference time \u003c 1 second 如果一个模型满足约束条件并且在您尝试优化的度量标准中做得更好，则它比另一个模型更好。 此外，为了获得模型的广义性能，通常在训练集中进行交叉验证。 在这种情况下，我们只是最大限度地提高交叉验证的准确性。 ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:9:0","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#机器学习模型"},{"categories":["机器学习"],"content":" Seperate Data首先切分数据: X = trainDF.drop(['Survived'],axis=1) y = trainDF['Survived'] Let’s also create a dataframe to store our results from training: resultDF = pd.DataFrame(columns=['model','hyperparams','train_acc','train_std','test_acc','test_std']) df_idx = 0 ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:9:1","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#seperate-data"},{"categories":["机器学习"],"content":" Baseline Models为了开始这个，我们将训练一些基线模型（具有默认值的模型）以获得要击败的基线。 首先，我们将创建一个模型字典，以便我们可以循环并迭代地构建和评估它们。 models = { 'knn': KNeighborsClassifier(), 'log': LogisticRegression(solver='lbfgs'), 'dt': DecisionTreeClassifier(), 'rf': RandomForestClassifier(), 'ab': AdaBoostClassifier() } 现在让我们编写一个函数来运行给定模型和数据集的交叉验证。 def crossValidate(typ, model, X, y, hyperparams='default', verbose=0): global df_idx # Update model hyperparameters if given if hyperparams is not 'default': model.set_params(**hyperparams) # Initialize scaler class scaler = StandardScaler() # Get kFolds kfold = KFold(n_splits=10) kfold.get_n_splits(X) # Initialize storage vectors trainACC = np.zeros(10) testACC = np.zeros(10) np_idx = 0 # Loop through folds for train_idx, test_idx in kfold.split(X): X_train, X_test = X.values[train_idx], X.values[test_idx] y_train, y_test = y.values[train_idx], y.values[test_idx] # Scale data X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Fit to training set model.fit(X_train, y_train) # Make predictions on testing set y_train_pred = model.predict(X_train) y_test_pred = model.predict(X_test) # Compute training and testing accuracy trainACC[np_idx] = accuracy_score(y_train, y_train_pred)*100 testACC[np_idx] = accuracy_score(y_test, y_test_pred)*100 np_idx += 1 # Print fold accuracy if verbose level 2 if verbose == 2: print (\" Fold {}: Accuracy: {}%\".format(np_idx, round(testACC[-1],3))) # Print average accuracy if verbose level 1 if verbose == 1: print (\" Average Score: {}%({}%)\".format(round(np.mean(testACC),3),round(np.std(testACC),3))) # Update dataframe resultDF.loc[df_idx, 'model'] = typ resultDF.loc[df_idx, 'hyperparams'] = str(hyperparams) resultDF.loc[df_idx, 'train_acc'] = trainACC.mean() resultDF.loc[df_idx, 'train_std'] = trainACC.std() resultDF.loc[df_idx, 'test_acc'] = testACC.mean() resultDF.loc[df_idx, 'test_std'] = testACC.std() df_idx += 1 # Return average testing accuracy, and fitted model return testACC.mean(), model 现在我们可以使用上面的函数循环并交叉验证每个模型。 for name, m in models.items(): print (\"Fitting \" + name + \" model\") _, models[name] = crossValidate(name, m, X, y, 'default', 1) Fitting dt model Average Score: 79.357%(4.778%) Fitting rf model Average Score: 80.923%(4.192%) Fitting ab model Average Score: 80.7%(3.331%) Fitting knn model Average Score: 80.7%(3.136%) Fitting log model Average Score: 82.27%(3.495%) 我们还可以查看我们为存储值而创建的数据框中的所有结果。 hyperparams train_acc train_std test_acc test_std model log default 83.002902 0.355647 82.269663 3.495014 rf default 96.919733 0.511827 80.922597 4.192015 ab default 84.249987 0.716843 80.700375 3.330767 knn default 86.619375 0.436454 80.700375 3.135529 dt default 98.428741 0.138747 79.357054 4.777630 ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:9:2","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#baseline-models"},{"categories":["机器学习"],"content":" Plot ROC curve for baseline models我们现在可以使用以下函数绘制我们构建的每个模型的roc曲线。 def plot_roc_curve(name, model, X, y): # Initialize scaler scaler = StandardScaler() # Get kfolds kfold = KFold(n_splits=10) kfold.get_n_splits(X) # Initalize storage lists tprs = [] aucs = [] mean_fpr = np.linspace(0, 1, 100) i = 0 # Loop through folds for train_idx, test_idx in kfold.split(X): # Split data X_train, X_test = X.values[train_idx], X.values[test_idx] y_train, y_test = y.values[train_idx], y.values[test_idx] # Scale data X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Fit model to fold model.fit(X_train, y_train) # Get probabilities for X_test y_test_proba = model.predict_proba(X_test) # Get FPR, TPR, and AUC vals based on probabilities fpr, tpr, _ = roc_curve(y_test, y_test_proba[:,1]) roc_auc = auc(fpr, tpr) # Append tpr and auc tprs.append(interp(mean_fpr, fpr, tpr)) tprs[-1][0] = 0.0 aucs.append(roc_auc) # Plot roc for fold plt.plot(fpr, tpr, lw=1, alpha=0.3, label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc)) i += 1 # Plot random guessing line plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8) # Get mean tpr and mean/std for auc mean_tpr = np.mean(tprs, axis=0) mean_tpr[-1] = 1.0 mean_auc = auc(mean_fpr, mean_tpr) std_auc = np.std(aucs) # Plot mean curve plt.plot(mean_fpr, mean_tpr, color='b', label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc), lw=2, alpha=.8) # Fill area between plots std_tpr = np.std(tprs, axis=0) tprs_upper = np.minimum(mean_tpr + std_tpr, 1) tprs_lower = np.maximum(mean_tpr - std_tpr, 0) plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2, label=r'$\\pm$ 1 std. dev.') # style plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title(name + ' Receiver operating characteristic curve') plt.legend(loc=\"lower right\") return mean_fpr, mean_tpr, mean_auc, std_auc 现在，对于我们创建的每个模型，我们将循环并绘制每个折叠的ROC曲线+平均ROC曲线。 我们还将在同一图上绘制所有模型的平均ROC曲线以进行比较 plt_id = 1 plt.figure(figsize=(15,15)) for name, m in models.items(): ax = plt.subplot(2,3,plt_id) fpr, tpr, auc_mean, auc_std = plot_roc_curve(name, m, X, y) plt_id+=1 ax = plt.subplot(2,3,6) plt.plot(fpr, tpr, label=name + r' Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (auc_mean, auc_std), lw=2, alpha=.8) plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Overall Receiver operating characteristic curve') plt.legend(loc=\"lower right\") ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:9:3","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#plot-roc-curve-for-baseline-models"},{"categories":["机器学习"],"content":" Hyperparameter Tuning – Improve model performance 现在我们已经构建了基线模型，我们现在可以尝试进行超参数调整，以构建一个模型，该模型在我们选择的度量标准上的开发集上做得更好（在这种情况下，使用acc进行交叉验证）。 进行超参数调整的3种常见方法是： Discrete Grid Search Randomized Grid Search Coarse-to-fine grid search Discrete Grid Search 创建一个与超级参数的数量相对应的n空间网格，以便调整并尝试每种组合 可以构建最佳模型 可能需要很长时间才能尝试每种组合 超参数的重要性不相同。 Randomized Grid Search 随机尝试一组参数 有利于解决不相等的超参数 证明可以非常快速地获得次优结果 Coarse-to-fine grid search 这可以与随机/离散网格搜索结合使用 从粗网格开始，迭代地转到更精细的网格以缩小结果范围。 ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:9:4","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#hyperparameter-tuning----improve-model-performance"},{"categories":["机器学习"],"content":" Randomized Grid Search下面我们将展示随机网格搜索技术的一个例子。 以下技术明确编码以供参考。 这在sklearn中打包为RandomizedGridSearchCV。 首先，我们将编写一个函数来应用随机网格搜索给定模型，一组超参数和数据。 def gridSearch(name, models, param_grids, X, y, num_trials=100, verbose=0): # Get model and param grid model = models[name] current_param_grid = param_grids[name] # Create variable to store best model best_model = model # Loop through trials for nTrial in range(num_trials): if verbose == 2 and nTrial % 10 == 0: print (' Trial: %d' % (nTrial)) # Get current best accuracy for model from the results dataframe best_acc = resultDF[resultDF['model'] == name]['test_acc'].max() # Randomly select params from grid params = {} for k, v in current_param_grid.items(): params[k] = np.random.choice(v) # Cross validate model with selected hyperparams using the function we generated earlier acc,model = crossValidate(name, model, X, y, params, 0) # Update best model if it satisfies our optimizing metric if acc \u003e best_acc: if verbose == 1: print (' New best ' + name + ' model: ' + str(acc)) best_model = model # Return best model return best_model 现在让我们指定一组要循环的参数。 请记住考虑基本规模。 例如，对于学习率，更理想的是以对数标度（0.001,0.01,0.1等）与线性标度（0.001,0.002,0.003,0.004等）进行网格搜索。 param_grids = {} param_grids['knn'] = {\"n_neighbors\": np.arange(1,11,1)} param_grids['log'] = {'C': [0.001,0.01,0.1,1,10,100], 'solver':['newton-cg','lbfgs','liblinear','sag']} param_grids['dt'] = {'criterion': ['gini','entropy'], 'max_depth': np.arange(1,6,1), 'min_samples_split': np.arange(3,10,1), 'max_features': np.arange(1,6,1) } param_grids['rf'] = {'n_estimators': [int(x) for x in np.arange(10,200,10)], 'max_features': ['auto','sqrt','log2'], 'max_depth': [int(x) for x in np.arange(1,5)] + [None], 'min_samples_split': [2,5,10], 'min_samples_leaf': [1,2,4], 'bootstrap': [True,False] } param_grids['ab'] = {'n_estimators': [int(x) for x in np.arange(10,200,10)], 'learning_rate': [0.01,0.1,1,2]} 现在使用以上的参数来对模型进行调优 for name in models.keys(): print (name) models[name] = gridSearch(name, models, param_grids, X, y, 50, 1) dt New best dt model: 82.4918851436 New best dt model: 82.7153558052 New best dt model: 82.7166042447 New best dt model: 82.8302122347 rf New best rf model: 82.4906367041 New best rf model: 83.2784019975 New best rf model: 83.5043695381 New best rf model: 84.1797752809 ab New best ab model: 81.0349563046 New best ab model: 82.3807740325 New best ab model: 82.9413233458 knn New best knn model: 82.0486891386 New best knn model: 82.1610486891 log New best log model: 82.8289637953 我们还可以在结果数据框中查看当前最佳结果。 hyperparams train_acc train_std test_acc test_std model rf {’n_estimators’: 90, ‘min_samples_leaf’: 2, ' 88.0534 0.3362 84.1797 4.2544 ab {’n_estimators’: 30, ’learning_rate’: 0.10. 83.0777 0.4232 82.9413 4.2242 dt {‘max_features’: 5, ‘criterion’: ’entropy’, 84.3373 0.4706 82.8302 2.2328 log {‘solver’: ‘sag’, ‘C’: 0.10000000000000001} 83.0153 0.3921 82.8289 3.9226 knn {’n_neighbors’: 8} 85.0729 0.5309 82.1610 4.3016 ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:9:5","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#randomized-grid-search"},{"categories":["机器学习"],"content":" Plot ROC curve for tuned models我们现在可以使用上面的函数绘制我们调整的每个模型的roc曲线。 plt_id = 1 plt.figure(figsize=(15,15)) for name, m in models.items(): ax = plt.subplot(2,3,plt_id) fpr, tpr, auc_mean, auc_std = plot_roc_curve(name, m, X, y) plt_id+=1 ax = plt.subplot(2,3,6) plt.plot(fpr, tpr, label=name + r' Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (auc_mean, auc_std), lw=2, alpha=.8) plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Overall Receiver operating characteristic curve') plt.legend(loc=\"lower right\") ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:9:6","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#plot-roc-curve-for-tuned-models"},{"categories":["机器学习"],"content":" Evaluating a new model使用上述函数，我们可以轻松地在我们的数据集上评估另一个模型。 例如，让我们在我们的数据集上评估一个神经网络。 # build baseline model models['nn'] = MLPClassifier((32, 32), early_stopping=False) _, models['nn'] = crossValidate('nn', models['nn'], X, y, 'default', 1) Average Score: 82.045%(3.242%) # randomized grid search to tune param_grids['nn'] = {'alpha': [1e-4,1e-3,1e-2,1e-1], 'hidden_layer_sizes': [(20,10),(80,10),(10,20,20,10), (10,10,10),(64,64,64),(64,64,64,64), (16,32,64,64,32,16),(32,32,32,32,32,32)]} models['nn'] = gridSearch('nn', models, param_grids, X, y, 50, 1) New best nn model: 82.1585518102 New best nn model: 82.4918851436 New best nn model: 82.4981273408 New best nn model: 82.6092384519 经过调节超参数的结果后： hyperparams train_acc train_std test_acc test_std model rf {’n_estimators’: 90, ‘min_samples_leaf’: 2, . 88.0534 0.3362 84.1797 4.2544 ab {’n_estimators’: 30, ’learning_rate’: 0.10.. 83.0777 0.4232 82.9413 4.2242 dt {‘max_features’: 5, ‘criterion’: ’entropy’, . 84.3373 0.4706 82.8302 2.2328 log {‘solver’: ‘sag’, ‘C’: 0.10000000000000001} 83.0153 0.3921 82.8289 3.9226 nn {‘hidden_layer_sizes’: (80, 10), ‘alpha’: … 83.0029 0.7526 82.6092 3.7357 knn {’n_neighbors’: 8} 85.0729 0.5309 82.1610 4.3016 # View updated roc curves plt_id = 1 plt.figure(figsize=(15,15)) for name, m in models.items(): ax = plt.subplot(3,3,plt_id) fpr, tpr, auc_mean, auc_std = plot_roc_curve(name, m, X, y) plt_id+=1 ax = plt.subplot(3,3,8) plt.plot(fpr, tpr, label=name + r' Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (auc_mean, auc_std), lw=2, alpha=.8) plt.xlim([-0.05, 1.05]) plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Overall Receiver operating characteristic curve') plt.legend(loc=\"lower right\") ","date":"2019-01-06","objectID":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/:9:7","series":null,"tags":["机器学习","python"],"title":"机器学习案例之Titanic生存预测分析","uri":"/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E4%B9%8Btitanic%E7%94%9F%E5%AD%98%E9%A2%84%E6%B5%8B%E5%88%86%E6%9E%90/#evaluating-a-new-model"},{"categories":["深度学习"],"content":" 为什么选择PyTorch 简洁：PyTorch的设计追求最少的封装，尽量避免重复造轮子。不像TensorFlow中充斥着session、graph、operation、name_scope、variable、tensor、layer等全新的概念，PyTorch的设计遵循tensor→autograd→nn.Module 三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），而且这三个抽象之间联系紧密，可以同时进行修改和操作。 速度：PyTorch的灵活性不以速度为代价，在许多评测中，PyTorch的速度表现胜过TensorFlow和Keras等框架 。框架的运行速度和程序员的编码水平有极大关系，但同样的算法，使用PyTorch实现的那个更有可能快过用其他框架实现的。 易用：PyTorch是所有的框架中面向对象设计的最优雅的一个。PyTorch的面向对象的接口设计来源于Torch，而Torch的接口设计以灵活易用而著称，Keras作者最初就是受Torch的启发才开发了Keras。PyTorch继承了Torch的衣钵，尤其是API的设计和模块的接口都与Torch高度一致。PyTorch的设计最符合人们的思维，它让用户尽可能地专注于实现自己的想法，即所思即所得，不需要考虑太多关于框架本身的束缚。 活跃的社区：PyTorch提供了完整的文档，循序渐进的指南，作者亲自维护的论坛 供用户交流和求教问题。Facebook 人工智能研究院对PyTorch提供了强力支持，作为当今排名前三的深度学习研究机构，FAIR的支持足以确保PyTorch获得持续的开发更新，不至于像许多由个人开发的框架那样昙花一现。 PyTorch还有一个优点就是Torch自称为神经网络界的Numpy，它能将torch产生的tensor放在GPU中加速运算，就想Numpy会把array放在CPU中加速运算。所以在神经网络中，用Torch的tensor形式更优。我们可以把Pytorch当做Numpy来用。 PyTorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建，所以它能够使用Python控制语句（如for、if等）根据需求创建计算图。这点在自然语言处理领域中很有用，它意味着你不需要事先构建所有可能用到的图的路径，图在运行时才构建。 ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:1","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#为什么选择pytorch"},{"categories":["深度学习"],"content":" PyTorch的安装 pip安装方式 conda安装方式 # win+python3.6 pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-win_amd64.whl pip3 install torchvision # win+python3.6+conda conda install pytorch torchvision cuda80 -c pytorch 更多方法参见此处 ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:2","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#pytorch的安装"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#pytorch的核心概念"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor张量"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor属性"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#torchdtype"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#torchdevice"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#torchlayout"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor方法"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensorcopy_src-asyncfalse"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensorcudadevicenone-asyncfalse"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#heading"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensorexpandsizes"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensornarrowdimension-start-length"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensorresize_sizes"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#详解tensor操作"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#创建tensor"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor的基本操作"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor索引操作"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor元素操作"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor归并操作"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor比较操作"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor线性代数"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor广播法则"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#tensor内部结构"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#其他tensor使用技巧"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#gpucpu"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#持久化"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#autograd自动微分"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#variable"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#autograd常用方法"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#torchautogradbackwardtensors-grad_tensorsnone-retain_graphnone-create_graphfalse-grad_variablesnone"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#torchautogradgradoutputs-inputs-grad_outputsnone-retain_graphnone-create_graphfalse-only_inputstrue-allow_unusedfalse"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#计算图"},{"categories":["深度学习"],"content":" PyTorch的核心概念 Tensor：张量 Tensor是PyTorch中重要的数据结构，可认为是一个高维数组。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）以及更高维的数组。 Tensor和Numpy的ndarrays类似，但Tensor可以使用GPU进行加速。Tensor的使用和Numpy及Matlab的接口十分相似。 torch.Tensor是一种包含单一数据类型元素的多维矩阵。 Tensor属性 每个torch.Tensor都有torch.dtype, torch.device,和torch.layout。 torch.dtype Torch定义了七种CPU张量类型和八种GPU张量类型： Data tyoe CPU tensor GPU tensor 32-bit floating point torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point N/A torch.cuda.HalfTensor 8-bit integer (unsigned) torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.LongTensor torch.cuda.LongTensor torch.device torch.device代表将torch.Tensor分配到的设备的对象。 torch.device包含一个设备类型（'cpu'或'cuda'设备类型）和可选的设备的序号。如果设备序号不存在，则为当前设备; 例如，torch.Tensor用设备构建'cuda'的结果等同于'cuda:X',其中X是torch.cuda.current_device()的结果。 torch.Tensor的设备可以通过Tensor.device访问属性。 构造torch.device可以通过字符串/字符串和设备编号。 torch.device('cuda:0') # device(type='cuda', index=0) torch.device('cpu') # device(type='cpu') torch.device('cuda', 0) # device(type='cuda', index=0) 注意 torch.device函数中的参数通常可以用一个字符串替代。这允许使用代码快速构建原型。 # Example of a function that takes in a torch.device cuda1 = torch.device('cuda:1') torch.randn((2,3), device=cuda1) # You can substitute the torch.device with a string torch.randn((2,3), 'cuda:1') torch.layout torch.layout表示torch.Tensor内存布局的对象。目前，我们支持torch.strided(dense Tensors)并为torch.sparse_coo(sparse COO Tensors)提供实验支持。 torch.strided代表密集张量，是最常用的内存布局。每个strided张量都会关联 一个torch.Storage，它保存着它的数据。这些张力提供了多维度， 存储的strided视图。Strides是一个整数型列表：k-th stride表示在张量的第k维从一个元素跳转到下一个元素所需的内存。这个概念使得可以有效地执行多张量。 x = torch.Tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]) x.stride() # (5, 1) x.t().stride() # (1, 5) Tensor方法 Tensor的方法中，带有_的方法代表能够修改Tensor本身。比如，torch.FloatTensor.abs_()会在原地计算绝对值并返回修改的张量，而tensor.FloatTensor.abs()将会在新张量中计算结果。 Tensor.copy_(src, async=False) 将src中的元素复制到tensor中并返回这个tensor。 如果broadcast是True，则源张量必须可以使用该张量广播。否则两个tensor应该有相同数目的元素，可以是不同的数据类型或存储在不同的设备上。 参数： src（Tensor） - 要复制的源张量 async（bool） - 如果为True，并且此副本位于CPU和GPU之间，则副本可能会相对于主机异步发生。对于其他副本，此参数无效。 broadcast（bool） - 如果为True，src将广播到底层张量的形状。 Tensor.cuda(device=None, async=False) 返回此对象在CPU内存中的一个副本 如果该对象已经在CUDA内存中，并且在正确的设备上，则不会执行任何副本，并返回原始对象。 参数： device（int） ：目标GPU ID。默认为当前设备。 async（bool） ：如果为True并且源处于固定内存中，则该副本将相对于主机是异步的。否则，该参数没有意义。 Tensor.expand(*sizes) 返回tensor的一个新视图，单个维度扩大为更大的尺寸。 tensor也可以扩大为更高维，新增加的维度将附在前面。 扩大tensor不需要分配新内存，只是仅仅新建一个tensor的视图，其中通过将stride设为0，一维将会扩展位更高维。任何一个一维的在不分配新内存情况下可扩展为任意的数值。 参数： sizes(torch.Size or int…)-需要扩展的大小 Tensor.narrow(*dimension, start, length*) 返回这个张量的缩小版本的新张量。维度dim缩小范围是start到start+length。返回的张量和该张量共享相同的底层存储。 参数： dimension (int)-需要缩小的维度 start (int)-起始维度 length (int)-长度 Tensor.resize_(*sizes) 将tensor的大小调整为指定的大小。如果元素个数比当前的内存大小大，就将底层存储大小调整为与新元素数目一致的大小。如果元素个数比当前内存小，则底层存储不会被改变。原来tensor中被保存下来的元素将保持不变，但新内存将不会被初始化。 参数： sizes (torch.Size or int…)-需要调整的大小 更多的方法参考此处 下面通过一些实例学习，Tensor的使用。 # 构建 5x3 矩阵，只是分配了空间，未初始化 x = t.Tensor(5, 3) x = t.Tensor([[1,2],[3,4]]) # tensor([[ 1., 2.], # [ 3., 4.]]) # 使用[0,1]均匀分布随机初始化二维数组 x = t.rand(5, 3) # tensor([[ 0.8052, 0.7188, 0.0332], # [ 0.6054, 0.8955, 0.8972], # [ 0.1107, 0.3319, 0.0336], # [ 0.2394, 0.5188, 0.2201], # [ 0.9730, 0.9370, 0.5677]]) x.size()[1], x.size(1) # 查看列的个数, 两种写法等价 # (3,3) torch.Size 是tuple对象的子类，因此它支持tuple的所有操作，如x.size()[0] # 加减乘除运算 y = t.rand(5, 3) # 加法的第一种写法 x + y # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第二种写法 t.add(x, y) # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, 1.3919], # [ 0.7139, 0.6348, 0.8439], # [ 0.7022, 1.5079, 0.4776], # [ 1.7892, 1.6383, 0.7774]]) # 加法的第三种写法：指定加法结果的输出目标为result result = t.Tensor(5, 3) # 预先分配空间 t.add(x, y, out=result) # 输入到result # tensor([[ 0.9639, 0.8763, 0.2834], # [ 1.3785, 1.5090, ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:3","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#autograd高级用法"},{"categories":["深度学习"],"content":" Pytorch实例线性回归 三种方法实现线性回归，第一种：自动计算导数，第二种：使用autograd来计算导数，第三种：使用优化器自动优化 线性回归0 import torch as t %matplotlib inline from matplotlib import pyplot as plt from matplotlib import style from IPython import display style.use(\"ggplot\") device = t.device('cuda') #如果你想用gpu，改成t.device('cuda:0') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1, 1).to(device) b = t.zeros(1, 1).to(device) lr =0.02 # 学习率 num_epochs = 500 # 训练500次 for ii in range(num_epochs): # 获取数据 x, y = get_fake_data(batch_size=4) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);for python3 only loss = 0.5 * (y_pred - y) ** 2 # 均方误差 loss = loss.mean() # backward：手动计算梯度 # 模拟计算图的方式计算误差 dloss = 1 dy_pred = dloss * (y_pred - y) dw = x.t().mm(dy_pred) db = dy_pred.sum() # 更新参数 w.sub_(lr * dw) b.sub_(lr * db) if ii%50 ==0: # 画图 # 使用display实现动态图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1).to(device) y = x.mm(w) + b.expand_as(x) plt.plot(x.cpu().numpy(), y.cpu().numpy(),c = 'green') # predicted x2, y2 = get_fake_data(batch_size=32) plt.scatter(x2.cpu().numpy(), y2.cpu().numpy()) # true data plt.xlim(0, 5) plt.ylim(0, 13) plt.show() plt.pause(0.5) print('w: ', w.item(), 'b: ', b.item()) 线性回归1 import torch as t %matplotlib inline from matplotlib import pyplot as plt from matplotlib import style from IPython import display style.use(\"ggplot\") # 注意，此处如果使用cuda，那么计算梯度的时候，会出错 device = t.device('cpu') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1, 1).to(device) b = t.zeros(1, 1).to(device) lr =0.02 # 学习率 num_epochs = 500 # 训练500次 for ii in range(num_epochs): x, y = get_fake_data(batch_size=32) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) loss = 0.5 * (y_pred - y) ** 2 loss = loss.sum() losses[ii] = loss.item() # backward：手动计算梯度 loss.backward() # 更新参数 w.data.sub_(lr * w.grad.data) b.data.sub_(lr * b.grad.data) # 梯度清零 w.grad.data.zero_() b.grad.data.zero_() if ii % 10 ==0: # 画图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1) y = x.mm(w.data) + b.data.expand_as(x) plt.plot(x.numpy(), y.numpy(),c = 'blue') # predicted print(w.item(),b.item()) x2, y2 = get_fake_data(batch_size=20) plt.scatter(x2.numpy(), y2.numpy()) # true data plt.xlim(0,5) plt.ylim(0,13) plt.show() plt.pause(0.5) print(w.item(), b.item()) # 1.9642277956008911 3.0166714191436768 线性回归2 import torch as t %matplotlib inline from matplotlib import pyplot as plt from torch import optim from torch import nn from matplotlib import style from IPython import display style.use(\"ggplot\") # 不知道为啥使用cuda梯度为不存在了 device = t.device('cpu') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1,1, requires_grad=True).to(device) b = t.zeros(1,1, requires_grad=True).to(device) lr = 0.0005 # 学习率不能太大 num_epochs = 500 losses = np.zeros(500) # 导入优化器 opt = optim.SGD([w,b],lr = lr) # 引入mse损失函数 loss_func = nn.MSELoss() for ii in range(num_epochs): x, y = get_fake_data(batch_size=32) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) loss = loss_func(y_pred,y) losses[ii] = loss.item() # 优化器梯度清零 opt.zero_grad() # backward：手动计算梯度 loss.backward() # 逐步优化 opt.step() if ii % 10 == 0: # 画图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1) y = x.mm(w.data) + b.data.expand_as(x) plt.plot(x.numpy(), y.numpy(),c = 'blue') # predicted print(w.item(),b.item()) x2, y2 = get_fake_data(batch_size=20) plt.scatter(x2.numpy(), y2.numpy()) # true data plt.xlim(0,5) plt.ylim(0,13) plt.show() plt.pause(0.5) print(w.item(), b.item","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:4","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#pytorch实例线性回归"},{"categories":["深度学习"],"content":" Pytorch实例线性回归 三种方法实现线性回归，第一种：自动计算导数，第二种：使用autograd来计算导数，第三种：使用优化器自动优化 线性回归0 import torch as t %matplotlib inline from matplotlib import pyplot as plt from matplotlib import style from IPython import display style.use(\"ggplot\") device = t.device('cuda') #如果你想用gpu，改成t.device('cuda:0') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1, 1).to(device) b = t.zeros(1, 1).to(device) lr =0.02 # 学习率 num_epochs = 500 # 训练500次 for ii in range(num_epochs): # 获取数据 x, y = get_fake_data(batch_size=4) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);for python3 only loss = 0.5 * (y_pred - y) ** 2 # 均方误差 loss = loss.mean() # backward：手动计算梯度 # 模拟计算图的方式计算误差 dloss = 1 dy_pred = dloss * (y_pred - y) dw = x.t().mm(dy_pred) db = dy_pred.sum() # 更新参数 w.sub_(lr * dw) b.sub_(lr * db) if ii%50 ==0: # 画图 # 使用display实现动态图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1).to(device) y = x.mm(w) + b.expand_as(x) plt.plot(x.cpu().numpy(), y.cpu().numpy(),c = 'green') # predicted x2, y2 = get_fake_data(batch_size=32) plt.scatter(x2.cpu().numpy(), y2.cpu().numpy()) # true data plt.xlim(0, 5) plt.ylim(0, 13) plt.show() plt.pause(0.5) print('w: ', w.item(), 'b: ', b.item()) 线性回归1 import torch as t %matplotlib inline from matplotlib import pyplot as plt from matplotlib import style from IPython import display style.use(\"ggplot\") # 注意，此处如果使用cuda，那么计算梯度的时候，会出错 device = t.device('cpu') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1, 1).to(device) b = t.zeros(1, 1).to(device) lr =0.02 # 学习率 num_epochs = 500 # 训练500次 for ii in range(num_epochs): x, y = get_fake_data(batch_size=32) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) loss = 0.5 * (y_pred - y) ** 2 loss = loss.sum() losses[ii] = loss.item() # backward：手动计算梯度 loss.backward() # 更新参数 w.data.sub_(lr * w.grad.data) b.data.sub_(lr * b.grad.data) # 梯度清零 w.grad.data.zero_() b.grad.data.zero_() if ii % 10 ==0: # 画图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1) y = x.mm(w.data) + b.data.expand_as(x) plt.plot(x.numpy(), y.numpy(),c = 'blue') # predicted print(w.item(),b.item()) x2, y2 = get_fake_data(batch_size=20) plt.scatter(x2.numpy(), y2.numpy()) # true data plt.xlim(0,5) plt.ylim(0,13) plt.show() plt.pause(0.5) print(w.item(), b.item()) # 1.9642277956008911 3.0166714191436768 线性回归2 import torch as t %matplotlib inline from matplotlib import pyplot as plt from torch import optim from torch import nn from matplotlib import style from IPython import display style.use(\"ggplot\") # 不知道为啥使用cuda梯度为不存在了 device = t.device('cpu') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1,1, requires_grad=True).to(device) b = t.zeros(1,1, requires_grad=True).to(device) lr = 0.0005 # 学习率不能太大 num_epochs = 500 losses = np.zeros(500) # 导入优化器 opt = optim.SGD([w,b],lr = lr) # 引入mse损失函数 loss_func = nn.MSELoss() for ii in range(num_epochs): x, y = get_fake_data(batch_size=32) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) loss = loss_func(y_pred,y) losses[ii] = loss.item() # 优化器梯度清零 opt.zero_grad() # backward：手动计算梯度 loss.backward() # 逐步优化 opt.step() if ii % 10 == 0: # 画图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1) y = x.mm(w.data) + b.data.expand_as(x) plt.plot(x.numpy(), y.numpy(),c = 'blue') # predicted print(w.item(),b.item()) x2, y2 = get_fake_data(batch_size=20) plt.scatter(x2.numpy(), y2.numpy()) # true data plt.xlim(0,5) plt.ylim(0,13) plt.show() plt.pause(0.5) print(w.item(), b.item","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:4","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#线性回归0"},{"categories":["深度学习"],"content":" Pytorch实例线性回归 三种方法实现线性回归，第一种：自动计算导数，第二种：使用autograd来计算导数，第三种：使用优化器自动优化 线性回归0 import torch as t %matplotlib inline from matplotlib import pyplot as plt from matplotlib import style from IPython import display style.use(\"ggplot\") device = t.device('cuda') #如果你想用gpu，改成t.device('cuda:0') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1, 1).to(device) b = t.zeros(1, 1).to(device) lr =0.02 # 学习率 num_epochs = 500 # 训练500次 for ii in range(num_epochs): # 获取数据 x, y = get_fake_data(batch_size=4) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);for python3 only loss = 0.5 * (y_pred - y) ** 2 # 均方误差 loss = loss.mean() # backward：手动计算梯度 # 模拟计算图的方式计算误差 dloss = 1 dy_pred = dloss * (y_pred - y) dw = x.t().mm(dy_pred) db = dy_pred.sum() # 更新参数 w.sub_(lr * dw) b.sub_(lr * db) if ii%50 ==0: # 画图 # 使用display实现动态图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1).to(device) y = x.mm(w) + b.expand_as(x) plt.plot(x.cpu().numpy(), y.cpu().numpy(),c = 'green') # predicted x2, y2 = get_fake_data(batch_size=32) plt.scatter(x2.cpu().numpy(), y2.cpu().numpy()) # true data plt.xlim(0, 5) plt.ylim(0, 13) plt.show() plt.pause(0.5) print('w: ', w.item(), 'b: ', b.item()) 线性回归1 import torch as t %matplotlib inline from matplotlib import pyplot as plt from matplotlib import style from IPython import display style.use(\"ggplot\") # 注意，此处如果使用cuda，那么计算梯度的时候，会出错 device = t.device('cpu') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1, 1).to(device) b = t.zeros(1, 1).to(device) lr =0.02 # 学习率 num_epochs = 500 # 训练500次 for ii in range(num_epochs): x, y = get_fake_data(batch_size=32) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) loss = 0.5 * (y_pred - y) ** 2 loss = loss.sum() losses[ii] = loss.item() # backward：手动计算梯度 loss.backward() # 更新参数 w.data.sub_(lr * w.grad.data) b.data.sub_(lr * b.grad.data) # 梯度清零 w.grad.data.zero_() b.grad.data.zero_() if ii % 10 ==0: # 画图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1) y = x.mm(w.data) + b.data.expand_as(x) plt.plot(x.numpy(), y.numpy(),c = 'blue') # predicted print(w.item(),b.item()) x2, y2 = get_fake_data(batch_size=20) plt.scatter(x2.numpy(), y2.numpy()) # true data plt.xlim(0,5) plt.ylim(0,13) plt.show() plt.pause(0.5) print(w.item(), b.item()) # 1.9642277956008911 3.0166714191436768 线性回归2 import torch as t %matplotlib inline from matplotlib import pyplot as plt from torch import optim from torch import nn from matplotlib import style from IPython import display style.use(\"ggplot\") # 不知道为啥使用cuda梯度为不存在了 device = t.device('cpu') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1,1, requires_grad=True).to(device) b = t.zeros(1,1, requires_grad=True).to(device) lr = 0.0005 # 学习率不能太大 num_epochs = 500 losses = np.zeros(500) # 导入优化器 opt = optim.SGD([w,b],lr = lr) # 引入mse损失函数 loss_func = nn.MSELoss() for ii in range(num_epochs): x, y = get_fake_data(batch_size=32) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) loss = loss_func(y_pred,y) losses[ii] = loss.item() # 优化器梯度清零 opt.zero_grad() # backward：手动计算梯度 loss.backward() # 逐步优化 opt.step() if ii % 10 == 0: # 画图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1) y = x.mm(w.data) + b.data.expand_as(x) plt.plot(x.numpy(), y.numpy(),c = 'blue') # predicted print(w.item(),b.item()) x2, y2 = get_fake_data(batch_size=20) plt.scatter(x2.numpy(), y2.numpy()) # true data plt.xlim(0,5) plt.ylim(0,13) plt.show() plt.pause(0.5) print(w.item(), b.item","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:4","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#线性回归1"},{"categories":["深度学习"],"content":" Pytorch实例线性回归 三种方法实现线性回归，第一种：自动计算导数，第二种：使用autograd来计算导数，第三种：使用优化器自动优化 线性回归0 import torch as t %matplotlib inline from matplotlib import pyplot as plt from matplotlib import style from IPython import display style.use(\"ggplot\") device = t.device('cuda') #如果你想用gpu，改成t.device('cuda:0') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1, 1).to(device) b = t.zeros(1, 1).to(device) lr =0.02 # 学习率 num_epochs = 500 # 训练500次 for ii in range(num_epochs): # 获取数据 x, y = get_fake_data(batch_size=4) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);for python3 only loss = 0.5 * (y_pred - y) ** 2 # 均方误差 loss = loss.mean() # backward：手动计算梯度 # 模拟计算图的方式计算误差 dloss = 1 dy_pred = dloss * (y_pred - y) dw = x.t().mm(dy_pred) db = dy_pred.sum() # 更新参数 w.sub_(lr * dw) b.sub_(lr * db) if ii%50 ==0: # 画图 # 使用display实现动态图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1).to(device) y = x.mm(w) + b.expand_as(x) plt.plot(x.cpu().numpy(), y.cpu().numpy(),c = 'green') # predicted x2, y2 = get_fake_data(batch_size=32) plt.scatter(x2.cpu().numpy(), y2.cpu().numpy()) # true data plt.xlim(0, 5) plt.ylim(0, 13) plt.show() plt.pause(0.5) print('w: ', w.item(), 'b: ', b.item()) 线性回归1 import torch as t %matplotlib inline from matplotlib import pyplot as plt from matplotlib import style from IPython import display style.use(\"ggplot\") # 注意，此处如果使用cuda，那么计算梯度的时候，会出错 device = t.device('cpu') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1, 1).to(device) b = t.zeros(1, 1).to(device) lr =0.02 # 学习率 num_epochs = 500 # 训练500次 for ii in range(num_epochs): x, y = get_fake_data(batch_size=32) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) loss = 0.5 * (y_pred - y) ** 2 loss = loss.sum() losses[ii] = loss.item() # backward：手动计算梯度 loss.backward() # 更新参数 w.data.sub_(lr * w.grad.data) b.data.sub_(lr * b.grad.data) # 梯度清零 w.grad.data.zero_() b.grad.data.zero_() if ii % 10 ==0: # 画图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1) y = x.mm(w.data) + b.data.expand_as(x) plt.plot(x.numpy(), y.numpy(),c = 'blue') # predicted print(w.item(),b.item()) x2, y2 = get_fake_data(batch_size=20) plt.scatter(x2.numpy(), y2.numpy()) # true data plt.xlim(0,5) plt.ylim(0,13) plt.show() plt.pause(0.5) print(w.item(), b.item()) # 1.9642277956008911 3.0166714191436768 线性回归2 import torch as t %matplotlib inline from matplotlib import pyplot as plt from torch import optim from torch import nn from matplotlib import style from IPython import display style.use(\"ggplot\") # 不知道为啥使用cuda梯度为不存在了 device = t.device('cpu') # 设置随机数种子，保证在不同电脑上运行时下面的输出一致 t.manual_seed(1000) def get_fake_data(batch_size=8): ''' 产生随机数据：y=x*2+3，加上了一些噪声''' x = t.rand(batch_size, 1, device=device) * 5 y = x * 2 + 3 + t.randn(batch_size, 1, device=device) return x, y # 随机初始化参数 w = t.rand(1,1, requires_grad=True).to(device) b = t.zeros(1,1, requires_grad=True).to(device) lr = 0.0005 # 学习率不能太大 num_epochs = 500 losses = np.zeros(500) # 导入优化器 opt = optim.SGD([w,b],lr = lr) # 引入mse损失函数 loss_func = nn.MSELoss() for ii in range(num_epochs): x, y = get_fake_data(batch_size=32) # forward：计算loss y_pred = x.mm(w) + b.expand_as(y) loss = loss_func(y_pred,y) losses[ii] = loss.item() # 优化器梯度清零 opt.zero_grad() # backward：手动计算梯度 loss.backward() # 逐步优化 opt.step() if ii % 10 == 0: # 画图 display.clear_output(wait=True) x = t.arange(0, 6).view(-1, 1) y = x.mm(w.data) + b.data.expand_as(x) plt.plot(x.numpy(), y.numpy(),c = 'blue') # predicted print(w.item(),b.item()) x2, y2 = get_fake_data(batch_size=20) plt.scatter(x2.numpy(), y2.numpy()) # true data plt.xlim(0,5) plt.ylim(0,13) plt.show() plt.pause(0.5) print(w.item(), b.item","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:4","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#线性回归2"},{"categories":["深度学习"],"content":" 参考 深度学习之Pytorch(陈云) ","date":"2019-01-05","objectID":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/:0:5","series":null,"tags":["python","深度学习","Pytorch"],"title":"Pytorch快速入门0","uri":"/pytorch%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A80/#参考"},{"categories":["深度学习"],"content":" 在本篇文章中，使用两种方法来做MNIST分类，一个是全连接层，一个是CNN。 MNIST 数据集来自美国国家标准与技术研究所， National Institute of Standards and Technology (NIST)。训练集 (training set) 由来自 250 个不同人手写的数字构成，其中 50% 是高中学生，50% 来自人口普查局 (the Census Bureau) 的工作人员.。测试集(test set) 也是同样比例的手写数字数据。 MNIST 数据集中有 60000 个训练样本和 10000 个测试样本，每个图片的大小是28×28。 ","date":"2019-01-02","objectID":"/tensorflow%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B%E4%B9%8Bmnist/:0:0","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow综合实例之MNIST","uri":"/tensorflow%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B%E4%B9%8Bmnist/#"},{"categories":["深度学习"],"content":" MNIST分类0 tensorflow实现MLP进行分类，使用两种方法来实现MLP，一种是手动前向计算，一种是使用tf.layersAPI。 # 参考代码 # mnist分类实例 import tensorflow as tf import numpy as np from tqdm import tqdm import matplotlib.pyplot as plt # style设置 from matplotlib import style from IPython import display style.use('ggplot') np.random.seed(42) %matplotlib inline data = np.load(\"./data/mnist.npz\") x_train,y_train,x_test,y_test = data['x_train'],data['y_train'],data[\"x_test\"],data['y_test'] # 数据规整 x_train = x_train.reshape(-1,784) / 255 - 0.5 x_test = x_test.reshape(-1,784) / 255 - 0.5 # 这个能生成一个OneHot的10维向量，作为Y_train的一行，这样Y_train就有60000行OneHot作为输出 y_train = (np.arange(10) == y_train[:, None]).astype(int) # 整理输出 y_test = (np.arange(10) == y_test[:, None]).astype(int) batch_size = 8 # 使用MBGD算法，设定batch_size为8 # batchsize的获取 def generatebatch(X,Y,n_examples, batch_size): for batch_i in range(n_examples // batch_size): start = batch_i*batch_size end = start + batch_size batch_xs = X[start:end] batch_ys = Y[start:end] yield batch_xs, batch_ys # 生成每一个batch def modle_mlp_0(x): print(\"mlp0\") # 第一隐藏层 w1 = tf.Variable(tf.random_normal([784,400])) b1 = tf.Variable(tf.random_normal([400])) l1 = tf.nn.relu(tf.matmul(x,w1) + b1) print(l1) # 输出层 w2 = tf.Variable(tf.random_normal([400,10])) b2 = tf.Variable(tf.random_normal([10])) out = tf.nn.softmax(tf.matmul(l1,w2) + b2) print(out) return out def modle_mlp_1(x): print(\"mlp1\") # 使用dense层 l1 = tf.layers.dense(x,400,tf.nn.relu) print(l1) # 输出层 out = tf.layers.dense(l1,10,tf.nn.softmax) print(out) return out tf.reset_default_graph() # 输入层 tf_X = tf.placeholder(tf.float32,[None,784]) tf_Y = tf.placeholder(tf.float32,[None,10]) # model的输出 # mlp0 # out = modle_mlp_0(tf_X) # mlp1 out = model_mlp_1(tf_X) # loss loss = -tf.reduce_mean(tf_Y*tf.log(tf.clip_by_value(out,1e-11,1.0))) # 优化 train_step = tf.train.AdamOptimizer(1e-3).minimize(loss) # 计算准确率 y_pred = tf.arg_max(out,1) bool_pred = tf.equal(tf.arg_max(tf_Y,1),y_pred) accuracy = tf.reduce_mean(tf.cast(bool_pred,tf.float32)) # 准确率 num_epochs = 100 accs = [] with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(num_epochs): # 迭代1000个周期 for batch_xs,batch_ys in generatebatch(x_train,y_train,y_train.shape[0],batch_size): # 每个周期进行MBGD算法 sess.run(train_step,feed_dict={tf_X:batch_xs,tf_Y:batch_ys}) if(epoch % 10 == 0): res = sess.run(accuracy,feed_dict={tf_X:x_test,tf_Y:y_test}) accs.append(res) print(epoch,res) mlp0的acc输出结果： 以下是mlp1的acc输出结果： ","date":"2019-01-02","objectID":"/tensorflow%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B%E4%B9%8Bmnist/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow综合实例之MNIST","uri":"/tensorflow%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B%E4%B9%8Bmnist/#mnist分类0"},{"categories":["深度学习"],"content":" MNIST分类1 tensorflow实现CNN进行分类，采用两种方式，一种是自定义权重和偏置前向计算，一种是使用默认的API。 # mnist CNN分类实例 import tensorflow as tf import numpy as np from tqdm import tqdm import matplotlib.pyplot as plt # style设置 from matplotlib import style from IPython import display style.use('ggplot') np.random.seed(42) %matplotlib inline data = np.load(\"./data/mnist.npz\") x_train,y_train,x_test,y_test = data['x_train'],data['y_train'],data[\"x_test\"],data['y_test'] # 数据规整 x_train = x_train.reshape(-1,28,28,1) / 255 - 0.5 x_test = x_test.reshape(-1,28,28,1) / 255 - 0.5 # 这个能生成一个OneHot的10维向量，作为Y_train的一行，这样Y_train就有60000行OneHot作为输出 y_train = (np.arange(10) == y_train[:, None]).astype(int) # 整理输出 y_test = (np.arange(10) == y_test[:, None]).astype(int) batch_size = 8 # 使用MBGD算法，设定batch_size为8 # batchsize的获取 def generatebatch(X,Y,n_examples, batch_size): for batch_i in range(n_examples // batch_size): start = batch_i*batch_size end = start + batch_size batch_xs = X[start:end] batch_ys = Y[start:end] yield batch_xs, batch_ys # 生成每一个batch def model_cnn_0(tf_X): print(\"cnn1\") # 第一卷积层+激活层 conv_filter_w1 = tf.Variable(tf.random_normal([3, 3, 1, 10])) conv_filter_b1 = tf.Variable(tf.random_normal([10])) relu_feature_maps1 = tf.nn.relu(tf.nn.conv2d(tf_X, conv_filter_w1,strides=[1, 1, 1, 1], padding='SAME') + conv_filter_b1) print(relu_feature_maps1) # 池化层 max_pool1 = tf.nn.max_pool(relu_feature_maps1,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME') print(max_pool1) # 第二卷积层 conv_filter_w2 = tf.Variable(tf.random_normal([3, 3, 10, 5])) conv_filter_b2 = tf.Variable(tf.random_normal([5])) conv_out2 = tf.nn.conv2d(relu_feature_maps1, conv_filter_w2,strides=[1, 2, 2, 1], padding='SAME') + conv_filter_b2 # BN归一化层+激活层 batch_mean, batch_var = tf.nn.moments(conv_out2, [0, 1, 2], keep_dims=True) shift = tf.Variable(tf.zeros([5])) scale = tf.Variable(tf.ones([5])) epsilon = 1e-3 BN_out = tf.nn.batch_normalization(conv_out2, batch_mean, batch_var, shift, scale, epsilon) relu_BN_maps2 = tf.nn.relu(BN_out) # 池化层 max_pool2 = tf.nn.max_pool(relu_BN_maps2,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME') print(max_pool2) # 将特征图进行展开 max_pool2_flat = tf.reshape(max_pool2, [-1, 7*7*5]) # 全连接层 fc_w1 = tf.Variable(tf.random_normal([7*7*5,50])) fc_b1 = tf.Variable(tf.random_normal([50])) fc_out1 = tf.nn.relu(tf.matmul(max_pool2_flat, fc_w1) + fc_b1) # 输出层 out_w1 = tf.Variable(tf.random_normal([50,10])) out_b1 = tf.Variable(tf.random_normal([10])) pred = tf.nn.softmax(tf.matmul(fc_out1,out_w1)+out_b1) return pred def model_cnn_1(x): print(\"cnn2\") #conv2 #layers.conv2d parameters #inputs 输入，是一个张量 #filters 卷积核个数，也就是卷积层的厚度 #kernel_size 卷积核的尺寸 #strides: 扫描步长 #padding: 边边补0 valid不需要补0，same需要补0，为了保证输入输出的尺寸一致,补多少不需要知道 #activation: 激活函数 cn1 = tf.layers.conv2d(inputs=x,filters=10,kernel_size=(3,3),strides=1,padding=\"same\",activation=tf.nn.relu) print(cn1) #tf.layers.max_pooling2d #inputs 输入，张量必须要有四个维度 #pool_size: 过滤器的尺寸 pool1 = tf.layers.max_pooling2d(inputs=cn1,pool_size=(3,3),strides=2,padding=\"same\") print(pool1) #conv2 cn2 = tf.layers.conv2d(inputs=pool1,filters=5,kernel_size=3,strides=1,padding=\"same\",activation=tf.nn.relu) print(cn2) # bn归一化层 batch_mean, batch_var = tf.nn.moments(cn2, [0, 1, 2], keep_dims=True) shift = tf.Variable(tf.zeros([5])) scale = tf.Variable(tf.ones([5])) epsilon = 1e-3 BN_out = tf.nn.batch_normalization(cn2, batch_mean, batch_var, shift, scale, epsilon) relu_BN_maps2 = tf.nn.relu(BN_out) #pool2 2*2 pool2 = tf.layers.max_pooling2d(inputs=relu_BN_maps2,pool_size=3,strides=2,padding=\"same\") print(pool2) #flat(平坦化) flat = tf.reshape(pool2,[-1,7*7*5]) dense = tf.layers.dense(inputs=flat,units=50,activation=tf.nn.relu) #输出层，不用激活函数（本质就是一个全连接层） out = tf.layers.dense(inputs = dense,units=10,activation=tf.nn.softmax) return out tf.reset_default_graph() # 输入层 tf_X = tf.placeholder(tf.float32,[None,28,28,1]) tf_Y = tf.placeholder(tf.float32,[None,10]) # model的输出 # 第一种cnn模型 out = model_cnn_0(tf_X) # 第二种cnn模型 # out = model_cnn_1(tf_X) # loss loss = -tf.reduce_mean(tf_Y*tf.log(tf.clip_by_val","date":"2019-01-02","objectID":"/tensorflow%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B%E4%B9%8Bmnist/:0:2","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow综合实例之MNIST","uri":"/tensorflow%E7%BB%BC%E5%90%88%E5%AE%9E%E4%BE%8B%E4%B9%8Bmnist/#mnist分类1"},{"categories":["hexo"],"content":" 第一步： 安装Kramed npm uninstall hexo-renderer-marked --save npm install hexo-renderer-kramed --save ","date":"2018-12-29","objectID":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/:0:1","series":null,"tags":["hexo"],"title":"配置hexo支持latex公式","uri":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/#第一步-安装kramed"},{"categories":["hexo"],"content":" 第二步：更改文件配置 打开/node_modules/hexo-renderer-kramed/lib/renderer.js,更改： // Change inline math rule function formatText(text) { // Fit kramed's rule: $$ + \\1 + $$ return text.replace(/`\\$(.*?)\\$`/g, '$$$$$1$$$$'); } 为，直接返回text // Change inline math rule function formatText(text) { return text; } ","date":"2018-12-29","objectID":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/:0:2","series":null,"tags":["hexo"],"title":"配置hexo支持latex公式","uri":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/#第二步更改文件配置"},{"categories":["hexo"],"content":" 第三步: 停止使用 hexo-math，并安装mathjax包 npm uninstall hexo-math --save npm install hexo-renderer-mathjax --save ","date":"2018-12-29","objectID":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/:0:3","series":null,"tags":["hexo"],"title":"配置hexo支持latex公式","uri":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/#第三步-停止使用-hexo-math并安装mathjax包"},{"categories":["hexo"],"content":" 第四步: 更新 Mathjax 的 配置文件 打开/node_modules/hexo-renderer-mathjax/mathjax.html ，注释掉第二个\u003cscript\u003e \u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML\"\u003e\u003c/script\u003e ","date":"2018-12-29","objectID":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/:0:4","series":null,"tags":["hexo"],"title":"配置hexo支持latex公式","uri":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/#第四步-更新-mathjax-的-配置文件"},{"categories":["hexo"],"content":" 第五步: 更改默认转义规则 打开/node_modules\\kramed\\lib\\rules\\inline.js escape: /^\\\\([\\\\`*{}\\[\\]()#$+\\-.!_\u003e])/, 更改为 escape: /^\\\\([`*\\[\\]()# +\\-.!_\u003e])/, em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 更改为 em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, ","date":"2018-12-29","objectID":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/:0:5","series":null,"tags":["hexo"],"title":"配置hexo支持latex公式","uri":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/#第五步-更改默认转义规则"},{"categories":["hexo"],"content":" 第六步: 开启mathjax 打开你所使用主题的_config.yml文件 mathjax: enable: true ","date":"2018-12-29","objectID":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/:0:6","series":null,"tags":["hexo"],"title":"配置hexo支持latex公式","uri":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/#第六步-开启mathjax"},{"categories":["hexo"],"content":" 最后的最后在每个文章的开头添加 mathjax: true 例如: title: tensorflow实例与线性回归 date: 2018-12-29 15:16:08 mathjax: true tags: - python - 深度学习 - tensorflow categories: 深度学习 ","date":"2018-12-29","objectID":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/:0:7","series":null,"tags":["hexo"],"title":"配置hexo支持latex公式","uri":"/%E9%85%8D%E7%BD%AEhexo%E6%94%AF%E6%8C%81latex%E5%85%AC%E5%BC%8F/#最后的最后"},{"categories":["深度学习"],"content":" 在本篇文章中，我们使用四种方法来实现线性回归模型，然后在使用tensoflow实现一个二次函数拟合模型。 ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:0","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#"},{"categories":["深度学习"],"content":" 线性回归模型 线性回归要解决的问题就是根据给出的数据学习出一个线性模型。 线性回归模型： $$ \\begin{align} \u0026h(x) = WX+b \\quad \\quad w:权重，b:偏置 \\ \u0026其中: \\quad \\quad X = \\begin{bmatrix} x_0 \\ x_1 \\ .. \\ x_n \\end{bmatrix} \\quad \\quad W = \\left[ \\begin{matrix} w_0 \\ w_1 \\ .. \\ w_n \\end{matrix} \\right] \\ \u0026损失函数 : J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}(x^i) - y^i)^2\\quad m:样本数目 ,\\theta:参数\\ \u0026我们的目的是求出使J(\\theta)最小的参数\\theta的值,求最小值，对每个参数\\theta_j,\\\u0026求出梯度并使梯度等于0，此时J(\\theta)最小 \\end{align} $$ 梯度下降法 思想：沿着梯度最大的方向，迭代计算参数 迭代函数： $$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)。 $$ $$ \\begin{align} \u0026w和b的求导结果为：\\ \u0026\\frac {\\partial}{\\partial w_j}J(\\theta) = \\frac{\\partial}{\\partial w_j}\\frac{1}{2} (h_{\\theta}(x) - y)^2\\ \u0026\\quad \\quad \\quad \\quad = 2 * \\frac{1}{2} (h_{\\theta}(x) - y)^2\\frac{\\partial}{\\partial w_j}(h_{\\theta} - y)\\ \u0026\\quad \\quad \\quad \\quad = (h_{\\theta}(x) - y) *x\\ \u0026\\frac {\\partial}{\\partial b_j}J(\\theta) = \\frac{\\partial}{\\partial b_j}\\frac{1}{2} (h_{\\theta}(x) - y)^2\\ \u0026\\quad \\quad \\quad \\quad = 2 * \\frac{1}{2} (h_{\\theta}(x) - y)^2\\frac{\\partial}{\\partial b_j}(h_{\\theta} - y)\\ \u0026\\quad \\quad \\quad \\quad = (h_{\\theta}(x) - y) \\ \\end{align} $$ 最小二乘法 何为最小二乘法，其实很简单。我们有很多的给定点，这时候我们需要找出一条线去拟合它，那么我先假设这个线的方程，然后把数据点代入假设的方程得到观测值，求使得实际值与观测值相减的平方和最小的参数。对变量求偏导联立便可求。 从矩阵乘和矩阵求导的方面来考虑解决问题。 $$ \\begin{align} \u0026矩阵形式损失函数： J(\\theta) = \\frac{1}{2}(XW - Y)^T(XW - Y) \\ \u0026\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad =\\frac{1}{2}[W^TX^TXW-W^TX^TY - Y^TXW+Y^TY]\\ \u0026\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad =\\frac{1}{2}[W^TX^TXW-2W^TX^TY+Y^TY]\\ \u0026计算导数：\\frac{\\partial J(\\theta)}{W} = \\frac{1}{2}\\frac{\\partial}{W}[W^TX^TXW-2X^TY]=0\\ \u0026所以：X^TW-X^TY=0 \\Rightarrow W=(X^TX)^{-1}X^TY. \\end{align} $$ ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#线性回归模型"},{"categories":["深度学习"],"content":" 线性回归模型 线性回归要解决的问题就是根据给出的数据学习出一个线性模型。 线性回归模型： $$ \\begin{align} \u0026h(x) = WX+b \\quad \\quad w:权重，b:偏置 \\ \u0026其中: \\quad \\quad X = \\begin{bmatrix} x_0 \\ x_1 \\ .. \\ x_n \\end{bmatrix} \\quad \\quad W = \\left[ \\begin{matrix} w_0 \\ w_1 \\ .. \\ w_n \\end{matrix} \\right] \\ \u0026损失函数 : J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}(x^i) - y^i)^2\\quad m:样本数目 ,\\theta:参数\\ \u0026我们的目的是求出使J(\\theta)最小的参数\\theta的值,求最小值，对每个参数\\theta_j,\\\u0026求出梯度并使梯度等于0，此时J(\\theta)最小 \\end{align} $$ 梯度下降法 思想：沿着梯度最大的方向，迭代计算参数 迭代函数： $$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)。 $$ $$ \\begin{align} \u0026w和b的求导结果为：\\ \u0026\\frac {\\partial}{\\partial w_j}J(\\theta) = \\frac{\\partial}{\\partial w_j}\\frac{1}{2} (h_{\\theta}(x) - y)^2\\ \u0026\\quad \\quad \\quad \\quad = 2 * \\frac{1}{2} (h_{\\theta}(x) - y)^2\\frac{\\partial}{\\partial w_j}(h_{\\theta} - y)\\ \u0026\\quad \\quad \\quad \\quad = (h_{\\theta}(x) - y) *x\\ \u0026\\frac {\\partial}{\\partial b_j}J(\\theta) = \\frac{\\partial}{\\partial b_j}\\frac{1}{2} (h_{\\theta}(x) - y)^2\\ \u0026\\quad \\quad \\quad \\quad = 2 * \\frac{1}{2} (h_{\\theta}(x) - y)^2\\frac{\\partial}{\\partial b_j}(h_{\\theta} - y)\\ \u0026\\quad \\quad \\quad \\quad = (h_{\\theta}(x) - y) \\ \\end{align} $$ 最小二乘法 何为最小二乘法，其实很简单。我们有很多的给定点，这时候我们需要找出一条线去拟合它，那么我先假设这个线的方程，然后把数据点代入假设的方程得到观测值，求使得实际值与观测值相减的平方和最小的参数。对变量求偏导联立便可求。 从矩阵乘和矩阵求导的方面来考虑解决问题。 $$ \\begin{align} \u0026矩阵形式损失函数： J(\\theta) = \\frac{1}{2}(XW - Y)^T(XW - Y) \\ \u0026\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad =\\frac{1}{2}[W^TX^TXW-W^TX^TY - Y^TXW+Y^TY]\\ \u0026\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad =\\frac{1}{2}[W^TX^TXW-2W^TX^TY+Y^TY]\\ \u0026计算导数：\\frac{\\partial J(\\theta)}{W} = \\frac{1}{2}\\frac{\\partial}{W}[W^TX^TXW-2X^TY]=0\\ \u0026所以：X^TW-X^TY=0 \\Rightarrow W=(X^TX)^{-1}X^TY. \\end{align} $$ ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#梯度下降法"},{"categories":["深度学习"],"content":" 线性回归模型 线性回归要解决的问题就是根据给出的数据学习出一个线性模型。 线性回归模型： $$ \\begin{align} \u0026h(x) = WX+b \\quad \\quad w:权重，b:偏置 \\ \u0026其中: \\quad \\quad X = \\begin{bmatrix} x_0 \\ x_1 \\ .. \\ x_n \\end{bmatrix} \\quad \\quad W = \\left[ \\begin{matrix} w_0 \\ w_1 \\ .. \\ w_n \\end{matrix} \\right] \\ \u0026损失函数 : J(\\theta) = \\frac{1}{2}\\sum_{i=1}^{m}(h_{\\theta}(x^i) - y^i)^2\\quad m:样本数目 ,\\theta:参数\\ \u0026我们的目的是求出使J(\\theta)最小的参数\\theta的值,求最小值，对每个参数\\theta_j,\\\u0026求出梯度并使梯度等于0，此时J(\\theta)最小 \\end{align} $$ 梯度下降法 思想：沿着梯度最大的方向，迭代计算参数 迭代函数： $$ \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)。 $$ $$ \\begin{align} \u0026w和b的求导结果为：\\ \u0026\\frac {\\partial}{\\partial w_j}J(\\theta) = \\frac{\\partial}{\\partial w_j}\\frac{1}{2} (h_{\\theta}(x) - y)^2\\ \u0026\\quad \\quad \\quad \\quad = 2 * \\frac{1}{2} (h_{\\theta}(x) - y)^2\\frac{\\partial}{\\partial w_j}(h_{\\theta} - y)\\ \u0026\\quad \\quad \\quad \\quad = (h_{\\theta}(x) - y) *x\\ \u0026\\frac {\\partial}{\\partial b_j}J(\\theta) = \\frac{\\partial}{\\partial b_j}\\frac{1}{2} (h_{\\theta}(x) - y)^2\\ \u0026\\quad \\quad \\quad \\quad = 2 * \\frac{1}{2} (h_{\\theta}(x) - y)^2\\frac{\\partial}{\\partial b_j}(h_{\\theta} - y)\\ \u0026\\quad \\quad \\quad \\quad = (h_{\\theta}(x) - y) \\ \\end{align} $$ 最小二乘法 何为最小二乘法，其实很简单。我们有很多的给定点，这时候我们需要找出一条线去拟合它，那么我先假设这个线的方程，然后把数据点代入假设的方程得到观测值，求使得实际值与观测值相减的平方和最小的参数。对变量求偏导联立便可求。 从矩阵乘和矩阵求导的方面来考虑解决问题。 $$ \\begin{align} \u0026矩阵形式损失函数： J(\\theta) = \\frac{1}{2}(XW - Y)^T(XW - Y) \\ \u0026\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad =\\frac{1}{2}[W^TX^TXW-W^TX^TY - Y^TXW+Y^TY]\\ \u0026\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad =\\frac{1}{2}[W^TX^TXW-2W^TX^TY+Y^TY]\\ \u0026计算导数：\\frac{\\partial J(\\theta)}{W} = \\frac{1}{2}\\frac{\\partial}{W}[W^TX^TXW-2X^TY]=0\\ \u0026所以：X^TW-X^TY=0 \\Rightarrow W=(X^TX)^{-1}X^TY. \\end{align} $$ ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#最小二乘法"},{"categories":["深度学习"],"content":" 线性回归0 第一种实现线性回归的方法，只使用numpy来实现，根据损失函数，手动计算梯度。 # 使用numpy实现线性回归 import numpy as np import matplotlib.pyplot as plt # style设置 from matplotlib import style style.use('ggplot') np.random.seed(42) %matplotlib inline w,b = 0,0 # 生成数据 x_data = np.random.rand(100).astype(np.float32) noise = np.random.normal(0,0.05,x_data.shape) y_data = 5 * x_data + 0.8 + noise num_epoch = 500 lr = 1e-3 # 梯度下降计算w，和b for e in range(num_epoch): y_pred = w * x_data + b # 手动计算梯度，根据mse公式 grad_w,grad_b = (y_pred - y_data).dot(x_data),(y_pred - y_data).sum() w -= lr * grad_w b -= lr * grad_b if e % 50 == 0: print(e,w,b) # 画图可视化 plt.scatter(x_data,y_data) plt.plot(x_data,w * x_data + b,color = \"gray\") ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:2","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#线性回归0"},{"categories":["深度学习"],"content":" 线性回归1 第二种实现线性回归的方法，只使用tensorflow的基本量来实现，根据损失函数，手动的计算梯度。 # tensorflow 实现线性回归模型 import tensorflow as tf import numpy as np np.random.seed(42) import matplotlib.pyplot as plt # style设置 from matplotlib import style style.use('ggplot') %matplotlib inline # 定义数据流，计算图 # 定义学习率的占位符 lr_ = tf.placeholder(tf.float32) # 定义x，y的占位符 xs=tf.placeholder(tf.float32,[None,1]) ys=tf.placeholder(tf.float32,[None,1]) # 使用变量作用域的方法来设置权重和偏置 with tf.variable_scope(\"variable\",reuse=tf.AUTO_REUSE): a = tf.get_variable(\"a\",dtype=tf.float32,shape = [],initializer=tf.zeros_initializer) b = tf.get_variable('b',dtype=tf.float32,shape = [],initializer=tf.zeros_initializer) # 计算预测值 y_pred = a * xs + b # 计算损失，使用mse loss = tf.constant(0.5) * tf.reduce_sum(tf.square(y_pred - ys)) # 反向传播，手动计算模型的梯度，根据mse公式计算得到 grad_a = tf.reduce_sum((y_pred - ys) * xs) grad_b = tf.reduce_sum((y_pred - ys)) ## 梯度下降法，手动更新参数 new_a = a - lr_ * grad_a new_b = b - lr_ * grad_b update_a = tf.assign(a,new_a) update_b = tf.assign(b,new_b) # 需要运算的节点 train = [update_a,update_b] train_op = [a,b] # 数据流图定义到此结束 # 直到现在为止，一直都是在定义数据图 # 定义真实的数据 num_epoch = 500 lr = 1e-3 # 生成数据 x = np.linspace(-1.0,1.0,300,dtype=np.float32)[:,np.newaxis] noise = np.random.normal(0,0.05,x.shape) # 假设w是5，b是0.8，然后添加噪声 y = 5 * x + 0.8 + noise with tf.Session() as sess: # 初始化变量a和b # tf.global_variables_initializer().run() sess.run(tf.global_variables_initializer()) # 循环num_epoch次，迭代更新计算 for e in range(num_epoch): sess.run(train,feed_dict={xs:x,ys:y,lr_:lr}) w,b_ = sess.run(train_op) if e % 50 == 0: print(e,w,b_) \"\"\" 0 0.5038601 0.23991679 50 4.98277 0.7997225 100 5.005011 0.7997225 150 5.00512 0.7997225 200 5.00512 0.7997225 250 5.00512 0.7997225 300 5.00512 0.7997225 350 5.00512 0.7997225 400 5.00512 0.7997225 450 5.00512 0.7997225 \"\"\" plt.scatter(x,y) plt.plot(x,w * x + b_,c='blue') ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:3","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#线性回归1"},{"categories":["深度学习"],"content":" 线性回归2 第三种实现线性回归的方法，使用tensorflow提供的优化器来计算自动计算梯度。 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt # style设置 from matplotlib import style style.use('ggplot') np.random.seed(42) %matplotlib inline # 生成数据 x_data = np.random.rand(100).astype(np.float32) noise = np.random.normal(0,0.05,x_data.shape) y_data = 5 * x_data + 0.8 + noise # 创建w和b Weights = tf.Variable(tf.random_uniform([1],-1,1)) # 创建一个一维w，范围是[-1,1] biase = tf.Variable(tf.zeros([1])) y = Weights * x_data + biase # 定义真实数据 num_epoch = 500 lr = 0.5 # mse的方法计算loss loss = tf.reduce_mean(tf.square(y - y_data)) # 梯度下降优化器 optimizer = tf.train.GradientDescentOptimizer(lr) train = optimizer.minimize(loss) # 需要计算的权值和偏重 train_op = [Weights,biase] #初始化变量 init = tf.global_variables_initializer() with tf.Session() as sess: sess.run(init) for step in range(num_epoch): # 运行优化器 sess.run(train) if step % 50 == 0: w,b = sess.run(train_op) print(step,w,b) \"\"\" 0 [1.5435352] [3.4052124] 50 [4.8807707] [0.859452] 100 [4.9745603] [0.81199497] 150 [4.976949] [0.8107863] 200 [4.977009] [0.810756] 250 [4.977009] [0.810756] 300 [4.977009] [0.810756] 350 [4.977009] [0.810756] 400 [4.977009] [0.810756] 450 [4.977009] [0.810756] \"\"\" plt.scatter(x_data,y_data) plt.plot(x_data,w * x_data + b,color = \"green\") ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#线性回归2"},{"categories":["深度学习"],"content":" 线性回归3 第三种实现线性回归的方法，使用tensoflow Eager Execution模式来实现。 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt # style设置 from matplotlib import style style.use('ggplot') np.random.seed(42) %matplotlib inline # 声明使用Eager Execution模式 tf.enable_eager_execution() # 定义变量 with tf.variable_scope(\"eager\"): w = tf.get_variable('w',dtype=tf.float32,shape=[],initializer=tf.zeros_initializer) b = tf.get_variable('b',dtype=tf.float32,shape=[],initializer=tf.zeros_initializer) variables = [w,b] # 生成数据 x_data = np.random.rand(100).astype(np.float32) noise = np.random.normal(0,0.05,x_data.shape) y_data = 5 * x_data + 0.8 + noise num_epoch = 500 lr = 1e-3 # 定义优化器 optimizer = tf.train.GradientDescentOptimizer(lr) # 迭代计算 for e in range(num_epoch): # 开启自动求导机制来计算导数 with tf.GradientTape() as tape: # 预测值 y_pred = w * x_data + b # 损失结果 loss = 0.5 * tf.reduce_sum(tf.square(y_pred - y_data)) if e % 50 == 0: print(e,w.numpy(),b.numpy()) # 自动计算变量的梯度 grads = tape.gradient(loss,variables) # 优化器根据梯度自动更新参数 optimizer.apply_gradients(grads_and_vars=zip(grads,variables)) \"\"\" 0 0.0 0.0 50 2.4261763 2.096965 100 3.1899536 1.714988 150 3.7241528 1.4446927 200 4.098665 1.2551923 250 4.3612247 1.1223388 300 4.5453 1.0291982 350 4.6743484 0.9639001 400 4.7648215 0.9181214 450 4.8282514 0.8860268 \"\"\" plt.scatter(x_data,y_data) plt.plot(x_data,w.numpy() * x_data + b.numpy(),color = \"black\") ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:5","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#线性回归3"},{"categories":["深度学习"],"content":" tensorflow拟合二次函数 import tensorflow as tf import numpy as np import matplotlib.pyplot as plt # style设置 from matplotlib import style from IPython import display style.use('ggplot') np.random.seed(42) %matplotlib inline # 手动全连接层==tf.layers.dense def add_layer(inputs,in_size,out_size,activation_function=None): #Weights是一个矩阵，[行，列]为[in_size,out_size] Weights=tf.Variable(tf.random_normal([in_size,out_size]))#正态分布 #初始值推荐不为0，所以加上0.1，一行，out_size列 biases=tf.Variable(tf.zeros([1,out_size])+0.1) #Weights*x+b的初始化的值，也就是未激活的值 Wx_plus_b=tf.matmul(inputs,Weights)+biases # matmul 矩阵乘法 #激活 if activation_function is None: #激活函数为None，也就是线性函数 outputs=Wx_plus_b else: outputs = activation_function(Wx_plus_b) return outputs \"\"\"定义数据形式\"\"\" # (-1,1)之间，有300个单位，后面的是维度，x_data是有300行（300个例子） x_data=np.linspace(-1,1,300)[:,np.newaxis] # 加噪声,均值为0，方差为0.05，大小和x_data一样 noise=np.random.normal(0,0.05,x_data.shape) # 定义二次函数 y_data=np.square(x_data) - 0.5 + noise # 定义占位符 xs=tf.placeholder(tf.float32,[None,1]) ys=tf.placeholder(tf.float32,[None,1]) \"\"\"建立网络\"\"\" #定义隐藏层，输入1个节点，输出10个节点 l1 = add_layer(xs,1,10,activation_function=tf.nn.relu) #定义输出层 out = add_layer(l1,10,1,activation_function=None) # 以上的两句等同于下面这两句 # logits = tf.layers.dense(xs,10,activation=tf.nn.relu) # out = tf.layers.dense(logits,1) \"\"\"预测\"\"\" #损失函数,算出的是每个例子的平方，要求和（reduction_indices=[1]，按行求和）,再求均值 loss=tf.reduce_mean(tf.reduce_sum(tf.square(ys-out),reduction_indices=[1])) \"\"\"训练\"\"\" #优化算法,minimize(loss)以0.1的学习率对loss进行减小 train_step=tf.train.GradientDescentOptimizer(0.1).minimize(loss) init=tf.global_variables_initializer() num_epoch = 500 lr = 1e-3 fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(x_data,y_data) # 抑制plt.show()的暂停 plt.ion() plt.show() with tf.Session() as sess: sess.run(init) for i in range(num_epoch): sess.run(train_step,feed_dict={xs:x_data,ys:y_data}) if i%50 == 0: # print(i,sess.run(loss,feed_dict={xs:x_data,ys:y_data})) try: # 抹除图里面的第一条直线，也就是每次画图之前，先删除之前的 # 由于初始lines不存在，所以会有异常 ax.lines.remove(lines[0]) except Exception: pass # 获取预测值 out_val = sess.run(out,feed_dict={xs:x_data}) # 画出拟合直线 lines = ax.plot(x_data,out_val,\"b-\",lw=5) # 暂停0.1s plt.pause(0.1) ","date":"2018-12-29","objectID":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:6","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow实例与线性回归","uri":"/tensorflow%E5%AE%9E%E4%BE%8B%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#tensorflow拟合二次函数"},{"categories":["深度学习"],"content":" 学习完tensorflow变量常量等基本量的操作，意味着最基本的东西都有了，使用这些基本的操作，我们就做一些数学运算，至于接下来如何操作基本量和组成更大的计算图，那就需要学习Graph和Session了。 ","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:0","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#"},{"categories":["深度学习"],"content":" tensorflow的基础知识 tensorflow基础概念之Graph(tf.Graph) Graph是一个TensorFlow的一种运算，被表示为一个数据流的图。 一个Graph包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点 每一个任务都需要一个图，即使不去手动的声明，tensorflow也会在后台默认的构建图，然后将操作添加到图里面。 一般情况下，我们只需要使用默认生成的图即可，特殊情况下，再去显示的声明多个图。 属性： building_function:Returns True iff this graph represents a function. finalized:返回True，要是这个图被终止了 graph_def_versions:The GraphDef version information of this graph. seed:The graph-level random seed of this graph. version:Returns a version number that increases as ops are added to the graph. 函数： add_to_collection(name,value)：存放值在给定名称的collection里面(因为collection不是sets,所以有可能一个值会添加很多次) . as_default()：返回一个上下文管理器,使得这个Graph对象成为当前默认的graph. finalize()：结束这个graph,使得他只读(read-only). tensorflow基础概念之Session(tf.Session) 运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境. Session是一个比较重要的东西，TensorFlow中只有让Graph（计算图）上的节点Session（会话）中执行，才会得到结果。Session的开启涉及真实的运算，因此比较消耗资源。在使用结束后，务必关闭Session。一般在使用过程中，我们可以通过with上下文管理器来使用Session。 # Using the context manager. with tf.Session() as sess: sess.run(...) 属性： graph：“投放”到session中的图 **graph_def：**图的描述 函数： tf.Session.init(target=”, graph=None, config=None)：Session构造函数，可以在声明Session的时候指定Graph，如果未指定，则使用默认图。 tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)：运行操作估算（计算）tensor。 tf.Session.close()：Session使用之后，一定要关闭 tf.as_default() ：返回一个上下文管理器，使得这个对象成为当前默认的session/使用with关键字然后可以在with关键字代码块中执行 tensorflow之激活函数 激活操作提供了在神经网络中使用的不同类型的非线性模型。包括光滑非线性模型(sigmoid, tanh, elu, softplus, and softsign)。连续但是不是处处可微的函数(relu, relu6, crelu and relu_x)。当然还有随机正则化 (dropout) ,所有的激活操作都是作用在每个元素上面的，输出一个tensor和输入的tensor又相同的形状和数据类型。 激活函数列表： tf.nn.relu tf.nn.relu6 tf.nn.crelu tf.nn.elu tf.nn.softplus tf.nn.softsign tf.nn.dropout tf.nn.bias_add tf.sigmoid tf.tanh tf.nn.softmax tf.nn.relu(features, name=None)： 计算修正线性单元:max(features,0) 如果，你不知道使用哪个激活函数，那么使用relu准没错。 tf.nn.softmax(logits,dim=-1,name=None)： 计算softmax激活值，多分类输出层的激活函数。 tensorflow之优化器 深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化，此处记录一下tensorflow的优化器api。 优化器列表： tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.AdadeltaOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer tf.train.Optimizer： 优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类。 tf.train.GradientDescentOptimizer(learning_rate,use_locking=False,name=‘GradientDescent’) ： 作用：创建一个梯度下降优化器对象 参数： learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=‘Adadelta’) 作用：构造一个使用Adadelta算法的优化器 参数： learning_rate: tensor或者浮点数，学习率 rho: tensor或者浮点数. 优化参数 epsilon: tensor或者浮点数. 优化参数 use_locking: If True use locks for update operations. name: 【可选】这个操作的名字，默认是”Adadelta” tensorflow变量作用域机制 在深度学习中，我们可能需要用到大量的变量集，而且这些变量集可能在多处都要用到。例如，训练模型时，训练参数如权重（weights）、偏置（biases）等已经定下来，要拿到验证集去验证，我们自然希望这些参数是同一组。以往写简单的程序，可能使用全局限量就可以了，但在深度学习中，这显然是不行的，一方面不便管理，另外这样一来代码的封装性受到极大影响。因此，TensorFlow提供了一种变量管理方法：变量作用域机制，以此解决上面出现的问题。 在Tensoflow中，提供了两种作用域： 命名域(name scope)：通过tf.name_scope()来实现； 变量域（variable scope）：通过tf.variable_scope()来实现；可以通过设置reuse 标志以及初始化方式来影响域下的变量。 这两种作用域都会给tf.Variable()创建的变量加上词头，而tf.name_scope对tf.get_variable()创建的变量没有词头影响。 tf.name_scope(‘scope_name’) tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。 import tensorflow as tf # 与 tf.Variable() 结合使用。简化了命名 with tf.name_scope('conv1') as scope: weights1 = tf.Variable([1.0, 2.0], name='weights') bias1 = tf.Variable([0.3], name='bias') # 注意，这里的 with 和 python 中其他的 with 是不一样的 # 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行上面的代码 # 就会再生成其他命名空间 # 下面是在另外一个命名空间来定义变量的 with tf.name_scope('conv2') as scope: weights2 = tf.Variable([4.0, 2.0], name='weights') bias2 = tf.Variable([0.33], name='bias') # 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突 print(weights1.name) print(weights2.name) # ----------------- # conv1/weights:0 # conv2/weights:0 tf.variable_scope(‘scope_name’","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#tensorflow的基础知识"},{"categories":["深度学习"],"content":" tensorflow的基础知识 tensorflow基础概念之Graph(tf.Graph) Graph是一个TensorFlow的一种运算，被表示为一个数据流的图。 一个Graph包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点 每一个任务都需要一个图，即使不去手动的声明，tensorflow也会在后台默认的构建图，然后将操作添加到图里面。 一般情况下，我们只需要使用默认生成的图即可，特殊情况下，再去显示的声明多个图。 属性： building_function:Returns True iff this graph represents a function. finalized:返回True，要是这个图被终止了 graph_def_versions:The GraphDef version information of this graph. seed:The graph-level random seed of this graph. version:Returns a version number that increases as ops are added to the graph. 函数： add_to_collection(name,value)：存放值在给定名称的collection里面(因为collection不是sets,所以有可能一个值会添加很多次) . as_default()：返回一个上下文管理器,使得这个Graph对象成为当前默认的graph. finalize()：结束这个graph,使得他只读(read-only). tensorflow基础概念之Session(tf.Session) 运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境. Session是一个比较重要的东西，TensorFlow中只有让Graph（计算图）上的节点Session（会话）中执行，才会得到结果。Session的开启涉及真实的运算，因此比较消耗资源。在使用结束后，务必关闭Session。一般在使用过程中，我们可以通过with上下文管理器来使用Session。 # Using the context manager. with tf.Session() as sess: sess.run(...) 属性： graph：“投放”到session中的图 **graph_def：**图的描述 函数： tf.Session.init(target=”, graph=None, config=None)：Session构造函数，可以在声明Session的时候指定Graph，如果未指定，则使用默认图。 tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)：运行操作估算（计算）tensor。 tf.Session.close()：Session使用之后，一定要关闭 tf.as_default() ：返回一个上下文管理器，使得这个对象成为当前默认的session/使用with关键字然后可以在with关键字代码块中执行 tensorflow之激活函数 激活操作提供了在神经网络中使用的不同类型的非线性模型。包括光滑非线性模型(sigmoid, tanh, elu, softplus, and softsign)。连续但是不是处处可微的函数(relu, relu6, crelu and relu_x)。当然还有随机正则化 (dropout) ,所有的激活操作都是作用在每个元素上面的，输出一个tensor和输入的tensor又相同的形状和数据类型。 激活函数列表： tf.nn.relu tf.nn.relu6 tf.nn.crelu tf.nn.elu tf.nn.softplus tf.nn.softsign tf.nn.dropout tf.nn.bias_add tf.sigmoid tf.tanh tf.nn.softmax tf.nn.relu(features, name=None)： 计算修正线性单元:max(features,0) 如果，你不知道使用哪个激活函数，那么使用relu准没错。 tf.nn.softmax(logits,dim=-1,name=None)： 计算softmax激活值，多分类输出层的激活函数。 tensorflow之优化器 深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化，此处记录一下tensorflow的优化器api。 优化器列表： tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.AdadeltaOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer tf.train.Optimizer： 优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类。 tf.train.GradientDescentOptimizer(learning_rate,use_locking=False,name=‘GradientDescent’) ： 作用：创建一个梯度下降优化器对象 参数： learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=‘Adadelta’) 作用：构造一个使用Adadelta算法的优化器 参数： learning_rate: tensor或者浮点数，学习率 rho: tensor或者浮点数. 优化参数 epsilon: tensor或者浮点数. 优化参数 use_locking: If True use locks for update operations. name: 【可选】这个操作的名字，默认是”Adadelta” tensorflow变量作用域机制 在深度学习中，我们可能需要用到大量的变量集，而且这些变量集可能在多处都要用到。例如，训练模型时，训练参数如权重（weights）、偏置（biases）等已经定下来，要拿到验证集去验证，我们自然希望这些参数是同一组。以往写简单的程序，可能使用全局限量就可以了，但在深度学习中，这显然是不行的，一方面不便管理，另外这样一来代码的封装性受到极大影响。因此，TensorFlow提供了一种变量管理方法：变量作用域机制，以此解决上面出现的问题。 在Tensoflow中，提供了两种作用域： 命名域(name scope)：通过tf.name_scope()来实现； 变量域（variable scope）：通过tf.variable_scope()来实现；可以通过设置reuse 标志以及初始化方式来影响域下的变量。 这两种作用域都会给tf.Variable()创建的变量加上词头，而tf.name_scope对tf.get_variable()创建的变量没有词头影响。 tf.name_scope(‘scope_name’) tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。 import tensorflow as tf # 与 tf.Variable() 结合使用。简化了命名 with tf.name_scope('conv1') as scope: weights1 = tf.Variable([1.0, 2.0], name='weights') bias1 = tf.Variable([0.3], name='bias') # 注意，这里的 with 和 python 中其他的 with 是不一样的 # 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行上面的代码 # 就会再生成其他命名空间 # 下面是在另外一个命名空间来定义变量的 with tf.name_scope('conv2') as scope: weights2 = tf.Variable([4.0, 2.0], name='weights') bias2 = tf.Variable([0.33], name='bias') # 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突 print(weights1.name) print(weights2.name) # ----------------- # conv1/weights:0 # conv2/weights:0 tf.variable_scope(‘scope_name’","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#tensorflow基础概念之graphtfgraph"},{"categories":["深度学习"],"content":" tensorflow的基础知识 tensorflow基础概念之Graph(tf.Graph) Graph是一个TensorFlow的一种运算，被表示为一个数据流的图。 一个Graph包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点 每一个任务都需要一个图，即使不去手动的声明，tensorflow也会在后台默认的构建图，然后将操作添加到图里面。 一般情况下，我们只需要使用默认生成的图即可，特殊情况下，再去显示的声明多个图。 属性： building_function:Returns True iff this graph represents a function. finalized:返回True，要是这个图被终止了 graph_def_versions:The GraphDef version information of this graph. seed:The graph-level random seed of this graph. version:Returns a version number that increases as ops are added to the graph. 函数： add_to_collection(name,value)：存放值在给定名称的collection里面(因为collection不是sets,所以有可能一个值会添加很多次) . as_default()：返回一个上下文管理器,使得这个Graph对象成为当前默认的graph. finalize()：结束这个graph,使得他只读(read-only). tensorflow基础概念之Session(tf.Session) 运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境. Session是一个比较重要的东西，TensorFlow中只有让Graph（计算图）上的节点Session（会话）中执行，才会得到结果。Session的开启涉及真实的运算，因此比较消耗资源。在使用结束后，务必关闭Session。一般在使用过程中，我们可以通过with上下文管理器来使用Session。 # Using the context manager. with tf.Session() as sess: sess.run(...) 属性： graph：“投放”到session中的图 **graph_def：**图的描述 函数： tf.Session.init(target=”, graph=None, config=None)：Session构造函数，可以在声明Session的时候指定Graph，如果未指定，则使用默认图。 tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)：运行操作估算（计算）tensor。 tf.Session.close()：Session使用之后，一定要关闭 tf.as_default() ：返回一个上下文管理器，使得这个对象成为当前默认的session/使用with关键字然后可以在with关键字代码块中执行 tensorflow之激活函数 激活操作提供了在神经网络中使用的不同类型的非线性模型。包括光滑非线性模型(sigmoid, tanh, elu, softplus, and softsign)。连续但是不是处处可微的函数(relu, relu6, crelu and relu_x)。当然还有随机正则化 (dropout) ,所有的激活操作都是作用在每个元素上面的，输出一个tensor和输入的tensor又相同的形状和数据类型。 激活函数列表： tf.nn.relu tf.nn.relu6 tf.nn.crelu tf.nn.elu tf.nn.softplus tf.nn.softsign tf.nn.dropout tf.nn.bias_add tf.sigmoid tf.tanh tf.nn.softmax tf.nn.relu(features, name=None)： 计算修正线性单元:max(features,0) 如果，你不知道使用哪个激活函数，那么使用relu准没错。 tf.nn.softmax(logits,dim=-1,name=None)： 计算softmax激活值，多分类输出层的激活函数。 tensorflow之优化器 深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化，此处记录一下tensorflow的优化器api。 优化器列表： tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.AdadeltaOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer tf.train.Optimizer： 优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类。 tf.train.GradientDescentOptimizer(learning_rate,use_locking=False,name=‘GradientDescent’) ： 作用：创建一个梯度下降优化器对象 参数： learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=‘Adadelta’) 作用：构造一个使用Adadelta算法的优化器 参数： learning_rate: tensor或者浮点数，学习率 rho: tensor或者浮点数. 优化参数 epsilon: tensor或者浮点数. 优化参数 use_locking: If True use locks for update operations. name: 【可选】这个操作的名字，默认是”Adadelta” tensorflow变量作用域机制 在深度学习中，我们可能需要用到大量的变量集，而且这些变量集可能在多处都要用到。例如，训练模型时，训练参数如权重（weights）、偏置（biases）等已经定下来，要拿到验证集去验证，我们自然希望这些参数是同一组。以往写简单的程序，可能使用全局限量就可以了，但在深度学习中，这显然是不行的，一方面不便管理，另外这样一来代码的封装性受到极大影响。因此，TensorFlow提供了一种变量管理方法：变量作用域机制，以此解决上面出现的问题。 在Tensoflow中，提供了两种作用域： 命名域(name scope)：通过tf.name_scope()来实现； 变量域（variable scope）：通过tf.variable_scope()来实现；可以通过设置reuse 标志以及初始化方式来影响域下的变量。 这两种作用域都会给tf.Variable()创建的变量加上词头，而tf.name_scope对tf.get_variable()创建的变量没有词头影响。 tf.name_scope(‘scope_name’) tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。 import tensorflow as tf # 与 tf.Variable() 结合使用。简化了命名 with tf.name_scope('conv1') as scope: weights1 = tf.Variable([1.0, 2.0], name='weights') bias1 = tf.Variable([0.3], name='bias') # 注意，这里的 with 和 python 中其他的 with 是不一样的 # 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行上面的代码 # 就会再生成其他命名空间 # 下面是在另外一个命名空间来定义变量的 with tf.name_scope('conv2') as scope: weights2 = tf.Variable([4.0, 2.0], name='weights') bias2 = tf.Variable([0.33], name='bias') # 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突 print(weights1.name) print(weights2.name) # ----------------- # conv1/weights:0 # conv2/weights:0 tf.variable_scope(‘scope_name’","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#tensorflow基础概念之sessiontfsession"},{"categories":["深度学习"],"content":" tensorflow的基础知识 tensorflow基础概念之Graph(tf.Graph) Graph是一个TensorFlow的一种运算，被表示为一个数据流的图。 一个Graph包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点 每一个任务都需要一个图，即使不去手动的声明，tensorflow也会在后台默认的构建图，然后将操作添加到图里面。 一般情况下，我们只需要使用默认生成的图即可，特殊情况下，再去显示的声明多个图。 属性： building_function:Returns True iff this graph represents a function. finalized:返回True，要是这个图被终止了 graph_def_versions:The GraphDef version information of this graph. seed:The graph-level random seed of this graph. version:Returns a version number that increases as ops are added to the graph. 函数： add_to_collection(name,value)：存放值在给定名称的collection里面(因为collection不是sets,所以有可能一个值会添加很多次) . as_default()：返回一个上下文管理器,使得这个Graph对象成为当前默认的graph. finalize()：结束这个graph,使得他只读(read-only). tensorflow基础概念之Session(tf.Session) 运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境. Session是一个比较重要的东西，TensorFlow中只有让Graph（计算图）上的节点Session（会话）中执行，才会得到结果。Session的开启涉及真实的运算，因此比较消耗资源。在使用结束后，务必关闭Session。一般在使用过程中，我们可以通过with上下文管理器来使用Session。 # Using the context manager. with tf.Session() as sess: sess.run(...) 属性： graph：“投放”到session中的图 **graph_def：**图的描述 函数： tf.Session.init(target=”, graph=None, config=None)：Session构造函数，可以在声明Session的时候指定Graph，如果未指定，则使用默认图。 tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)：运行操作估算（计算）tensor。 tf.Session.close()：Session使用之后，一定要关闭 tf.as_default() ：返回一个上下文管理器，使得这个对象成为当前默认的session/使用with关键字然后可以在with关键字代码块中执行 tensorflow之激活函数 激活操作提供了在神经网络中使用的不同类型的非线性模型。包括光滑非线性模型(sigmoid, tanh, elu, softplus, and softsign)。连续但是不是处处可微的函数(relu, relu6, crelu and relu_x)。当然还有随机正则化 (dropout) ,所有的激活操作都是作用在每个元素上面的，输出一个tensor和输入的tensor又相同的形状和数据类型。 激活函数列表： tf.nn.relu tf.nn.relu6 tf.nn.crelu tf.nn.elu tf.nn.softplus tf.nn.softsign tf.nn.dropout tf.nn.bias_add tf.sigmoid tf.tanh tf.nn.softmax tf.nn.relu(features, name=None)： 计算修正线性单元:max(features,0) 如果，你不知道使用哪个激活函数，那么使用relu准没错。 tf.nn.softmax(logits,dim=-1,name=None)： 计算softmax激活值，多分类输出层的激活函数。 tensorflow之优化器 深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化，此处记录一下tensorflow的优化器api。 优化器列表： tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.AdadeltaOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer tf.train.Optimizer： 优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类。 tf.train.GradientDescentOptimizer(learning_rate,use_locking=False,name=‘GradientDescent’) ： 作用：创建一个梯度下降优化器对象 参数： learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=‘Adadelta’) 作用：构造一个使用Adadelta算法的优化器 参数： learning_rate: tensor或者浮点数，学习率 rho: tensor或者浮点数. 优化参数 epsilon: tensor或者浮点数. 优化参数 use_locking: If True use locks for update operations. name: 【可选】这个操作的名字，默认是”Adadelta” tensorflow变量作用域机制 在深度学习中，我们可能需要用到大量的变量集，而且这些变量集可能在多处都要用到。例如，训练模型时，训练参数如权重（weights）、偏置（biases）等已经定下来，要拿到验证集去验证，我们自然希望这些参数是同一组。以往写简单的程序，可能使用全局限量就可以了，但在深度学习中，这显然是不行的，一方面不便管理，另外这样一来代码的封装性受到极大影响。因此，TensorFlow提供了一种变量管理方法：变量作用域机制，以此解决上面出现的问题。 在Tensoflow中，提供了两种作用域： 命名域(name scope)：通过tf.name_scope()来实现； 变量域（variable scope）：通过tf.variable_scope()来实现；可以通过设置reuse 标志以及初始化方式来影响域下的变量。 这两种作用域都会给tf.Variable()创建的变量加上词头，而tf.name_scope对tf.get_variable()创建的变量没有词头影响。 tf.name_scope(‘scope_name’) tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。 import tensorflow as tf # 与 tf.Variable() 结合使用。简化了命名 with tf.name_scope('conv1') as scope: weights1 = tf.Variable([1.0, 2.0], name='weights') bias1 = tf.Variable([0.3], name='bias') # 注意，这里的 with 和 python 中其他的 with 是不一样的 # 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行上面的代码 # 就会再生成其他命名空间 # 下面是在另外一个命名空间来定义变量的 with tf.name_scope('conv2') as scope: weights2 = tf.Variable([4.0, 2.0], name='weights') bias2 = tf.Variable([0.33], name='bias') # 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突 print(weights1.name) print(weights2.name) # ----------------- # conv1/weights:0 # conv2/weights:0 tf.variable_scope(‘scope_name’","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#tensorflow之激活函数"},{"categories":["深度学习"],"content":" tensorflow的基础知识 tensorflow基础概念之Graph(tf.Graph) Graph是一个TensorFlow的一种运算，被表示为一个数据流的图。 一个Graph包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点 每一个任务都需要一个图，即使不去手动的声明，tensorflow也会在后台默认的构建图，然后将操作添加到图里面。 一般情况下，我们只需要使用默认生成的图即可，特殊情况下，再去显示的声明多个图。 属性： building_function:Returns True iff this graph represents a function. finalized:返回True，要是这个图被终止了 graph_def_versions:The GraphDef version information of this graph. seed:The graph-level random seed of this graph. version:Returns a version number that increases as ops are added to the graph. 函数： add_to_collection(name,value)：存放值在给定名称的collection里面(因为collection不是sets,所以有可能一个值会添加很多次) . as_default()：返回一个上下文管理器,使得这个Graph对象成为当前默认的graph. finalize()：结束这个graph,使得他只读(read-only). tensorflow基础概念之Session(tf.Session) 运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境. Session是一个比较重要的东西，TensorFlow中只有让Graph（计算图）上的节点Session（会话）中执行，才会得到结果。Session的开启涉及真实的运算，因此比较消耗资源。在使用结束后，务必关闭Session。一般在使用过程中，我们可以通过with上下文管理器来使用Session。 # Using the context manager. with tf.Session() as sess: sess.run(...) 属性： graph：“投放”到session中的图 **graph_def：**图的描述 函数： tf.Session.init(target=”, graph=None, config=None)：Session构造函数，可以在声明Session的时候指定Graph，如果未指定，则使用默认图。 tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)：运行操作估算（计算）tensor。 tf.Session.close()：Session使用之后，一定要关闭 tf.as_default() ：返回一个上下文管理器，使得这个对象成为当前默认的session/使用with关键字然后可以在with关键字代码块中执行 tensorflow之激活函数 激活操作提供了在神经网络中使用的不同类型的非线性模型。包括光滑非线性模型(sigmoid, tanh, elu, softplus, and softsign)。连续但是不是处处可微的函数(relu, relu6, crelu and relu_x)。当然还有随机正则化 (dropout) ,所有的激活操作都是作用在每个元素上面的，输出一个tensor和输入的tensor又相同的形状和数据类型。 激活函数列表： tf.nn.relu tf.nn.relu6 tf.nn.crelu tf.nn.elu tf.nn.softplus tf.nn.softsign tf.nn.dropout tf.nn.bias_add tf.sigmoid tf.tanh tf.nn.softmax tf.nn.relu(features, name=None)： 计算修正线性单元:max(features,0) 如果，你不知道使用哪个激活函数，那么使用relu准没错。 tf.nn.softmax(logits,dim=-1,name=None)： 计算softmax激活值，多分类输出层的激活函数。 tensorflow之优化器 深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化，此处记录一下tensorflow的优化器api。 优化器列表： tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.AdadeltaOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer tf.train.Optimizer： 优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类。 tf.train.GradientDescentOptimizer(learning_rate,use_locking=False,name=‘GradientDescent’) ： 作用：创建一个梯度下降优化器对象 参数： learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=‘Adadelta’) 作用：构造一个使用Adadelta算法的优化器 参数： learning_rate: tensor或者浮点数，学习率 rho: tensor或者浮点数. 优化参数 epsilon: tensor或者浮点数. 优化参数 use_locking: If True use locks for update operations. name: 【可选】这个操作的名字，默认是”Adadelta” tensorflow变量作用域机制 在深度学习中，我们可能需要用到大量的变量集，而且这些变量集可能在多处都要用到。例如，训练模型时，训练参数如权重（weights）、偏置（biases）等已经定下来，要拿到验证集去验证，我们自然希望这些参数是同一组。以往写简单的程序，可能使用全局限量就可以了，但在深度学习中，这显然是不行的，一方面不便管理，另外这样一来代码的封装性受到极大影响。因此，TensorFlow提供了一种变量管理方法：变量作用域机制，以此解决上面出现的问题。 在Tensoflow中，提供了两种作用域： 命名域(name scope)：通过tf.name_scope()来实现； 变量域（variable scope）：通过tf.variable_scope()来实现；可以通过设置reuse 标志以及初始化方式来影响域下的变量。 这两种作用域都会给tf.Variable()创建的变量加上词头，而tf.name_scope对tf.get_variable()创建的变量没有词头影响。 tf.name_scope(‘scope_name’) tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。 import tensorflow as tf # 与 tf.Variable() 结合使用。简化了命名 with tf.name_scope('conv1') as scope: weights1 = tf.Variable([1.0, 2.0], name='weights') bias1 = tf.Variable([0.3], name='bias') # 注意，这里的 with 和 python 中其他的 with 是不一样的 # 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行上面的代码 # 就会再生成其他命名空间 # 下面是在另外一个命名空间来定义变量的 with tf.name_scope('conv2') as scope: weights2 = tf.Variable([4.0, 2.0], name='weights') bias2 = tf.Variable([0.33], name='bias') # 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突 print(weights1.name) print(weights2.name) # ----------------- # conv1/weights:0 # conv2/weights:0 tf.variable_scope(‘scope_name’","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#tensorflow之优化器"},{"categories":["深度学习"],"content":" tensorflow的基础知识 tensorflow基础概念之Graph(tf.Graph) Graph是一个TensorFlow的一种运算，被表示为一个数据流的图。 一个Graph包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点 每一个任务都需要一个图，即使不去手动的声明，tensorflow也会在后台默认的构建图，然后将操作添加到图里面。 一般情况下，我们只需要使用默认生成的图即可，特殊情况下，再去显示的声明多个图。 属性： building_function:Returns True iff this graph represents a function. finalized:返回True，要是这个图被终止了 graph_def_versions:The GraphDef version information of this graph. seed:The graph-level random seed of this graph. version:Returns a version number that increases as ops are added to the graph. 函数： add_to_collection(name,value)：存放值在给定名称的collection里面(因为collection不是sets,所以有可能一个值会添加很多次) . as_default()：返回一个上下文管理器,使得这个Graph对象成为当前默认的graph. finalize()：结束这个graph,使得他只读(read-only). tensorflow基础概念之Session(tf.Session) 运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境. Session是一个比较重要的东西，TensorFlow中只有让Graph（计算图）上的节点Session（会话）中执行，才会得到结果。Session的开启涉及真实的运算，因此比较消耗资源。在使用结束后，务必关闭Session。一般在使用过程中，我们可以通过with上下文管理器来使用Session。 # Using the context manager. with tf.Session() as sess: sess.run(...) 属性： graph：“投放”到session中的图 **graph_def：**图的描述 函数： tf.Session.init(target=”, graph=None, config=None)：Session构造函数，可以在声明Session的时候指定Graph，如果未指定，则使用默认图。 tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)：运行操作估算（计算）tensor。 tf.Session.close()：Session使用之后，一定要关闭 tf.as_default() ：返回一个上下文管理器，使得这个对象成为当前默认的session/使用with关键字然后可以在with关键字代码块中执行 tensorflow之激活函数 激活操作提供了在神经网络中使用的不同类型的非线性模型。包括光滑非线性模型(sigmoid, tanh, elu, softplus, and softsign)。连续但是不是处处可微的函数(relu, relu6, crelu and relu_x)。当然还有随机正则化 (dropout) ,所有的激活操作都是作用在每个元素上面的，输出一个tensor和输入的tensor又相同的形状和数据类型。 激活函数列表： tf.nn.relu tf.nn.relu6 tf.nn.crelu tf.nn.elu tf.nn.softplus tf.nn.softsign tf.nn.dropout tf.nn.bias_add tf.sigmoid tf.tanh tf.nn.softmax tf.nn.relu(features, name=None)： 计算修正线性单元:max(features,0) 如果，你不知道使用哪个激活函数，那么使用relu准没错。 tf.nn.softmax(logits,dim=-1,name=None)： 计算softmax激活值，多分类输出层的激活函数。 tensorflow之优化器 深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化，此处记录一下tensorflow的优化器api。 优化器列表： tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.AdadeltaOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer tf.train.Optimizer： 优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类。 tf.train.GradientDescentOptimizer(learning_rate,use_locking=False,name=‘GradientDescent’) ： 作用：创建一个梯度下降优化器对象 参数： learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=‘Adadelta’) 作用：构造一个使用Adadelta算法的优化器 参数： learning_rate: tensor或者浮点数，学习率 rho: tensor或者浮点数. 优化参数 epsilon: tensor或者浮点数. 优化参数 use_locking: If True use locks for update operations. name: 【可选】这个操作的名字，默认是”Adadelta” tensorflow变量作用域机制 在深度学习中，我们可能需要用到大量的变量集，而且这些变量集可能在多处都要用到。例如，训练模型时，训练参数如权重（weights）、偏置（biases）等已经定下来，要拿到验证集去验证，我们自然希望这些参数是同一组。以往写简单的程序，可能使用全局限量就可以了，但在深度学习中，这显然是不行的，一方面不便管理，另外这样一来代码的封装性受到极大影响。因此，TensorFlow提供了一种变量管理方法：变量作用域机制，以此解决上面出现的问题。 在Tensoflow中，提供了两种作用域： 命名域(name scope)：通过tf.name_scope()来实现； 变量域（variable scope）：通过tf.variable_scope()来实现；可以通过设置reuse 标志以及初始化方式来影响域下的变量。 这两种作用域都会给tf.Variable()创建的变量加上词头，而tf.name_scope对tf.get_variable()创建的变量没有词头影响。 tf.name_scope(‘scope_name’) tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。 import tensorflow as tf # 与 tf.Variable() 结合使用。简化了命名 with tf.name_scope('conv1') as scope: weights1 = tf.Variable([1.0, 2.0], name='weights') bias1 = tf.Variable([0.3], name='bias') # 注意，这里的 with 和 python 中其他的 with 是不一样的 # 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行上面的代码 # 就会再生成其他命名空间 # 下面是在另外一个命名空间来定义变量的 with tf.name_scope('conv2') as scope: weights2 = tf.Variable([4.0, 2.0], name='weights') bias2 = tf.Variable([0.33], name='bias') # 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突 print(weights1.name) print(weights2.name) # ----------------- # conv1/weights:0 # conv2/weights:0 tf.variable_scope(‘scope_name’","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#tensorflow变量作用域机制"},{"categories":["深度学习"],"content":" tensorflow的基础知识 tensorflow基础概念之Graph(tf.Graph) Graph是一个TensorFlow的一种运算，被表示为一个数据流的图。 一个Graph包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点 每一个任务都需要一个图，即使不去手动的声明，tensorflow也会在后台默认的构建图，然后将操作添加到图里面。 一般情况下，我们只需要使用默认生成的图即可，特殊情况下，再去显示的声明多个图。 属性： building_function:Returns True iff this graph represents a function. finalized:返回True，要是这个图被终止了 graph_def_versions:The GraphDef version information of this graph. seed:The graph-level random seed of this graph. version:Returns a version number that increases as ops are added to the graph. 函数： add_to_collection(name,value)：存放值在给定名称的collection里面(因为collection不是sets,所以有可能一个值会添加很多次) . as_default()：返回一个上下文管理器,使得这个Graph对象成为当前默认的graph. finalize()：结束这个graph,使得他只读(read-only). tensorflow基础概念之Session(tf.Session) 运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境. Session是一个比较重要的东西，TensorFlow中只有让Graph（计算图）上的节点Session（会话）中执行，才会得到结果。Session的开启涉及真实的运算，因此比较消耗资源。在使用结束后，务必关闭Session。一般在使用过程中，我们可以通过with上下文管理器来使用Session。 # Using the context manager. with tf.Session() as sess: sess.run(...) 属性： graph：“投放”到session中的图 **graph_def：**图的描述 函数： tf.Session.init(target=”, graph=None, config=None)：Session构造函数，可以在声明Session的时候指定Graph，如果未指定，则使用默认图。 tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)：运行操作估算（计算）tensor。 tf.Session.close()：Session使用之后，一定要关闭 tf.as_default() ：返回一个上下文管理器，使得这个对象成为当前默认的session/使用with关键字然后可以在with关键字代码块中执行 tensorflow之激活函数 激活操作提供了在神经网络中使用的不同类型的非线性模型。包括光滑非线性模型(sigmoid, tanh, elu, softplus, and softsign)。连续但是不是处处可微的函数(relu, relu6, crelu and relu_x)。当然还有随机正则化 (dropout) ,所有的激活操作都是作用在每个元素上面的，输出一个tensor和输入的tensor又相同的形状和数据类型。 激活函数列表： tf.nn.relu tf.nn.relu6 tf.nn.crelu tf.nn.elu tf.nn.softplus tf.nn.softsign tf.nn.dropout tf.nn.bias_add tf.sigmoid tf.tanh tf.nn.softmax tf.nn.relu(features, name=None)： 计算修正线性单元:max(features,0) 如果，你不知道使用哪个激活函数，那么使用relu准没错。 tf.nn.softmax(logits,dim=-1,name=None)： 计算softmax激活值，多分类输出层的激活函数。 tensorflow之优化器 深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化，此处记录一下tensorflow的优化器api。 优化器列表： tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.AdadeltaOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer tf.train.Optimizer： 优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类。 tf.train.GradientDescentOptimizer(learning_rate,use_locking=False,name=‘GradientDescent’) ： 作用：创建一个梯度下降优化器对象 参数： learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=‘Adadelta’) 作用：构造一个使用Adadelta算法的优化器 参数： learning_rate: tensor或者浮点数，学习率 rho: tensor或者浮点数. 优化参数 epsilon: tensor或者浮点数. 优化参数 use_locking: If True use locks for update operations. name: 【可选】这个操作的名字，默认是”Adadelta” tensorflow变量作用域机制 在深度学习中，我们可能需要用到大量的变量集，而且这些变量集可能在多处都要用到。例如，训练模型时，训练参数如权重（weights）、偏置（biases）等已经定下来，要拿到验证集去验证，我们自然希望这些参数是同一组。以往写简单的程序，可能使用全局限量就可以了，但在深度学习中，这显然是不行的，一方面不便管理，另外这样一来代码的封装性受到极大影响。因此，TensorFlow提供了一种变量管理方法：变量作用域机制，以此解决上面出现的问题。 在Tensoflow中，提供了两种作用域： 命名域(name scope)：通过tf.name_scope()来实现； 变量域（variable scope）：通过tf.variable_scope()来实现；可以通过设置reuse 标志以及初始化方式来影响域下的变量。 这两种作用域都会给tf.Variable()创建的变量加上词头，而tf.name_scope对tf.get_variable()创建的变量没有词头影响。 tf.name_scope(‘scope_name’) tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。 import tensorflow as tf # 与 tf.Variable() 结合使用。简化了命名 with tf.name_scope('conv1') as scope: weights1 = tf.Variable([1.0, 2.0], name='weights') bias1 = tf.Variable([0.3], name='bias') # 注意，这里的 with 和 python 中其他的 with 是不一样的 # 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行上面的代码 # 就会再生成其他命名空间 # 下面是在另外一个命名空间来定义变量的 with tf.name_scope('conv2') as scope: weights2 = tf.Variable([4.0, 2.0], name='weights') bias2 = tf.Variable([0.33], name='bias') # 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突 print(weights1.name) print(weights2.name) # ----------------- # conv1/weights:0 # conv2/weights:0 tf.variable_scope(‘scope_name’","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#tfname_scopescope_name"},{"categories":["深度学习"],"content":" tensorflow的基础知识 tensorflow基础概念之Graph(tf.Graph) Graph是一个TensorFlow的一种运算，被表示为一个数据流的图。 一个Graph包含一些操作（Operation）对象，这些对象是计算节点。前面说过的Tensor对象，则是表示在不同的操作（operation）间的数据节点 每一个任务都需要一个图，即使不去手动的声明，tensorflow也会在后台默认的构建图，然后将操作添加到图里面。 一般情况下，我们只需要使用默认生成的图即可，特殊情况下，再去显示的声明多个图。 属性： building_function:Returns True iff this graph represents a function. finalized:返回True，要是这个图被终止了 graph_def_versions:The GraphDef version information of this graph. seed:The graph-level random seed of this graph. version:Returns a version number that increases as ops are added to the graph. 函数： add_to_collection(name,value)：存放值在给定名称的collection里面(因为collection不是sets,所以有可能一个值会添加很多次) . as_default()：返回一个上下文管理器,使得这个Graph对象成为当前默认的graph. finalize()：结束这个graph,使得他只读(read-only). tensorflow基础概念之Session(tf.Session) 运行TensorFLow操作（operations）的类,一个Seesion包含了操作对象执行的环境. Session是一个比较重要的东西，TensorFlow中只有让Graph（计算图）上的节点Session（会话）中执行，才会得到结果。Session的开启涉及真实的运算，因此比较消耗资源。在使用结束后，务必关闭Session。一般在使用过程中，我们可以通过with上下文管理器来使用Session。 # Using the context manager. with tf.Session() as sess: sess.run(...) 属性： graph：“投放”到session中的图 **graph_def：**图的描述 函数： tf.Session.init(target=”, graph=None, config=None)：Session构造函数，可以在声明Session的时候指定Graph，如果未指定，则使用默认图。 tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)：运行操作估算（计算）tensor。 tf.Session.close()：Session使用之后，一定要关闭 tf.as_default() ：返回一个上下文管理器，使得这个对象成为当前默认的session/使用with关键字然后可以在with关键字代码块中执行 tensorflow之激活函数 激活操作提供了在神经网络中使用的不同类型的非线性模型。包括光滑非线性模型(sigmoid, tanh, elu, softplus, and softsign)。连续但是不是处处可微的函数(relu, relu6, crelu and relu_x)。当然还有随机正则化 (dropout) ,所有的激活操作都是作用在每个元素上面的，输出一个tensor和输入的tensor又相同的形状和数据类型。 激活函数列表： tf.nn.relu tf.nn.relu6 tf.nn.crelu tf.nn.elu tf.nn.softplus tf.nn.softsign tf.nn.dropout tf.nn.bias_add tf.sigmoid tf.tanh tf.nn.softmax tf.nn.relu(features, name=None)： 计算修正线性单元:max(features,0) 如果，你不知道使用哪个激活函数，那么使用relu准没错。 tf.nn.softmax(logits,dim=-1,name=None)： 计算softmax激活值，多分类输出层的激活函数。 tensorflow之优化器 深度学习常见的是对于梯度的优化，也就是说，优化器最后其实就是各种对于梯度下降算法的优化，此处记录一下tensorflow的优化器api。 优化器列表： tf.train.Optimizer tf.train.GradientDescentOptimizer tf.train.AdagradOptimizer tf.train.AdadeltaOptimizer tf.train.MomentumOptimizer tf.train.AdamOptimizer tf.train.FtrlOptimizer tf.train.RMSPropOptimizer tf.train.Optimizer： 优化器（optimizers）类的基类。这个类定义了在训练模型的时候添加一个操作的API。你基本上不会直接使用这个类。 tf.train.GradientDescentOptimizer(learning_rate,use_locking=False,name=‘GradientDescent’) ： 作用：创建一个梯度下降优化器对象 参数： learning_rate: A Tensor or a floating point value. 要使用的学习率 use_locking: 要是True的话，就对于更新操作（update operations.）使用锁 name: 名字，可选，默认是”GradientDescent”. tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-08, use_locking=False, name=‘Adadelta’) 作用：构造一个使用Adadelta算法的优化器 参数： learning_rate: tensor或者浮点数，学习率 rho: tensor或者浮点数. 优化参数 epsilon: tensor或者浮点数. 优化参数 use_locking: If True use locks for update operations. name: 【可选】这个操作的名字，默认是”Adadelta” tensorflow变量作用域机制 在深度学习中，我们可能需要用到大量的变量集，而且这些变量集可能在多处都要用到。例如，训练模型时，训练参数如权重（weights）、偏置（biases）等已经定下来，要拿到验证集去验证，我们自然希望这些参数是同一组。以往写简单的程序，可能使用全局限量就可以了，但在深度学习中，这显然是不行的，一方面不便管理，另外这样一来代码的封装性受到极大影响。因此，TensorFlow提供了一种变量管理方法：变量作用域机制，以此解决上面出现的问题。 在Tensoflow中，提供了两种作用域： 命名域(name scope)：通过tf.name_scope()来实现； 变量域（variable scope）：通过tf.variable_scope()来实现；可以通过设置reuse 标志以及初始化方式来影响域下的变量。 这两种作用域都会给tf.Variable()创建的变量加上词头，而tf.name_scope对tf.get_variable()创建的变量没有词头影响。 tf.name_scope(‘scope_name’) tf.name_scope 主要结合 tf.Variable() 来使用，方便参数命名管理。 import tensorflow as tf # 与 tf.Variable() 结合使用。简化了命名 with tf.name_scope('conv1') as scope: weights1 = tf.Variable([1.0, 2.0], name='weights') bias1 = tf.Variable([0.3], name='bias') # 注意，这里的 with 和 python 中其他的 with 是不一样的 # 执行完 with 里边的语句之后，这个 conv1/ 和 conv2/ 空间还是在内存中的。这时候如果再次执行上面的代码 # 就会再生成其他命名空间 # 下面是在另外一个命名空间来定义变量的 with tf.name_scope('conv2') as scope: weights2 = tf.Variable([4.0, 2.0], name='weights') bias2 = tf.Variable([0.33], name='bias') # 所以，实际上weights1 和 weights2 这两个引用名指向了不同的空间，不会冲突 print(weights1.name) print(weights2.name) # ----------------- # conv1/weights:0 # conv2/weights:0 tf.variable_scope(‘scope_name’","date":"2018-12-28","objectID":"/tensorflow%E4%B9%8Bgraphsession/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Graph、Session","uri":"/tensorflow%E4%B9%8Bgraphsession/#tfvariable_scopescope_name"},{"categories":["深度学习"],"content":" 为什么选择tensorflow TensorFlow 无可厚非地能被认定为 神经网络中最好用的库之一。它擅长的任务就是训练深度神经网络.通过使用TensorFlow我们就可以快速的入门神经网络，大大降低了深度学习（也就是深度神经网络）的开发成本和开发难度.。TensorFlow 的开源性，让所有人都能使用并且维护， 巩固它. 使它能迅速更新, 提升。 现在新版本的tensorflow除了支持Graph Execution之外，还提供了Eager Execution。 ","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:1","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#为什么选择tensorflow"},{"categories":["深度学习"],"content":" tensorflow编程思想 TensorFlow 使用图来表示计算任务. 图中的节点被称之为 op (operation 的缩写). 一个 op获得 0 个或多个 Tensor , 执行计算, 产生 0 个或多个 Tensor . 每个 Tensor 是一个类型化的多维数组.tensor也是tensorflow中的核心数据类型。 一个 TensorFlow 图（graph）描述了计算的过程. 为了进行计算, 图必须在会话（session）里被启动. 会话将图的op分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. TensorFlow 程序通常被组织成一个构建阶段和一个执行阶段. 在构建阶段, op 的执行步骤被描述成一个图. 在执行阶段, 使用会话执行执行图中的op.例如,通常在构建阶段创建一个图来表示和训练神经网络,然后在执行阶段反复执行图中的训练 op. ","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:2","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensorflow编程思想"},{"categories":["深度学习"],"content":" tensorflow的安装 Tensorflow 的安装方式很多. 比如官网提供的: Pip 安装 Virtualenv 安装 Anaconda 安装 Docker 安装 从安装源 安装 pip的安装 pip install tensorflow #python2 pip3 install tensorflow #python3 pip3 install tensorflow==x.x # 安装的同时指定版本号 pip3 install tensorflow-gpu #安装tenforflow-gpu版本，注意需要首先配置cuda和cudnn conda的安装 conda install tensorflow # 安装cpu版本 conda install tensorflow-gpu # 安装gpu版本，这种方法conda会自动配置cuda和cudnn 测试tensorflow import tensorflow as tf ","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:3","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensorflow的安装"},{"categories":["深度学习"],"content":" tensorflow的安装 Tensorflow 的安装方式很多. 比如官网提供的: Pip 安装 Virtualenv 安装 Anaconda 安装 Docker 安装 从安装源 安装 pip的安装 pip install tensorflow #python2 pip3 install tensorflow #python3 pip3 install tensorflow==x.x # 安装的同时指定版本号 pip3 install tensorflow-gpu #安装tenforflow-gpu版本，注意需要首先配置cuda和cudnn conda的安装 conda install tensorflow # 安装cpu版本 conda install tensorflow-gpu # 安装gpu版本，这种方法conda会自动配置cuda和cudnn 测试tensorflow import tensorflow as tf ","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:3","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#pip的安装"},{"categories":["深度学习"],"content":" tensorflow的安装 Tensorflow 的安装方式很多. 比如官网提供的: Pip 安装 Virtualenv 安装 Anaconda 安装 Docker 安装 从安装源 安装 pip的安装 pip install tensorflow #python2 pip3 install tensorflow #python3 pip3 install tensorflow==x.x # 安装的同时指定版本号 pip3 install tensorflow-gpu #安装tenforflow-gpu版本，注意需要首先配置cuda和cudnn conda的安装 conda install tensorflow # 安装cpu版本 conda install tensorflow-gpu # 安装gpu版本，这种方法conda会自动配置cuda和cudnn 测试tensorflow import tensorflow as tf ","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:3","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#conda的安装"},{"categories":["深度学习"],"content":" tensorflow的安装 Tensorflow 的安装方式很多. 比如官网提供的: Pip 安装 Virtualenv 安装 Anaconda 安装 Docker 安装 从安装源 安装 pip的安装 pip install tensorflow #python2 pip3 install tensorflow #python3 pip3 install tensorflow==x.x # 安装的同时指定版本号 pip3 install tensorflow-gpu #安装tenforflow-gpu版本，注意需要首先配置cuda和cudnn conda的安装 conda install tensorflow # 安装cpu版本 conda install tensorflow-gpu # 安装gpu版本，这种方法conda会自动配置cuda和cudnn 测试tensorflow import tensorflow as tf ","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:3","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#测试tensorflow"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensorflow的基础知识"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensorflow的处理结构"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensorflow基础概念之tensortftensor"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensorflow基础概念之variabletfvariable"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensorflow基本函数讲解"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensorflow基础实例"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#常量与图"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#tensor和变量"},{"categories":["深度学习"],"content":" tensorflow的基础知识 **图（Graph）：**用来表示计算任务，也就我们要做的一些操作。 **会话（Session）：**建立会话，此时会生成一张空图；在会话中添加节点和边，形成一张图，一个会话可以有多个图，通过执行这些图得到结果。如果把每个图看做一个车床，那会话就是一个车间，里面有若干个车床，用来把数据生产成结果。 **Tensor：**用来表示数据，是我们的原料。 **变量（Variable）：**用来记录一些数据和状态，是我们的容器。 **注入机制(feed):**通过占位符向模式中传入数据。 取回机制(fetch)：从模式中取得结果。 形象的比喻是：把会话看做车间，图看做车床，里面用Tensor做原料，变量做容器，feed和fetch做铲子，把数据加工成我们的结果。 Tensorflow是基于graph的并行计算模型。举个例子，计算a=(b+c)∗(c+2)，我们可以将算式拆分成一下： d = b + c e = c + 2 a = d * e 那么将算式转换成graph后： 将一个简单的算式搞成这样确实大材小用，但是我们可以通过这个例子发现：d=b+c和e=c+2是不相关的，也就是可以并行计算。对于更复杂的CNN和RNN，graph的并行计算的能力将得到更好的展现。 tensorflow的处理结构 Tensorflow 首先要定义神经网络的结构，然后再把数据放入结构当中去运算和 training。 因为TensorFlow是采用数据流图（data　flow　graphs）来计算, 所以首先我们得创建一个数据流流图，然后再将我们的数据（数据以张量(tensor)的形式存在）放在数据流图中计算，节点（Nodes）在下图中表示数学操作，图中的线（edges）则表示在节点间相互联系的多维数据数组，即张量（tensor)。训练模型时tensor会不断的从数据流图中的一个节点flow到另一节点, 这就是TensorFlow名字的由来。 如果输入tensor的维度是5000×645000×64，表示有5000个训练样本，每个样本有64个特征，则输入层必须有64个node来接受这些特征。 上图表示的三层网络包括：输入层(图中的input)、隐藏层(这里取名为ReLU layer表示它的激活函数是ReLU）、输出层(图中的Logit Layer)。 可以看到，每一层中都有相关tensor流入Gradient节点计算梯度，然后这些梯度tensor进入SGD Trainer节点进行网络优化（也就是update网络参数）。 Tensorflow正是通过graph表示神经网络，实现网络的并行计算，提高效率。下面我们将通过一个简单的例子来介绍TensorFlow的基础语法。 tensorflow基础概念之Tensor(tf.Tensor)Tensor类是最基本最核心的数据结构了，它表示的是一个操作的输出，但是他并不接收操作输出的值，而是提供了在TensorFlow的Session中计算这些值的方法。 Tensor类主要有两个目的： 一个Tensor能够作为一个输入来传递给其他的操作（Operation），由此构造了一个连接不同操作的数据流，使得TensorFLow能够执行一个表示很大，多步骤计算的图。 在图被“投放”进一个Session中后，Tensor的值能够通过把Tensor传到Seesion.run（）这个函数里面去得到结果。相同的，也可以用t.eval（）这个函数，其中的t就是你的tensor啦，这个函数可以算是tf.get_default_session().run(t)的简便写法。 举例说明： import tensorflow as tf #build a graph print(\"build a graph\") a=tf.constant([[1,2],[3,4]]) b=tf.constant([[1,1],[0,1]]) print(\"a:\",a) print(\"b:\",b) print(\"type of a:\",type(a)) c=tf.matmul(a,b) # 矩阵乘法==np.dot(a,b) print(\"c:\",c) print(\"\\n\") #construct a 'Session' to excute the graph sess=tf.Session() # Execute the graph and store the value that `c` represents in `result`. print(\"excuted in Session\") result_a=sess.run(a) result_a2=a.eval(session=sess) print(\"result_a:\\n\",result_a) print(\"result_a2:\\n\",result_a2) result_b=sess.run(b) print(\"result_b:\\n\",result_b) result_c=sess.run(c) print(\"result_c:\\n\",result_c) 整个程序分为3个过程，首先是构建计算图，一开始就用constant（）函数生成了两个tensor分别是a和b（下面有对于constant函数的介绍），然后我们试图直接输出a和b，但是输出并不是两个矩阵，而是各自度对应的tensor类型。然后我们通过print(\"type of a:\",type(a)) 这句话来输出a的类型，果然是tensor类型（tensor类）。然后我们把a和b这两个tensor传递给tf.matmul（）函数，这个函数是用来计算矩阵乘法的函数。返回的依然是tensor用c来接受。到这里为止，我们可以知道，tensor里面并不负责储存值，想要得到值，得去Session中run。我们可以把这部分看做是创建了一个图但是没有运行这个图。 然后我们构造了一个Session的对象用来执行图，sess=tf.Session() 。 最后就是在session里面执行之前的东西了，可以把一个tensor传递到session.run()里面去，得到其值。等价的也可以用result_a2=a.eval(session=sess) 来得到。则返回的结果是numpy.ndarray。 Tensor类的属性 **device:**表示tensor将被产生的设备名称 dtype：tensor元素类型 graph：这个tensor被哪个图所有 name:这个tensor的名称 op：产生这个tensor作为输出的操作（Operation） shape：tensor的形状（返回的是tf.TensorShape这个表示tensor形状的类） value_index:表示这个tensor在其操作结果中的索引 函数： tf.Tensor.consumers()：返回消耗这个tensor的操作列表 tf.Tensor.eval(feed_dict=None, session=None)：在一个Seesion里面“评估”tensor的值（其实就是计算),在激发tensor.eval()这个函数之前，tensor的图必须已经投入到session里面，或者一个默认的session是有效的，或者显式指定session。 tf.Tensor.get_shape():返回tensor的形状，类型是TensorShape。 tf.Tensor.set_shape(shape):设置更新这个tensor的形状。 tensorflow基础概念之Variable（tf.Variable） 通过构造一个Variable类的实例在图中添加一个变量（variable） Variable()这个构造函数需要初始值，这个初始值可以是一个任何类型任何形状的Tensor，初始值的形状和类型决定了这个变量的形状和类型。构造之后，这个变量的形状和类型就固定了，他的值可以通过assign()函数（或者assign类似的函数）来改变。如果你想要在之后改变变量的形状，你就需要assign()函数同时变量的validate_shape=False和任何的Tensor一样，通过**Variable()**创造的变量能够作为图中其他操作的输入使用。你也能够在图中添加节点，通过对变量进行算术操作。 举例说明： # Variable import numpy as np import tensorflow as tf # 定义变量 w = tf.Variable(initial_value=[[1,2],[3,4]],dtype=tf.float32) x = tf.Variable(initial_value=[[1,1],[1,1]],dtype=tf.float32) y = tf.matmul(w,x) # 矩阵乘法==np.dot(w,x) z = tf.sigmoid(y) print(z) # 初始化所有的变量 init = tf.global_variables_initializer() with tf.Session() as session: session.run(init) z = session.run(z) print(z) 属性： **device:**这个变量的device **dtype:**变量的元素类型 **graph:**存放变量的图 **initial_value:**这个变量的初始值 **initializer :**这个变量的初始化器 **name:**这个变脸的名","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:4","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#fetches和feeds"},{"categories":["深度学习"],"content":" 参考资料 莫烦tensorflow教程 ","date":"2018-12-27","objectID":"/tensorflow%E4%B9%8Btensorvariable/:0:5","series":null,"tags":["python","深度学习","tensorflow"],"title":"tensorflow之Tensor、Variable","uri":"/tensorflow%E4%B9%8Btensorvariable/#参考资料"},{"categories":["深度学习"],"content":" 计算图与张量 要说Pytorch/Tensorflow/Keras，就不能不提它的符号主义特性 事实上，Pytorch也好，Tensorflow也好，其实是一款符号主义的计算框架，未必是专为深度学习设计的。假如你有一个与深度学习完全无关的计算任务想运行在GPU上，你完全可以通过Pytorch/Tensorflow编写和运行。 假如我们要求两个数a和b的和，通常只要把值赋值给a和b，然后计算a+b就可以了，正常人类都是这么写的： a=3 b=5 z = a + b 运行到第一行，a是3，运行到第2行，b是5，然后运行第三行，电脑把a和b的值加起来赋给z了。 计算图的方式： a+b这个计算任务，可以分为三步。 声明两个变量a，b。建立输出变量z 确立a，b和z的计算关系，z=a+b 将两个数值a和b赋值到变量中，计算结果z 这种“先确定符号以及符号之间的计算关系，然后才放数据进去计算”的办法，就是符号式编程。当你声明a和b时，它们里面是空的。当你确立z=a+b的计算关系时，a，b和z仍然是空的，只有当你真的把数据放入a和b了，程序才开始做计算。 符号之间的运算关系，就称为运算图。 符号式计算的一大优点是，当确立了输入和输出的计算关系后，在进行运算前我们可以对这种运算关系进行自动化简，从而减少计算量，提高计算速度。另一个优势是，运算图一旦确定，整个计算过程就都清楚了，可以用内存复用的方式减少程序占用的内存。 在Keras，Pytorch和Tensorflow中，参与符号运算的那些变量统一称作张量。张量是矩阵的进一步推广。 规模最小的张量是0阶张量，即标量，也就是一个数。 当我们把一些数有序的排列起来，就形成了1阶张量，也就是一个向量 如果我们继续把一组向量有序的排列起来，就形成了2阶张量，也就是一个矩阵 把矩阵摞起来，就是3阶张量，我们可以称为一个立方体，具有3个颜色通道的彩色图片就是一个这样的立方体 把矩阵摞起来，好吧这次我们真的没有给它起别名了，就叫4阶张量了，不要去试图想像4阶张量是什么样子，它就是个数学上的概念。 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:1","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#计算图与张量"},{"categories":["深度学习"],"content":" Keras框架结构 backend：后端，对Tensorflow和Theano进行封装，完成低层的张量运算、计算图编译等 models：模型，模型是层的有序组合，也是层的“容器”，是“神经网络”的整体表示 layers：层，神经网络的层本质上规定了一种从输入张量到输出张量的计算规则，显然，整个神经网络的模型也是这样一种张量到张量的计算规则，因此keras的model是layer的子类。 上面的三个模块是Keras最为要紧和核心的三块内容，搭建一个神经网络，就只用上面的内容即可。注意的是，backend虽然很重要，但其内容多而杂，大部分内容都是被其他keras模块调用，而不是被用户直接使用。所以它不是新手马上就应该学的，初学Keras不妨先将backend放一旁，从model和layers学起。 为了训练神经网络，必须定义一个神经网络优化的目标和一套参数更新的方式，这部分就是目标函数和优化器： objectives：目标函数，规定了神经网络的优化方向 optimizers：优化器，规定了神经网络的参数如何更新 此外，Keras提供了一组模块用来对神经网络进行配置： initialization：初始化策略，规定了网络参数的初始化方法 regularizers：正则项，提供了一些用于参数正则的方法，以对抗过拟合 constraints：约束项，提供了对网络参数进行约束的方法 为了方便调试、分析和使用网络，处理数据，Keras提供了下面的模块： callbacks：回调函数，在网络训练的过程中返回一些预定义/自定义的信息 visualization：可视化，用于将网络结构绘制出来，以直观观察 preprocessing：提供了一组用于对文本、图像、序列信号进行预处理的函数 utils：常用函数库，比较重要的是utils.np_utils中的to_categorical，用于将1D标签转为one-hot的2D标签和convert_kernel函数，用于将卷积核在theano模式和Tensorflow模式之间转换。 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:2","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#keras框架结构"},{"categories":["深度学习"],"content":" api 详细介绍 backendbackend这个模块的主要作用，是对tensorflow和theano的底层张量运算进行了包装。用户不用关心具体执行张量运算的是theano还是tensorflow，就可以编写出能在两个框架下可以无缝对接的程序。backend中的函数要比文档里给出的多得多，完全就是一家百货商店。但一般情况下，文档里给出的那些就已经足够你完成大部分工作了，事实上就连文档里给出的函数大部分情况也不会用，这里提几个比较有用的函数： function：毫无疑问这估计是最有用的一个函数了，function用于将一个计算图（计算关系）编译为具体的函数。典型的使用场景是输出网络的中间层结果。 image_ordering和set_image_ordering：这组函数用于返回/设置图片的维度顺序，由于Theano和Tensorflow的图片维度顺序不一样，所以有时候需要获取/指定。典型应用是当希望网络自适应的根据使用的后端调整图片维度顺序时。 learning_phase：这个函数的主要作用是返回网络的运行状态，0代表测试，1代表训练。当你需要便携一个在训练和测试是行为不同的层（如Dropout）时，它会很有用。 int_shape：这是我最常用的一个函数，用于以整数tuple的形式返回张量的shape。要知道从前网络输出张量的shape是看都看不到的，int_shape可以在debug时起到很大作用。 gradients： 求损失函数关于变量的导数，也就是网络的反向计算过程。这个函数在不训练网络而只想用梯度做一点奇怪的事情的时候会很有用，如图像风格转移。 backend的其他大部分函数的函数名是望而知义的，什么max，min，equal，eval，zeros，ones，conv2d等等。函数的命名方式跟numpy差不多，下次想用时不妨先‘.’一下，说不定就有。 models/layers使用Keras最常见的目的，当然还是训练一个网络。之前说了网络就是张量到张量的映射，所以Keras的网络，其实是一个由多个子计算图构成的大计算图。当这些子计算图是顺序连接时，称为Sequential，否则就是一般的model，我们称为函数式模型。 模型有两套训练和测试的函数，一套是fit，evaluate等，另一套是fit_generator，evaluate_generator，前者适用于普通情况，后者适用于数据是以迭代器动态生成的情况。迭代器可以在内存/显存不足，实时动态数据提升进行网络训练，所以使用Keras的话，Python的迭代器这一部分是一定要掌握的内容。对模型而言，最核心的函数有两个： compile()：编译，模型在训练前必须编译，这个函数用于完成添加正则项啊，确定目标函数啊，确定优化器啊等等一系列模型配置功能。这个函数必须指定的参数是优化器和目标函数，经常还需要指定一个metrics来评价模型。 fit()/fit_generator()：用来训练模型，参数较多，是需要重点掌握的函数，对于keras使用者而言，这个函数的每一个参数都需要掌握。 另外，模型还有几个常用的属性和函数： layers：该属性是模型全部层对象的列表，是的就是一个普通的python list get_layer()：这个函数通过名字来返回模型中某个层对象 pop()：这个函数文档里没有，但是可以用。作用是弹出模型的最后一层，从前进行finetune时没有pop，大家一般用model.layers.pop()来完成同样的功能。 因为Model是Layer的子类，Layer的所有属性和方法也自动被Model所有 Keras的层对象是构筑模型的基石，除了卷积层，递归神经网络层，全连接层，激活层这种烂大街的Layer对象外，keras还有自己的一套东西： Advanced Activation：高级激活层，主要收录了包括leakyReLU，pReLU，ELU，SReLU等一系列高级激活函数，这些激活函数不是简单的element-wise计算，所以单独拿出来实现一下 Merge层：这个层用于将多个层对象的输出组合起来，支持级联、乘法、余弦等多种计算方式，它还有个小兄弟叫merge，这个函数完成与Merge相同的作用，但输入的对象是张量而不是层对象。 Lambda层：这是一个神奇的层，看名字就知道它用来把一个函数作用在输入张量上。这个层可以大大减少你的工作量，当你需要定义的新层的计算不是那么复杂的时候，可以通过lambda层来实现，而不用自己完全重写。 Highway/Maxout/AtrousConvolution2D层：这个就不多说了，懂的人自然懂，keras还是在一直跟着潮流走的 Wrapper层：Wrapper层用于将一个普通的层对象进行包装升级，赋予其更多功能。目前，Wrapper层里有一个TimeDistributed层，用于将普通的层包装为对时间序列输入处理的层，而Bidirectional可以将输入的递归神经网络层包装为双向的（如把LSTM做成BLSTM） Input：补一个特殊的层，Input，这个东西实际上是一个Keras tensor的占位符，主要用于在搭建Model模型时作为输入tensor使用，这个Input可以通过keras.layers来import。 stateful与unroll：Keras的递归神经网络层，如SimpleRNN，LSTM等，支持两种特殊的操作。一种是stateful，设置stateful为True意味着训练时每个batch的状态都会被重用于初始化下一个batch的初始状态。另一种是unroll，unroll可以将递归神经网络展开，以空间换取运行时间。 Keras的layers对象还有一些有用的属性和方法: name：别小看这个，从茫茫层海中搜索一个特定的层，如果你对数数没什么信心，最好是name配合get_layer()来用。 trainable：这个参数确定了层是可训练的还是不可训练的，在迁移学习中我们经常需要把某些层冻结起来而finetune别的层，冻结这个动作就是通过设置trainable来实现的。 input/output：这两个属性是层的输入和输出张量，是Keras tensor的对象，这两个属性在你需要获取中间层输入输出时非常有用 get_weights/set_weights：这是两个方法用于手动取出和载入层的参数，set_weights传入的权重必须与get_weights返回的权重具有同样的shape，一般可以用get_weights来看权重shape，用set_weights来载入权重 在Keras中经常有的一个需求是需要自己编写一个新的层，如果你的计算比较简单，那可以尝试通过Lambda层来解决，如果你不得不编写一个自己的层，那也不是什么大不了的事儿。要在Keras中编写一个自己的层，需要开一个从Layer（或其他层）继承的类，除了__init__以为你需要覆盖三个函数： build，这个函数用来确立这个层都有哪些参数，哪些参数是可训练的哪些参数是不可训练的。 call，这个函数在调用层对象时自动使用，里面就是该层的计算逻辑，或计算图了。显然，这个层的核心应该是一段符号式的输入张量到输出张量的计算过程。 get_output_shape_for：如果你的层计算后，输入张量和输出张量的shape不一致，那么你需要把这个函数也重新写一下，返回输出张量的shape，以保证Keras可以进行shape的自动推断 由于keras是python编写的，因此，我们可以随时查看keras其他层的源码，参考如何编写 优化器，目标函数，初始化策略 objectives是优化目标， 它本质上是一个从张量到数值的函数，当然，是用符号式编程表达的。具体的优化目标有mse，mae，交叉熵等等等等，根据具体任务取用即可，当然，也支持自己编写。需要特别说明的一点是，如果选用categorical_crossentropy作为目标函数，需要将标签转换为one-hot编码的形式，这个动作通过utils.np_utils.to_categorical来完成 optimizers是优化器，模型是可以传入优化器对象的，你可以自己配置一个SGD，然后将它传入模型中，参数clipnorm和clipvalue，用来对梯度进行裁剪。 activation是激活函数，这部分的内容一般不直接使用，而是通过激活层Activation来调用，此处的激活函数是普通的element-wise激活函数 callback是回调函数，这其实是一个比较重要的模块，回调函数不是一个函数而是一个类，用于在训练过程中收集信息或进行某种动作。比如我们经常想画一下每个epoch的训练误差和测试误差，那这些信息就需要在回调函数中收集。预定义的回调函数中CheckModelpoint，History和EarlyStopping都是比较重要和常用的。其中CheckPoint用于保存模型，History记录了训练和测试的信息，EarlyStopping用于在已经收敛时提前结束训练。 PS:History是模型训练函数fit的返回值 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:3","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#api-详细介绍"},{"categories":["深度学习"],"content":" api 详细介绍 backendbackend这个模块的主要作用，是对tensorflow和theano的底层张量运算进行了包装。用户不用关心具体执行张量运算的是theano还是tensorflow，就可以编写出能在两个框架下可以无缝对接的程序。backend中的函数要比文档里给出的多得多，完全就是一家百货商店。但一般情况下，文档里给出的那些就已经足够你完成大部分工作了，事实上就连文档里给出的函数大部分情况也不会用，这里提几个比较有用的函数： function：毫无疑问这估计是最有用的一个函数了，function用于将一个计算图（计算关系）编译为具体的函数。典型的使用场景是输出网络的中间层结果。 image_ordering和set_image_ordering：这组函数用于返回/设置图片的维度顺序，由于Theano和Tensorflow的图片维度顺序不一样，所以有时候需要获取/指定。典型应用是当希望网络自适应的根据使用的后端调整图片维度顺序时。 learning_phase：这个函数的主要作用是返回网络的运行状态，0代表测试，1代表训练。当你需要便携一个在训练和测试是行为不同的层（如Dropout）时，它会很有用。 int_shape：这是我最常用的一个函数，用于以整数tuple的形式返回张量的shape。要知道从前网络输出张量的shape是看都看不到的，int_shape可以在debug时起到很大作用。 gradients： 求损失函数关于变量的导数，也就是网络的反向计算过程。这个函数在不训练网络而只想用梯度做一点奇怪的事情的时候会很有用，如图像风格转移。 backend的其他大部分函数的函数名是望而知义的，什么max，min，equal，eval，zeros，ones，conv2d等等。函数的命名方式跟numpy差不多，下次想用时不妨先‘.’一下，说不定就有。 models/layers使用Keras最常见的目的，当然还是训练一个网络。之前说了网络就是张量到张量的映射，所以Keras的网络，其实是一个由多个子计算图构成的大计算图。当这些子计算图是顺序连接时，称为Sequential，否则就是一般的model，我们称为函数式模型。 模型有两套训练和测试的函数，一套是fit，evaluate等，另一套是fit_generator，evaluate_generator，前者适用于普通情况，后者适用于数据是以迭代器动态生成的情况。迭代器可以在内存/显存不足，实时动态数据提升进行网络训练，所以使用Keras的话，Python的迭代器这一部分是一定要掌握的内容。对模型而言，最核心的函数有两个： compile()：编译，模型在训练前必须编译，这个函数用于完成添加正则项啊，确定目标函数啊，确定优化器啊等等一系列模型配置功能。这个函数必须指定的参数是优化器和目标函数，经常还需要指定一个metrics来评价模型。 fit()/fit_generator()：用来训练模型，参数较多，是需要重点掌握的函数，对于keras使用者而言，这个函数的每一个参数都需要掌握。 另外，模型还有几个常用的属性和函数： layers：该属性是模型全部层对象的列表，是的就是一个普通的python list get_layer()：这个函数通过名字来返回模型中某个层对象 pop()：这个函数文档里没有，但是可以用。作用是弹出模型的最后一层，从前进行finetune时没有pop，大家一般用model.layers.pop()来完成同样的功能。 因为Model是Layer的子类，Layer的所有属性和方法也自动被Model所有 Keras的层对象是构筑模型的基石，除了卷积层，递归神经网络层，全连接层，激活层这种烂大街的Layer对象外，keras还有自己的一套东西： Advanced Activation：高级激活层，主要收录了包括leakyReLU，pReLU，ELU，SReLU等一系列高级激活函数，这些激活函数不是简单的element-wise计算，所以单独拿出来实现一下 Merge层：这个层用于将多个层对象的输出组合起来，支持级联、乘法、余弦等多种计算方式，它还有个小兄弟叫merge，这个函数完成与Merge相同的作用，但输入的对象是张量而不是层对象。 Lambda层：这是一个神奇的层，看名字就知道它用来把一个函数作用在输入张量上。这个层可以大大减少你的工作量，当你需要定义的新层的计算不是那么复杂的时候，可以通过lambda层来实现，而不用自己完全重写。 Highway/Maxout/AtrousConvolution2D层：这个就不多说了，懂的人自然懂，keras还是在一直跟着潮流走的 Wrapper层：Wrapper层用于将一个普通的层对象进行包装升级，赋予其更多功能。目前，Wrapper层里有一个TimeDistributed层，用于将普通的层包装为对时间序列输入处理的层，而Bidirectional可以将输入的递归神经网络层包装为双向的（如把LSTM做成BLSTM） Input：补一个特殊的层，Input，这个东西实际上是一个Keras tensor的占位符，主要用于在搭建Model模型时作为输入tensor使用，这个Input可以通过keras.layers来import。 stateful与unroll：Keras的递归神经网络层，如SimpleRNN，LSTM等，支持两种特殊的操作。一种是stateful，设置stateful为True意味着训练时每个batch的状态都会被重用于初始化下一个batch的初始状态。另一种是unroll，unroll可以将递归神经网络展开，以空间换取运行时间。 Keras的layers对象还有一些有用的属性和方法: name：别小看这个，从茫茫层海中搜索一个特定的层，如果你对数数没什么信心，最好是name配合get_layer()来用。 trainable：这个参数确定了层是可训练的还是不可训练的，在迁移学习中我们经常需要把某些层冻结起来而finetune别的层，冻结这个动作就是通过设置trainable来实现的。 input/output：这两个属性是层的输入和输出张量，是Keras tensor的对象，这两个属性在你需要获取中间层输入输出时非常有用 get_weights/set_weights：这是两个方法用于手动取出和载入层的参数，set_weights传入的权重必须与get_weights返回的权重具有同样的shape，一般可以用get_weights来看权重shape，用set_weights来载入权重 在Keras中经常有的一个需求是需要自己编写一个新的层，如果你的计算比较简单，那可以尝试通过Lambda层来解决，如果你不得不编写一个自己的层，那也不是什么大不了的事儿。要在Keras中编写一个自己的层，需要开一个从Layer（或其他层）继承的类，除了__init__以为你需要覆盖三个函数： build，这个函数用来确立这个层都有哪些参数，哪些参数是可训练的哪些参数是不可训练的。 call，这个函数在调用层对象时自动使用，里面就是该层的计算逻辑，或计算图了。显然，这个层的核心应该是一段符号式的输入张量到输出张量的计算过程。 get_output_shape_for：如果你的层计算后，输入张量和输出张量的shape不一致，那么你需要把这个函数也重新写一下，返回输出张量的shape，以保证Keras可以进行shape的自动推断 由于keras是python编写的，因此，我们可以随时查看keras其他层的源码，参考如何编写 优化器，目标函数，初始化策略 objectives是优化目标， 它本质上是一个从张量到数值的函数，当然，是用符号式编程表达的。具体的优化目标有mse，mae，交叉熵等等等等，根据具体任务取用即可，当然，也支持自己编写。需要特别说明的一点是，如果选用categorical_crossentropy作为目标函数，需要将标签转换为one-hot编码的形式，这个动作通过utils.np_utils.to_categorical来完成 optimizers是优化器，模型是可以传入优化器对象的，你可以自己配置一个SGD，然后将它传入模型中，参数clipnorm和clipvalue，用来对梯度进行裁剪。 activation是激活函数，这部分的内容一般不直接使用，而是通过激活层Activation来调用，此处的激活函数是普通的element-wise激活函数 callback是回调函数，这其实是一个比较重要的模块，回调函数不是一个函数而是一个类，用于在训练过程中收集信息或进行某种动作。比如我们经常想画一下每个epoch的训练误差和测试误差，那这些信息就需要在回调函数中收集。预定义的回调函数中CheckModelpoint，History和EarlyStopping都是比较重要和常用的。其中CheckPoint用于保存模型，History记录了训练和测试的信息，EarlyStopping用于在已经收敛时提前结束训练。 PS:History是模型训练函数fit的返回值 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:3","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#backend"},{"categories":["深度学习"],"content":" api 详细介绍 backendbackend这个模块的主要作用，是对tensorflow和theano的底层张量运算进行了包装。用户不用关心具体执行张量运算的是theano还是tensorflow，就可以编写出能在两个框架下可以无缝对接的程序。backend中的函数要比文档里给出的多得多，完全就是一家百货商店。但一般情况下，文档里给出的那些就已经足够你完成大部分工作了，事实上就连文档里给出的函数大部分情况也不会用，这里提几个比较有用的函数： function：毫无疑问这估计是最有用的一个函数了，function用于将一个计算图（计算关系）编译为具体的函数。典型的使用场景是输出网络的中间层结果。 image_ordering和set_image_ordering：这组函数用于返回/设置图片的维度顺序，由于Theano和Tensorflow的图片维度顺序不一样，所以有时候需要获取/指定。典型应用是当希望网络自适应的根据使用的后端调整图片维度顺序时。 learning_phase：这个函数的主要作用是返回网络的运行状态，0代表测试，1代表训练。当你需要便携一个在训练和测试是行为不同的层（如Dropout）时，它会很有用。 int_shape：这是我最常用的一个函数，用于以整数tuple的形式返回张量的shape。要知道从前网络输出张量的shape是看都看不到的，int_shape可以在debug时起到很大作用。 gradients： 求损失函数关于变量的导数，也就是网络的反向计算过程。这个函数在不训练网络而只想用梯度做一点奇怪的事情的时候会很有用，如图像风格转移。 backend的其他大部分函数的函数名是望而知义的，什么max，min，equal，eval，zeros，ones，conv2d等等。函数的命名方式跟numpy差不多，下次想用时不妨先‘.’一下，说不定就有。 models/layers使用Keras最常见的目的，当然还是训练一个网络。之前说了网络就是张量到张量的映射，所以Keras的网络，其实是一个由多个子计算图构成的大计算图。当这些子计算图是顺序连接时，称为Sequential，否则就是一般的model，我们称为函数式模型。 模型有两套训练和测试的函数，一套是fit，evaluate等，另一套是fit_generator，evaluate_generator，前者适用于普通情况，后者适用于数据是以迭代器动态生成的情况。迭代器可以在内存/显存不足，实时动态数据提升进行网络训练，所以使用Keras的话，Python的迭代器这一部分是一定要掌握的内容。对模型而言，最核心的函数有两个： compile()：编译，模型在训练前必须编译，这个函数用于完成添加正则项啊，确定目标函数啊，确定优化器啊等等一系列模型配置功能。这个函数必须指定的参数是优化器和目标函数，经常还需要指定一个metrics来评价模型。 fit()/fit_generator()：用来训练模型，参数较多，是需要重点掌握的函数，对于keras使用者而言，这个函数的每一个参数都需要掌握。 另外，模型还有几个常用的属性和函数： layers：该属性是模型全部层对象的列表，是的就是一个普通的python list get_layer()：这个函数通过名字来返回模型中某个层对象 pop()：这个函数文档里没有，但是可以用。作用是弹出模型的最后一层，从前进行finetune时没有pop，大家一般用model.layers.pop()来完成同样的功能。 因为Model是Layer的子类，Layer的所有属性和方法也自动被Model所有 Keras的层对象是构筑模型的基石，除了卷积层，递归神经网络层，全连接层，激活层这种烂大街的Layer对象外，keras还有自己的一套东西： Advanced Activation：高级激活层，主要收录了包括leakyReLU，pReLU，ELU，SReLU等一系列高级激活函数，这些激活函数不是简单的element-wise计算，所以单独拿出来实现一下 Merge层：这个层用于将多个层对象的输出组合起来，支持级联、乘法、余弦等多种计算方式，它还有个小兄弟叫merge，这个函数完成与Merge相同的作用，但输入的对象是张量而不是层对象。 Lambda层：这是一个神奇的层，看名字就知道它用来把一个函数作用在输入张量上。这个层可以大大减少你的工作量，当你需要定义的新层的计算不是那么复杂的时候，可以通过lambda层来实现，而不用自己完全重写。 Highway/Maxout/AtrousConvolution2D层：这个就不多说了，懂的人自然懂，keras还是在一直跟着潮流走的 Wrapper层：Wrapper层用于将一个普通的层对象进行包装升级，赋予其更多功能。目前，Wrapper层里有一个TimeDistributed层，用于将普通的层包装为对时间序列输入处理的层，而Bidirectional可以将输入的递归神经网络层包装为双向的（如把LSTM做成BLSTM） Input：补一个特殊的层，Input，这个东西实际上是一个Keras tensor的占位符，主要用于在搭建Model模型时作为输入tensor使用，这个Input可以通过keras.layers来import。 stateful与unroll：Keras的递归神经网络层，如SimpleRNN，LSTM等，支持两种特殊的操作。一种是stateful，设置stateful为True意味着训练时每个batch的状态都会被重用于初始化下一个batch的初始状态。另一种是unroll，unroll可以将递归神经网络展开，以空间换取运行时间。 Keras的layers对象还有一些有用的属性和方法: name：别小看这个，从茫茫层海中搜索一个特定的层，如果你对数数没什么信心，最好是name配合get_layer()来用。 trainable：这个参数确定了层是可训练的还是不可训练的，在迁移学习中我们经常需要把某些层冻结起来而finetune别的层，冻结这个动作就是通过设置trainable来实现的。 input/output：这两个属性是层的输入和输出张量，是Keras tensor的对象，这两个属性在你需要获取中间层输入输出时非常有用 get_weights/set_weights：这是两个方法用于手动取出和载入层的参数，set_weights传入的权重必须与get_weights返回的权重具有同样的shape，一般可以用get_weights来看权重shape，用set_weights来载入权重 在Keras中经常有的一个需求是需要自己编写一个新的层，如果你的计算比较简单，那可以尝试通过Lambda层来解决，如果你不得不编写一个自己的层，那也不是什么大不了的事儿。要在Keras中编写一个自己的层，需要开一个从Layer（或其他层）继承的类，除了__init__以为你需要覆盖三个函数： build，这个函数用来确立这个层都有哪些参数，哪些参数是可训练的哪些参数是不可训练的。 call，这个函数在调用层对象时自动使用，里面就是该层的计算逻辑，或计算图了。显然，这个层的核心应该是一段符号式的输入张量到输出张量的计算过程。 get_output_shape_for：如果你的层计算后，输入张量和输出张量的shape不一致，那么你需要把这个函数也重新写一下，返回输出张量的shape，以保证Keras可以进行shape的自动推断 由于keras是python编写的，因此，我们可以随时查看keras其他层的源码，参考如何编写 优化器，目标函数，初始化策略 objectives是优化目标， 它本质上是一个从张量到数值的函数，当然，是用符号式编程表达的。具体的优化目标有mse，mae，交叉熵等等等等，根据具体任务取用即可，当然，也支持自己编写。需要特别说明的一点是，如果选用categorical_crossentropy作为目标函数，需要将标签转换为one-hot编码的形式，这个动作通过utils.np_utils.to_categorical来完成 optimizers是优化器，模型是可以传入优化器对象的，你可以自己配置一个SGD，然后将它传入模型中，参数clipnorm和clipvalue，用来对梯度进行裁剪。 activation是激活函数，这部分的内容一般不直接使用，而是通过激活层Activation来调用，此处的激活函数是普通的element-wise激活函数 callback是回调函数，这其实是一个比较重要的模块，回调函数不是一个函数而是一个类，用于在训练过程中收集信息或进行某种动作。比如我们经常想画一下每个epoch的训练误差和测试误差，那这些信息就需要在回调函数中收集。预定义的回调函数中CheckModelpoint，History和EarlyStopping都是比较重要和常用的。其中CheckPoint用于保存模型，History记录了训练和测试的信息，EarlyStopping用于在已经收敛时提前结束训练。 PS:History是模型训练函数fit的返回值 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:3","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#modelslayers"},{"categories":["深度学习"],"content":" api 详细介绍 backendbackend这个模块的主要作用，是对tensorflow和theano的底层张量运算进行了包装。用户不用关心具体执行张量运算的是theano还是tensorflow，就可以编写出能在两个框架下可以无缝对接的程序。backend中的函数要比文档里给出的多得多，完全就是一家百货商店。但一般情况下，文档里给出的那些就已经足够你完成大部分工作了，事实上就连文档里给出的函数大部分情况也不会用，这里提几个比较有用的函数： function：毫无疑问这估计是最有用的一个函数了，function用于将一个计算图（计算关系）编译为具体的函数。典型的使用场景是输出网络的中间层结果。 image_ordering和set_image_ordering：这组函数用于返回/设置图片的维度顺序，由于Theano和Tensorflow的图片维度顺序不一样，所以有时候需要获取/指定。典型应用是当希望网络自适应的根据使用的后端调整图片维度顺序时。 learning_phase：这个函数的主要作用是返回网络的运行状态，0代表测试，1代表训练。当你需要便携一个在训练和测试是行为不同的层（如Dropout）时，它会很有用。 int_shape：这是我最常用的一个函数，用于以整数tuple的形式返回张量的shape。要知道从前网络输出张量的shape是看都看不到的，int_shape可以在debug时起到很大作用。 gradients： 求损失函数关于变量的导数，也就是网络的反向计算过程。这个函数在不训练网络而只想用梯度做一点奇怪的事情的时候会很有用，如图像风格转移。 backend的其他大部分函数的函数名是望而知义的，什么max，min，equal，eval，zeros，ones，conv2d等等。函数的命名方式跟numpy差不多，下次想用时不妨先‘.’一下，说不定就有。 models/layers使用Keras最常见的目的，当然还是训练一个网络。之前说了网络就是张量到张量的映射，所以Keras的网络，其实是一个由多个子计算图构成的大计算图。当这些子计算图是顺序连接时，称为Sequential，否则就是一般的model，我们称为函数式模型。 模型有两套训练和测试的函数，一套是fit，evaluate等，另一套是fit_generator，evaluate_generator，前者适用于普通情况，后者适用于数据是以迭代器动态生成的情况。迭代器可以在内存/显存不足，实时动态数据提升进行网络训练，所以使用Keras的话，Python的迭代器这一部分是一定要掌握的内容。对模型而言，最核心的函数有两个： compile()：编译，模型在训练前必须编译，这个函数用于完成添加正则项啊，确定目标函数啊，确定优化器啊等等一系列模型配置功能。这个函数必须指定的参数是优化器和目标函数，经常还需要指定一个metrics来评价模型。 fit()/fit_generator()：用来训练模型，参数较多，是需要重点掌握的函数，对于keras使用者而言，这个函数的每一个参数都需要掌握。 另外，模型还有几个常用的属性和函数： layers：该属性是模型全部层对象的列表，是的就是一个普通的python list get_layer()：这个函数通过名字来返回模型中某个层对象 pop()：这个函数文档里没有，但是可以用。作用是弹出模型的最后一层，从前进行finetune时没有pop，大家一般用model.layers.pop()来完成同样的功能。 因为Model是Layer的子类，Layer的所有属性和方法也自动被Model所有 Keras的层对象是构筑模型的基石，除了卷积层，递归神经网络层，全连接层，激活层这种烂大街的Layer对象外，keras还有自己的一套东西： Advanced Activation：高级激活层，主要收录了包括leakyReLU，pReLU，ELU，SReLU等一系列高级激活函数，这些激活函数不是简单的element-wise计算，所以单独拿出来实现一下 Merge层：这个层用于将多个层对象的输出组合起来，支持级联、乘法、余弦等多种计算方式，它还有个小兄弟叫merge，这个函数完成与Merge相同的作用，但输入的对象是张量而不是层对象。 Lambda层：这是一个神奇的层，看名字就知道它用来把一个函数作用在输入张量上。这个层可以大大减少你的工作量，当你需要定义的新层的计算不是那么复杂的时候，可以通过lambda层来实现，而不用自己完全重写。 Highway/Maxout/AtrousConvolution2D层：这个就不多说了，懂的人自然懂，keras还是在一直跟着潮流走的 Wrapper层：Wrapper层用于将一个普通的层对象进行包装升级，赋予其更多功能。目前，Wrapper层里有一个TimeDistributed层，用于将普通的层包装为对时间序列输入处理的层，而Bidirectional可以将输入的递归神经网络层包装为双向的（如把LSTM做成BLSTM） Input：补一个特殊的层，Input，这个东西实际上是一个Keras tensor的占位符，主要用于在搭建Model模型时作为输入tensor使用，这个Input可以通过keras.layers来import。 stateful与unroll：Keras的递归神经网络层，如SimpleRNN，LSTM等，支持两种特殊的操作。一种是stateful，设置stateful为True意味着训练时每个batch的状态都会被重用于初始化下一个batch的初始状态。另一种是unroll，unroll可以将递归神经网络展开，以空间换取运行时间。 Keras的layers对象还有一些有用的属性和方法: name：别小看这个，从茫茫层海中搜索一个特定的层，如果你对数数没什么信心，最好是name配合get_layer()来用。 trainable：这个参数确定了层是可训练的还是不可训练的，在迁移学习中我们经常需要把某些层冻结起来而finetune别的层，冻结这个动作就是通过设置trainable来实现的。 input/output：这两个属性是层的输入和输出张量，是Keras tensor的对象，这两个属性在你需要获取中间层输入输出时非常有用 get_weights/set_weights：这是两个方法用于手动取出和载入层的参数，set_weights传入的权重必须与get_weights返回的权重具有同样的shape，一般可以用get_weights来看权重shape，用set_weights来载入权重 在Keras中经常有的一个需求是需要自己编写一个新的层，如果你的计算比较简单，那可以尝试通过Lambda层来解决，如果你不得不编写一个自己的层，那也不是什么大不了的事儿。要在Keras中编写一个自己的层，需要开一个从Layer（或其他层）继承的类，除了__init__以为你需要覆盖三个函数： build，这个函数用来确立这个层都有哪些参数，哪些参数是可训练的哪些参数是不可训练的。 call，这个函数在调用层对象时自动使用，里面就是该层的计算逻辑，或计算图了。显然，这个层的核心应该是一段符号式的输入张量到输出张量的计算过程。 get_output_shape_for：如果你的层计算后，输入张量和输出张量的shape不一致，那么你需要把这个函数也重新写一下，返回输出张量的shape，以保证Keras可以进行shape的自动推断 由于keras是python编写的，因此，我们可以随时查看keras其他层的源码，参考如何编写 优化器，目标函数，初始化策略 objectives是优化目标， 它本质上是一个从张量到数值的函数，当然，是用符号式编程表达的。具体的优化目标有mse，mae，交叉熵等等等等，根据具体任务取用即可，当然，也支持自己编写。需要特别说明的一点是，如果选用categorical_crossentropy作为目标函数，需要将标签转换为one-hot编码的形式，这个动作通过utils.np_utils.to_categorical来完成 optimizers是优化器，模型是可以传入优化器对象的，你可以自己配置一个SGD，然后将它传入模型中，参数clipnorm和clipvalue，用来对梯度进行裁剪。 activation是激活函数，这部分的内容一般不直接使用，而是通过激活层Activation来调用，此处的激活函数是普通的element-wise激活函数 callback是回调函数，这其实是一个比较重要的模块，回调函数不是一个函数而是一个类，用于在训练过程中收集信息或进行某种动作。比如我们经常想画一下每个epoch的训练误差和测试误差，那这些信息就需要在回调函数中收集。预定义的回调函数中CheckModelpoint，History和EarlyStopping都是比较重要和常用的。其中CheckPoint用于保存模型，History记录了训练和测试的信息，EarlyStopping用于在已经收敛时提前结束训练。 PS:History是模型训练函数fit的返回值 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:3","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#优化器目标函数初始化策略"},{"categories":["深度学习"],"content":" keras的一些特性 全部Layer都要callable Keras的一大性质是所有的layer对象都是callable的。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。 # 假设我们想要计算x的sigmoid值是多少，我们不去构建model # 而是构建几个单独的层就可以了 import keras.backend as K from keras.layers import Activation import numpy as np x = K.placeholder(shape=(3,)) y = Activation('sigmoid')(x) f = K.function([x],[y]) out = f([np.array([1,2,3])]) 把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。 代码中第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置 “的符号，翻译成中文就是”此处应有一个长为3的向量“。 注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面 又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation(‘sigmoid’)’是被当做一个函数来使用的。层是张量到张量的运算，那么其输出y自然也是一个张量。 第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。 模型也是张量到张量的映射，所以Layer是Model的父类，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。 Shape与Shape自动推断 使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。 Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现。在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。 然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。 一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个2242243的彩色图片，在内部运行时数据的shape是(None，224，224，3)。 TH与TF的相爱相杀现在由于theano已经停止更新，所以keras的默认后端是tensorflow。 dim_ordering，也就是维度顺序:tf的维度顺序是(224，224，3)，只需要记住这个顺序就行。 keras读取模型某一层的输出 # model.layer返回的是一个list，其中每一个元素是model中的层 output = model.layer[3].output # 查看在layer中的index和名字 layers = model.layers for i , layer in enumerate(layers): print(i, layer) keras不易发现的坑 当我们做分类任务时： output layer的activation=“sigmoid”，对应的loss=“binary_crossentropy” output layer的activation=“softmax”，对应的loss=“categorical_crossentropy” 对于2来说，得到的是join distribution and a multinomial likelihood，相当于一个概率分布，和为1； 对于1来说，得到的是marginal distribution and a Bernoulli likelihood, p(y0/x) , p(y1/x) etc。 如果是multi-label classification，就是一个样本属于多类的情况下，需要用1。否则，如果各个类别之间有相互关系（比如简单的情感分类，如果是正向情感就一定意味着负向情感的概率低），可以使用softmax；如果各个类别之间偏向于独立，可以使用sigmoid。 对于任务1和2，metric=[‘acc’]的’acc’并不是完全同一个评价方法。我们可以直接令metrics=[‘binary_accuracy’,‘categorical_accuracy’]，在训练过程中会两个结果都输出，这样方便自己的判断。 keras中的masking层到底是干什么的 直接看下面的lstm的例子 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#keras的一些特性"},{"categories":["深度学习"],"content":" keras的一些特性 全部Layer都要callable Keras的一大性质是所有的layer对象都是callable的。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。 # 假设我们想要计算x的sigmoid值是多少，我们不去构建model # 而是构建几个单独的层就可以了 import keras.backend as K from keras.layers import Activation import numpy as np x = K.placeholder(shape=(3,)) y = Activation('sigmoid')(x) f = K.function([x],[y]) out = f([np.array([1,2,3])]) 把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。 代码中第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置 “的符号，翻译成中文就是”此处应有一个长为3的向量“。 注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面 又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation(‘sigmoid’)’是被当做一个函数来使用的。层是张量到张量的运算，那么其输出y自然也是一个张量。 第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。 模型也是张量到张量的映射，所以Layer是Model的父类，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。 Shape与Shape自动推断 使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。 Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现。在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。 然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。 一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个2242243的彩色图片，在内部运行时数据的shape是(None，224，224，3)。 TH与TF的相爱相杀现在由于theano已经停止更新，所以keras的默认后端是tensorflow。 dim_ordering，也就是维度顺序:tf的维度顺序是(224，224，3)，只需要记住这个顺序就行。 keras读取模型某一层的输出 # model.layer返回的是一个list，其中每一个元素是model中的层 output = model.layer[3].output # 查看在layer中的index和名字 layers = model.layers for i , layer in enumerate(layers): print(i, layer) keras不易发现的坑 当我们做分类任务时： output layer的activation=“sigmoid”，对应的loss=“binary_crossentropy” output layer的activation=“softmax”，对应的loss=“categorical_crossentropy” 对于2来说，得到的是join distribution and a multinomial likelihood，相当于一个概率分布，和为1； 对于1来说，得到的是marginal distribution and a Bernoulli likelihood, p(y0/x) , p(y1/x) etc。 如果是multi-label classification，就是一个样本属于多类的情况下，需要用1。否则，如果各个类别之间有相互关系（比如简单的情感分类，如果是正向情感就一定意味着负向情感的概率低），可以使用softmax；如果各个类别之间偏向于独立，可以使用sigmoid。 对于任务1和2，metric=[‘acc’]的’acc’并不是完全同一个评价方法。我们可以直接令metrics=[‘binary_accuracy’,‘categorical_accuracy’]，在训练过程中会两个结果都输出，这样方便自己的判断。 keras中的masking层到底是干什么的 直接看下面的lstm的例子 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#全部layer都要callable"},{"categories":["深度学习"],"content":" keras的一些特性 全部Layer都要callable Keras的一大性质是所有的layer对象都是callable的。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。 # 假设我们想要计算x的sigmoid值是多少，我们不去构建model # 而是构建几个单独的层就可以了 import keras.backend as K from keras.layers import Activation import numpy as np x = K.placeholder(shape=(3,)) y = Activation('sigmoid')(x) f = K.function([x],[y]) out = f([np.array([1,2,3])]) 把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。 代码中第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置 “的符号，翻译成中文就是”此处应有一个长为3的向量“。 注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面 又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation(‘sigmoid’)’是被当做一个函数来使用的。层是张量到张量的运算，那么其输出y自然也是一个张量。 第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。 模型也是张量到张量的映射，所以Layer是Model的父类，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。 Shape与Shape自动推断 使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。 Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现。在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。 然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。 一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个2242243的彩色图片，在内部运行时数据的shape是(None，224，224，3)。 TH与TF的相爱相杀现在由于theano已经停止更新，所以keras的默认后端是tensorflow。 dim_ordering，也就是维度顺序:tf的维度顺序是(224，224，3)，只需要记住这个顺序就行。 keras读取模型某一层的输出 # model.layer返回的是一个list，其中每一个元素是model中的层 output = model.layer[3].output # 查看在layer中的index和名字 layers = model.layers for i , layer in enumerate(layers): print(i, layer) keras不易发现的坑 当我们做分类任务时： output layer的activation=“sigmoid”，对应的loss=“binary_crossentropy” output layer的activation=“softmax”，对应的loss=“categorical_crossentropy” 对于2来说，得到的是join distribution and a multinomial likelihood，相当于一个概率分布，和为1； 对于1来说，得到的是marginal distribution and a Bernoulli likelihood, p(y0/x) , p(y1/x) etc。 如果是multi-label classification，就是一个样本属于多类的情况下，需要用1。否则，如果各个类别之间有相互关系（比如简单的情感分类，如果是正向情感就一定意味着负向情感的概率低），可以使用softmax；如果各个类别之间偏向于独立，可以使用sigmoid。 对于任务1和2，metric=[‘acc’]的’acc’并不是完全同一个评价方法。我们可以直接令metrics=[‘binary_accuracy’,‘categorical_accuracy’]，在训练过程中会两个结果都输出，这样方便自己的判断。 keras中的masking层到底是干什么的 直接看下面的lstm的例子 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#shape与shape自动推断"},{"categories":["深度学习"],"content":" keras的一些特性 全部Layer都要callable Keras的一大性质是所有的layer对象都是callable的。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。 # 假设我们想要计算x的sigmoid值是多少，我们不去构建model # 而是构建几个单独的层就可以了 import keras.backend as K from keras.layers import Activation import numpy as np x = K.placeholder(shape=(3,)) y = Activation('sigmoid')(x) f = K.function([x],[y]) out = f([np.array([1,2,3])]) 把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。 代码中第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置 “的符号，翻译成中文就是”此处应有一个长为3的向量“。 注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面 又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation(‘sigmoid’)’是被当做一个函数来使用的。层是张量到张量的运算，那么其输出y自然也是一个张量。 第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。 模型也是张量到张量的映射，所以Layer是Model的父类，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。 Shape与Shape自动推断 使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。 Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现。在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。 然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。 一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个2242243的彩色图片，在内部运行时数据的shape是(None，224，224，3)。 TH与TF的相爱相杀现在由于theano已经停止更新，所以keras的默认后端是tensorflow。 dim_ordering，也就是维度顺序:tf的维度顺序是(224，224，3)，只需要记住这个顺序就行。 keras读取模型某一层的输出 # model.layer返回的是一个list，其中每一个元素是model中的层 output = model.layer[3].output # 查看在layer中的index和名字 layers = model.layers for i , layer in enumerate(layers): print(i, layer) keras不易发现的坑 当我们做分类任务时： output layer的activation=“sigmoid”，对应的loss=“binary_crossentropy” output layer的activation=“softmax”，对应的loss=“categorical_crossentropy” 对于2来说，得到的是join distribution and a multinomial likelihood，相当于一个概率分布，和为1； 对于1来说，得到的是marginal distribution and a Bernoulli likelihood, p(y0/x) , p(y1/x) etc。 如果是multi-label classification，就是一个样本属于多类的情况下，需要用1。否则，如果各个类别之间有相互关系（比如简单的情感分类，如果是正向情感就一定意味着负向情感的概率低），可以使用softmax；如果各个类别之间偏向于独立，可以使用sigmoid。 对于任务1和2，metric=[‘acc’]的’acc’并不是完全同一个评价方法。我们可以直接令metrics=[‘binary_accuracy’,‘categorical_accuracy’]，在训练过程中会两个结果都输出，这样方便自己的判断。 keras中的masking层到底是干什么的 直接看下面的lstm的例子 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#th与tf的相爱相杀"},{"categories":["深度学习"],"content":" keras的一些特性 全部Layer都要callable Keras的一大性质是所有的layer对象都是callable的。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。 # 假设我们想要计算x的sigmoid值是多少，我们不去构建model # 而是构建几个单独的层就可以了 import keras.backend as K from keras.layers import Activation import numpy as np x = K.placeholder(shape=(3,)) y = Activation('sigmoid')(x) f = K.function([x],[y]) out = f([np.array([1,2,3])]) 把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。 代码中第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置 “的符号，翻译成中文就是”此处应有一个长为3的向量“。 注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面 又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation(‘sigmoid’)’是被当做一个函数来使用的。层是张量到张量的运算，那么其输出y自然也是一个张量。 第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。 模型也是张量到张量的映射，所以Layer是Model的父类，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。 Shape与Shape自动推断 使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。 Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现。在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。 然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。 一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个2242243的彩色图片，在内部运行时数据的shape是(None，224，224，3)。 TH与TF的相爱相杀现在由于theano已经停止更新，所以keras的默认后端是tensorflow。 dim_ordering，也就是维度顺序:tf的维度顺序是(224，224，3)，只需要记住这个顺序就行。 keras读取模型某一层的输出 # model.layer返回的是一个list，其中每一个元素是model中的层 output = model.layer[3].output # 查看在layer中的index和名字 layers = model.layers for i , layer in enumerate(layers): print(i, layer) keras不易发现的坑 当我们做分类任务时： output layer的activation=“sigmoid”，对应的loss=“binary_crossentropy” output layer的activation=“softmax”，对应的loss=“categorical_crossentropy” 对于2来说，得到的是join distribution and a multinomial likelihood，相当于一个概率分布，和为1； 对于1来说，得到的是marginal distribution and a Bernoulli likelihood, p(y0/x) , p(y1/x) etc。 如果是multi-label classification，就是一个样本属于多类的情况下，需要用1。否则，如果各个类别之间有相互关系（比如简单的情感分类，如果是正向情感就一定意味着负向情感的概率低），可以使用softmax；如果各个类别之间偏向于独立，可以使用sigmoid。 对于任务1和2，metric=[‘acc’]的’acc’并不是完全同一个评价方法。我们可以直接令metrics=[‘binary_accuracy’,‘categorical_accuracy’]，在训练过程中会两个结果都输出，这样方便自己的判断。 keras中的masking层到底是干什么的 直接看下面的lstm的例子 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#keras读取模型某一层的输出"},{"categories":["深度学习"],"content":" keras的一些特性 全部Layer都要callable Keras的一大性质是所有的layer对象都是callable的。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。 # 假设我们想要计算x的sigmoid值是多少，我们不去构建model # 而是构建几个单独的层就可以了 import keras.backend as K from keras.layers import Activation import numpy as np x = K.placeholder(shape=(3,)) y = Activation('sigmoid')(x) f = K.function([x],[y]) out = f([np.array([1,2,3])]) 把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。 代码中第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置 “的符号，翻译成中文就是”此处应有一个长为3的向量“。 注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面 又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation(‘sigmoid’)’是被当做一个函数来使用的。层是张量到张量的运算，那么其输出y自然也是一个张量。 第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。 模型也是张量到张量的映射，所以Layer是Model的父类，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。 Shape与Shape自动推断 使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。 Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现。在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。 然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。 一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个2242243的彩色图片，在内部运行时数据的shape是(None，224，224，3)。 TH与TF的相爱相杀现在由于theano已经停止更新，所以keras的默认后端是tensorflow。 dim_ordering，也就是维度顺序:tf的维度顺序是(224，224，3)，只需要记住这个顺序就行。 keras读取模型某一层的输出 # model.layer返回的是一个list，其中每一个元素是model中的层 output = model.layer[3].output # 查看在layer中的index和名字 layers = model.layers for i , layer in enumerate(layers): print(i, layer) keras不易发现的坑 当我们做分类任务时： output layer的activation=“sigmoid”，对应的loss=“binary_crossentropy” output layer的activation=“softmax”，对应的loss=“categorical_crossentropy” 对于2来说，得到的是join distribution and a multinomial likelihood，相当于一个概率分布，和为1； 对于1来说，得到的是marginal distribution and a Bernoulli likelihood, p(y0/x) , p(y1/x) etc。 如果是multi-label classification，就是一个样本属于多类的情况下，需要用1。否则，如果各个类别之间有相互关系（比如简单的情感分类，如果是正向情感就一定意味着负向情感的概率低），可以使用softmax；如果各个类别之间偏向于独立，可以使用sigmoid。 对于任务1和2，metric=[‘acc’]的’acc’并不是完全同一个评价方法。我们可以直接令metrics=[‘binary_accuracy’,‘categorical_accuracy’]，在训练过程中会两个结果都输出，这样方便自己的判断。 keras中的masking层到底是干什么的 直接看下面的lstm的例子 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#keras不易发现的坑"},{"categories":["深度学习"],"content":" keras的一些特性 全部Layer都要callable Keras的一大性质是所有的layer对象都是callable的。所谓callable，就是能当作函数一样来使用，层的这个性质不需要依赖任何模型就能成立。 # 假设我们想要计算x的sigmoid值是多少，我们不去构建model # 而是构建几个单独的层就可以了 import keras.backend as K from keras.layers import Activation import numpy as np x = K.placeholder(shape=(3,)) y = Activation('sigmoid')(x) f = K.function([x],[y]) out = f([np.array([1,2,3])]) 把层和模型当作张量的函数来使用，是需要认真贯彻落实的一个东西。 代码中第1行先定义了一个“占位符”，它的shape是一个长为3的向量。所谓占位符就是“先占个位置 “的符号，翻译成中文就是”此处应有一个长为3的向量“。 注意第2行，这行我们使用了一个激活层，激活层的激活函数形式是sigmoid，在激活层的后面 又有一个括号，括号内是我们的输入张量x，可以看到，层对象‘Activation(‘sigmoid’)’是被当做一个函数来使用的。层是张量到张量的运算，那么其输出y自然也是一个张量。 第3行通过调用function函数对计算图进行编译，这个计算图很简单，就是输入张量经过sigmoid作用变成输出向量，计算图的各种优化通过这一步得以完成，现在，f就是一个真正的函数了，就可以按照一般的方法使用了。 模型也是张量到张量的映射，所以Layer是Model的父类，因此，一个模型本身也可以像上面一样使用。总而言之，在Keras中，层对象是callable的。 Shape与Shape自动推断 使用过Keras的都知道，Keras的所有的层有一个“input_shape”的参数，用来指定输入张量的shape。然而这个input_shape，或者有时候是input_dim，只需要在模型的首层加以指定。一旦模型的首层的input_shape指定了，后面的各层就不用再指定，而会根据计算图自动推断。这个功能称为shape的自动推断。 Keras的自动推断依赖于Layer中的get_output_shape_for函数来实现。在所有的Keras中都有这样一个函数，因此后面的层可以通过查看这个函数的返回值获取前层的输入shape，并通过自己的get_output_shape_for将这个信息传递下去。 然而，有时候，这个自动推断会出错。这种情况发生在一个RNN层后面接Flatten然后又接Dense的时候，这个时候Dense的output_shape无法自动推断出。这时需要指定RNN的输入序列长度input_length，或者在网络的第一层通过input_shape就指定。这种情况极少见，大致有个印象即可，遇到的话知道大概是哪里出了问题就好。 一般而言，神经网络的数据是以batch为单位的，但在指明input_shape时不需要说明一个batch的样本数。假如你的输入是一个2242243的彩色图片，在内部运行时数据的shape是(None，224，224，3)。 TH与TF的相爱相杀现在由于theano已经停止更新，所以keras的默认后端是tensorflow。 dim_ordering，也就是维度顺序:tf的维度顺序是(224，224，3)，只需要记住这个顺序就行。 keras读取模型某一层的输出 # model.layer返回的是一个list，其中每一个元素是model中的层 output = model.layer[3].output # 查看在layer中的index和名字 layers = model.layers for i , layer in enumerate(layers): print(i, layer) keras不易发现的坑 当我们做分类任务时： output layer的activation=“sigmoid”，对应的loss=“binary_crossentropy” output layer的activation=“softmax”，对应的loss=“categorical_crossentropy” 对于2来说，得到的是join distribution and a multinomial likelihood，相当于一个概率分布，和为1； 对于1来说，得到的是marginal distribution and a Bernoulli likelihood, p(y0/x) , p(y1/x) etc。 如果是multi-label classification，就是一个样本属于多类的情况下，需要用1。否则，如果各个类别之间有相互关系（比如简单的情感分类，如果是正向情感就一定意味着负向情感的概率低），可以使用softmax；如果各个类别之间偏向于独立，可以使用sigmoid。 对于任务1和2，metric=[‘acc’]的’acc’并不是完全同一个评价方法。我们可以直接令metrics=[‘binary_accuracy’,‘categorical_accuracy’]，在训练过程中会两个结果都输出，这样方便自己的判断。 keras中的masking层到底是干什么的 直接看下面的lstm的例子 ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#keras中的masking层到底是干什么的"},{"categories":["深度学习"],"content":" 参考链接 Keras使用过程中的tricks和errors(持续更新) 一个不负责任的Keras介绍（上） 一个不负责任的Keras介绍（中） 一个不负责任的Keras介绍（下） ","date":"2018-12-27","objectID":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras详细介绍","uri":"/keras%E8%AF%A6%E7%BB%86%E4%BB%8B%E7%BB%8D/#参考链接"},{"categories":["算法","数据结构"],"content":" 定义 算法: 算法中的指令描述的是一个计算，当其运行时能从一个初始状态和（可能为空的）初始输入开始，经过一系列有限而清晰定义的状态，最终产生输出并停止于一个终态。一个状态到另一个状态的转移不一定是确定的。随机化算法在内的一些算法，包含了一些随机输入. 数据结构: 数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率. 先说数据结构,毕竟算法也得有东西算,才又算法,所谓先有鸡后有蛋 数据结构:既然是结构，里面肯定是有不同的数据的，与其叫数据结构，不如叫数据流,数据流中包含了各种数据，数据结构构成流，流亦是数据结构，面向对象就是，万物皆数据，以流的形式呈现. 算法就是解决问题的方法,以函数或者是类的形式出现，它决定传入的参数，对数据流进行修改，留下需要的，抛弃不需要的，将数据和其他数据组成在一起，形成数据流，将数据流和数据流组成在一起，形成新的数据流，将数据流拆分成数据，再聚合，这就是算法的作用。 算法就是数据流的灵魂,没有算法的数据其实没有灵魂的 数据结构 数据结构的分类： 线性结构：静态数组，动态数组，链表，队列，栈，字符串 树形结构：二叉树，B/B+/B-树，红黑树，哈夫曼树 图形结构：图 其他：跳跃表，map，set 基于数据结构的基本算法： 数组的算法：插入，删除，新建，查找 链表的算法：插入，删除，合并，查找 栈的算法：新建，进栈，出栈 队列的算法：新建，入队，出队，优先队列 字符串的算法：查找，字串匹配 二叉树的算法：前中序遍历，查找，深度，平衡二叉树的插入删除等等 图的算法：DFS/BFS，图的结构，Prim算法，Dijkstra算法，kruskal算法，拓扑排序 排序算法：各种排序算法 以上是的这些算法都是比较基本的，在学习其数据结构时，就依附存在的算法 算法 算法是一种解题思想，解决问题的方法 算法的分类 算法的复杂度分析 递归与分治算法 动态规划 贪心算法 回溯算法 分支限界算法 概率算法 具体概念讲解 算法的复杂度 时间复杂度和空间复杂度的分析：应该会对自己写的代码进行复杂度分析，比如空间和时间复杂度是多少。 PS：排序的时间复杂度：最优的情况是O(nlogn)，最坏的情况是O(n^2),有一个特殊情况是O(n)–位排序和计数排序 递归和分治算法 **递归：直接或间接调用自身的算法成为递归算法。**PS：树结构非常适合递归算法 递归的一个例子：斐波那契数列（入门必学的一个错误的递归算法），可以用递归来做，但是实际上效率比较低。 分治算法：将一个大问题分解为k个规模比较小的子问题，并且这些子问题互相独立，但是又与原问题相同。 PS：分治思想是一个基本的思想，递归和其后的dp(动态规划)以及贪心算法很多时候都是基于分治来做。 从分治出发设计出来的算法，一般是递归算法。 常见的分治算法：排序算法中的归并排序，快速排序，二分搜索 动态规划算法 动态规划算法：与分治算法思想类似，两者不同点在于，dp的子问题并不是互相独立的，在分治算法中，有些子问题会被重复计算，效率很低，因此，动态规划，对分治有了优化，记录子问题，去掉重复性。 PS：动态规划是对分治算法的优化 动态规划适用于最优化算法 使用动态规划特征： 求一个问题的最优解 大问题可以分解为子问题，子问题还有重叠的更小的子问题 整体问题最优解取决于子问题的最优解（状态转移方程） 从上往下分析问题，从下往上解决问题 讨论底层的边界问题 注意上面的三点：子问题重叠，状态转移方程，边界问题 PS：尝试使用动态规划去解斐波那契数列 常见的动态规划题目：最长公共子串，最长公共子序列，最长回文串，矩阵乘法，背包问题 贪心算法 贪心算法：类似于动态规划，两者不同点在于，贪心算法考虑局部最优，动态规划考虑全局最优，如果一个问题，全局最优是通过局部最优逼近的，那么此时可以考虑贪心算法 PS：贪心算法不能对所有的问题都能得到整体最优解，但对某些问题可以。 贪心算法是对某些动态规划问题的一个简化 常见的贪心算法：图的单源最短路径，最小生成树的问题。 PS：如何区别贪心算法和动态规划呢？ 回溯算法 回溯算法：回溯算法具有\"通用解题法\"之称， 回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。 PS：它是用于求解组合数较大的问题。 回溯法的解题步骤： 针对所给问题，确定问题的解空间： 首先应明确定义问题的解空间，问题的解空间应至少包含问题的一个（最优）解。 确定结点的扩展搜索规则。 以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。 常见的回溯算法：皇后问题，输出不重复数字的全排列，求一个集合的所有子集 分支限界算法 分支限界算法：分支限界算法类似于回溯算法，两者不同点在于，回溯算法是找出解空间中的所有解，分支限界算法是找出某一个解(这个解或者是最优的)，其次，回溯算法是用深度优先方式搜索解空间，分支限界算法以广度优先的方式搜索解空间。 PS：某种意义上来说，分支限界算法也是寻找最优解。 常见的分支限界算法：TSP（旅行商问题），单源最短路径 概率算法 概率算法分类： 数值概率算法：用于求解没有精确解的数值问题 蒙特卡罗算法：求解问题的准确解 拉斯维加斯算法：不会得到不正确的解，眼下之意，要么不存在，要么正确 舍伍德算法：总能求问题的某一个解，且必然正确 常见的概率算法： 数值概率：设计伪随机数 舍伍德算法：跳跃表 拉斯维加斯算法：皇后问题 蒙特卡罗算法：素数测试 其他的未提到的算法 字符串的算法： KMP算法 Manacher算法 最长回文串，公共子串，公共子序列 全排列 字符串替换 正则表达式 数组 丑数 水仙花 剩余定理 乘积数组 矩阵中路径 逆序对 连续数组最大和 链表 链表的合并 重复节点删除 反转链表 链表环的入口 查找 二叉搜索 约瑟夫环 二进制1的个数 二维数组的查找 排序 大约9个排序算法，分析时间空间复杂度，分析稳定性 二叉树 对称二叉树 层次遍历 序列话二叉树 非递归的前中后序遍历 二叉搜索树的第k个节点 map,set,array在java、C++的用法 剩余定理 快速幂取模 欧几里德算法 ","date":"2018-12-27","objectID":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:0","series":null,"tags":["算法","数据结构"],"title":"算法与数据结构","uri":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/#"},{"categories":["算法","数据结构"],"content":" 定义 算法: 算法中的指令描述的是一个计算，当其运行时能从一个初始状态和（可能为空的）初始输入开始，经过一系列有限而清晰定义的状态，最终产生输出并停止于一个终态。一个状态到另一个状态的转移不一定是确定的。随机化算法在内的一些算法，包含了一些随机输入. 数据结构: 数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率. 先说数据结构,毕竟算法也得有东西算,才又算法,所谓先有鸡后有蛋 数据结构:既然是结构，里面肯定是有不同的数据的，与其叫数据结构，不如叫数据流,数据流中包含了各种数据，数据结构构成流，流亦是数据结构，面向对象就是，万物皆数据，以流的形式呈现. 算法就是解决问题的方法,以函数或者是类的形式出现，它决定传入的参数，对数据流进行修改，留下需要的，抛弃不需要的，将数据和其他数据组成在一起，形成数据流，将数据流和数据流组成在一起，形成新的数据流，将数据流拆分成数据，再聚合，这就是算法的作用。 算法就是数据流的灵魂,没有算法的数据其实没有灵魂的 数据结构 数据结构的分类： 线性结构：静态数组，动态数组，链表，队列，栈，字符串 树形结构：二叉树，B/B+/B-树，红黑树，哈夫曼树 图形结构：图 其他：跳跃表，map，set 基于数据结构的基本算法： 数组的算法：插入，删除，新建，查找 链表的算法：插入，删除，合并，查找 栈的算法：新建，进栈，出栈 队列的算法：新建，入队，出队，优先队列 字符串的算法：查找，字串匹配 二叉树的算法：前中序遍历，查找，深度，平衡二叉树的插入删除等等 图的算法：DFS/BFS，图的结构，Prim算法，Dijkstra算法，kruskal算法，拓扑排序 排序算法：各种排序算法 以上是的这些算法都是比较基本的，在学习其数据结构时，就依附存在的算法 算法 算法是一种解题思想，解决问题的方法 算法的分类 算法的复杂度分析 递归与分治算法 动态规划 贪心算法 回溯算法 分支限界算法 概率算法 具体概念讲解 算法的复杂度 时间复杂度和空间复杂度的分析：应该会对自己写的代码进行复杂度分析，比如空间和时间复杂度是多少。 PS：排序的时间复杂度：最优的情况是O(nlogn)，最坏的情况是O(n^2),有一个特殊情况是O(n)–位排序和计数排序 递归和分治算法 **递归：直接或间接调用自身的算法成为递归算法。**PS：树结构非常适合递归算法 递归的一个例子：斐波那契数列（入门必学的一个错误的递归算法），可以用递归来做，但是实际上效率比较低。 分治算法：将一个大问题分解为k个规模比较小的子问题，并且这些子问题互相独立，但是又与原问题相同。 PS：分治思想是一个基本的思想，递归和其后的dp(动态规划)以及贪心算法很多时候都是基于分治来做。 从分治出发设计出来的算法，一般是递归算法。 常见的分治算法：排序算法中的归并排序，快速排序，二分搜索 动态规划算法 动态规划算法：与分治算法思想类似，两者不同点在于，dp的子问题并不是互相独立的，在分治算法中，有些子问题会被重复计算，效率很低，因此，动态规划，对分治有了优化，记录子问题，去掉重复性。 PS：动态规划是对分治算法的优化 动态规划适用于最优化算法 使用动态规划特征： 求一个问题的最优解 大问题可以分解为子问题，子问题还有重叠的更小的子问题 整体问题最优解取决于子问题的最优解（状态转移方程） 从上往下分析问题，从下往上解决问题 讨论底层的边界问题 注意上面的三点：子问题重叠，状态转移方程，边界问题 PS：尝试使用动态规划去解斐波那契数列 常见的动态规划题目：最长公共子串，最长公共子序列，最长回文串，矩阵乘法，背包问题 贪心算法 贪心算法：类似于动态规划，两者不同点在于，贪心算法考虑局部最优，动态规划考虑全局最优，如果一个问题，全局最优是通过局部最优逼近的，那么此时可以考虑贪心算法 PS：贪心算法不能对所有的问题都能得到整体最优解，但对某些问题可以。 贪心算法是对某些动态规划问题的一个简化 常见的贪心算法：图的单源最短路径，最小生成树的问题。 PS：如何区别贪心算法和动态规划呢？ 回溯算法 回溯算法：回溯算法具有\"通用解题法\"之称， 回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。 PS：它是用于求解组合数较大的问题。 回溯法的解题步骤： 针对所给问题，确定问题的解空间： 首先应明确定义问题的解空间，问题的解空间应至少包含问题的一个（最优）解。 确定结点的扩展搜索规则。 以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。 常见的回溯算法：皇后问题，输出不重复数字的全排列，求一个集合的所有子集 分支限界算法 分支限界算法：分支限界算法类似于回溯算法，两者不同点在于，回溯算法是找出解空间中的所有解，分支限界算法是找出某一个解(这个解或者是最优的)，其次，回溯算法是用深度优先方式搜索解空间，分支限界算法以广度优先的方式搜索解空间。 PS：某种意义上来说，分支限界算法也是寻找最优解。 常见的分支限界算法：TSP（旅行商问题），单源最短路径 概率算法 概率算法分类： 数值概率算法：用于求解没有精确解的数值问题 蒙特卡罗算法：求解问题的准确解 拉斯维加斯算法：不会得到不正确的解，眼下之意，要么不存在，要么正确 舍伍德算法：总能求问题的某一个解，且必然正确 常见的概率算法： 数值概率：设计伪随机数 舍伍德算法：跳跃表 拉斯维加斯算法：皇后问题 蒙特卡罗算法：素数测试 其他的未提到的算法 字符串的算法： KMP算法 Manacher算法 最长回文串，公共子串，公共子序列 全排列 字符串替换 正则表达式 数组 丑数 水仙花 剩余定理 乘积数组 矩阵中路径 逆序对 连续数组最大和 链表 链表的合并 重复节点删除 反转链表 链表环的入口 查找 二叉搜索 约瑟夫环 二进制1的个数 二维数组的查找 排序 大约9个排序算法，分析时间空间复杂度，分析稳定性 二叉树 对称二叉树 层次遍历 序列话二叉树 非递归的前中后序遍历 二叉搜索树的第k个节点 map,set,array在java、C++的用法 剩余定理 快速幂取模 欧几里德算法 ","date":"2018-12-27","objectID":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:0","series":null,"tags":["算法","数据结构"],"title":"算法与数据结构","uri":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/#定义"},{"categories":["算法","数据结构"],"content":" 定义 算法: 算法中的指令描述的是一个计算，当其运行时能从一个初始状态和（可能为空的）初始输入开始，经过一系列有限而清晰定义的状态，最终产生输出并停止于一个终态。一个状态到另一个状态的转移不一定是确定的。随机化算法在内的一些算法，包含了一些随机输入. 数据结构: 数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率. 先说数据结构,毕竟算法也得有东西算,才又算法,所谓先有鸡后有蛋 数据结构:既然是结构，里面肯定是有不同的数据的，与其叫数据结构，不如叫数据流,数据流中包含了各种数据，数据结构构成流，流亦是数据结构，面向对象就是，万物皆数据，以流的形式呈现. 算法就是解决问题的方法,以函数或者是类的形式出现，它决定传入的参数，对数据流进行修改，留下需要的，抛弃不需要的，将数据和其他数据组成在一起，形成数据流，将数据流和数据流组成在一起，形成新的数据流，将数据流拆分成数据，再聚合，这就是算法的作用。 算法就是数据流的灵魂,没有算法的数据其实没有灵魂的 数据结构 数据结构的分类： 线性结构：静态数组，动态数组，链表，队列，栈，字符串 树形结构：二叉树，B/B+/B-树，红黑树，哈夫曼树 图形结构：图 其他：跳跃表，map，set 基于数据结构的基本算法： 数组的算法：插入，删除，新建，查找 链表的算法：插入，删除，合并，查找 栈的算法：新建，进栈，出栈 队列的算法：新建，入队，出队，优先队列 字符串的算法：查找，字串匹配 二叉树的算法：前中序遍历，查找，深度，平衡二叉树的插入删除等等 图的算法：DFS/BFS，图的结构，Prim算法，Dijkstra算法，kruskal算法，拓扑排序 排序算法：各种排序算法 以上是的这些算法都是比较基本的，在学习其数据结构时，就依附存在的算法 算法 算法是一种解题思想，解决问题的方法 算法的分类 算法的复杂度分析 递归与分治算法 动态规划 贪心算法 回溯算法 分支限界算法 概率算法 具体概念讲解 算法的复杂度 时间复杂度和空间复杂度的分析：应该会对自己写的代码进行复杂度分析，比如空间和时间复杂度是多少。 PS：排序的时间复杂度：最优的情况是O(nlogn)，最坏的情况是O(n^2),有一个特殊情况是O(n)–位排序和计数排序 递归和分治算法 **递归：直接或间接调用自身的算法成为递归算法。**PS：树结构非常适合递归算法 递归的一个例子：斐波那契数列（入门必学的一个错误的递归算法），可以用递归来做，但是实际上效率比较低。 分治算法：将一个大问题分解为k个规模比较小的子问题，并且这些子问题互相独立，但是又与原问题相同。 PS：分治思想是一个基本的思想，递归和其后的dp(动态规划)以及贪心算法很多时候都是基于分治来做。 从分治出发设计出来的算法，一般是递归算法。 常见的分治算法：排序算法中的归并排序，快速排序，二分搜索 动态规划算法 动态规划算法：与分治算法思想类似，两者不同点在于，dp的子问题并不是互相独立的，在分治算法中，有些子问题会被重复计算，效率很低，因此，动态规划，对分治有了优化，记录子问题，去掉重复性。 PS：动态规划是对分治算法的优化 动态规划适用于最优化算法 使用动态规划特征： 求一个问题的最优解 大问题可以分解为子问题，子问题还有重叠的更小的子问题 整体问题最优解取决于子问题的最优解（状态转移方程） 从上往下分析问题，从下往上解决问题 讨论底层的边界问题 注意上面的三点：子问题重叠，状态转移方程，边界问题 PS：尝试使用动态规划去解斐波那契数列 常见的动态规划题目：最长公共子串，最长公共子序列，最长回文串，矩阵乘法，背包问题 贪心算法 贪心算法：类似于动态规划，两者不同点在于，贪心算法考虑局部最优，动态规划考虑全局最优，如果一个问题，全局最优是通过局部最优逼近的，那么此时可以考虑贪心算法 PS：贪心算法不能对所有的问题都能得到整体最优解，但对某些问题可以。 贪心算法是对某些动态规划问题的一个简化 常见的贪心算法：图的单源最短路径，最小生成树的问题。 PS：如何区别贪心算法和动态规划呢？ 回溯算法 回溯算法：回溯算法具有\"通用解题法\"之称， 回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。 PS：它是用于求解组合数较大的问题。 回溯法的解题步骤： 针对所给问题，确定问题的解空间： 首先应明确定义问题的解空间，问题的解空间应至少包含问题的一个（最优）解。 确定结点的扩展搜索规则。 以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。 常见的回溯算法：皇后问题，输出不重复数字的全排列，求一个集合的所有子集 分支限界算法 分支限界算法：分支限界算法类似于回溯算法，两者不同点在于，回溯算法是找出解空间中的所有解，分支限界算法是找出某一个解(这个解或者是最优的)，其次，回溯算法是用深度优先方式搜索解空间，分支限界算法以广度优先的方式搜索解空间。 PS：某种意义上来说，分支限界算法也是寻找最优解。 常见的分支限界算法：TSP（旅行商问题），单源最短路径 概率算法 概率算法分类： 数值概率算法：用于求解没有精确解的数值问题 蒙特卡罗算法：求解问题的准确解 拉斯维加斯算法：不会得到不正确的解，眼下之意，要么不存在，要么正确 舍伍德算法：总能求问题的某一个解，且必然正确 常见的概率算法： 数值概率：设计伪随机数 舍伍德算法：跳跃表 拉斯维加斯算法：皇后问题 蒙特卡罗算法：素数测试 其他的未提到的算法 字符串的算法： KMP算法 Manacher算法 最长回文串，公共子串，公共子序列 全排列 字符串替换 正则表达式 数组 丑数 水仙花 剩余定理 乘积数组 矩阵中路径 逆序对 连续数组最大和 链表 链表的合并 重复节点删除 反转链表 链表环的入口 查找 二叉搜索 约瑟夫环 二进制1的个数 二维数组的查找 排序 大约9个排序算法，分析时间空间复杂度，分析稳定性 二叉树 对称二叉树 层次遍历 序列话二叉树 非递归的前中后序遍历 二叉搜索树的第k个节点 map,set,array在java、C++的用法 剩余定理 快速幂取模 欧几里德算法 ","date":"2018-12-27","objectID":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:0","series":null,"tags":["算法","数据结构"],"title":"算法与数据结构","uri":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/#数据结构"},{"categories":["算法","数据结构"],"content":" 定义 算法: 算法中的指令描述的是一个计算，当其运行时能从一个初始状态和（可能为空的）初始输入开始，经过一系列有限而清晰定义的状态，最终产生输出并停止于一个终态。一个状态到另一个状态的转移不一定是确定的。随机化算法在内的一些算法，包含了一些随机输入. 数据结构: 数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率. 先说数据结构,毕竟算法也得有东西算,才又算法,所谓先有鸡后有蛋 数据结构:既然是结构，里面肯定是有不同的数据的，与其叫数据结构，不如叫数据流,数据流中包含了各种数据，数据结构构成流，流亦是数据结构，面向对象就是，万物皆数据，以流的形式呈现. 算法就是解决问题的方法,以函数或者是类的形式出现，它决定传入的参数，对数据流进行修改，留下需要的，抛弃不需要的，将数据和其他数据组成在一起，形成数据流，将数据流和数据流组成在一起，形成新的数据流，将数据流拆分成数据，再聚合，这就是算法的作用。 算法就是数据流的灵魂,没有算法的数据其实没有灵魂的 数据结构 数据结构的分类： 线性结构：静态数组，动态数组，链表，队列，栈，字符串 树形结构：二叉树，B/B+/B-树，红黑树，哈夫曼树 图形结构：图 其他：跳跃表，map，set 基于数据结构的基本算法： 数组的算法：插入，删除，新建，查找 链表的算法：插入，删除，合并，查找 栈的算法：新建，进栈，出栈 队列的算法：新建，入队，出队，优先队列 字符串的算法：查找，字串匹配 二叉树的算法：前中序遍历，查找，深度，平衡二叉树的插入删除等等 图的算法：DFS/BFS，图的结构，Prim算法，Dijkstra算法，kruskal算法，拓扑排序 排序算法：各种排序算法 以上是的这些算法都是比较基本的，在学习其数据结构时，就依附存在的算法 算法 算法是一种解题思想，解决问题的方法 算法的分类 算法的复杂度分析 递归与分治算法 动态规划 贪心算法 回溯算法 分支限界算法 概率算法 具体概念讲解 算法的复杂度 时间复杂度和空间复杂度的分析：应该会对自己写的代码进行复杂度分析，比如空间和时间复杂度是多少。 PS：排序的时间复杂度：最优的情况是O(nlogn)，最坏的情况是O(n^2),有一个特殊情况是O(n)–位排序和计数排序 递归和分治算法 **递归：直接或间接调用自身的算法成为递归算法。**PS：树结构非常适合递归算法 递归的一个例子：斐波那契数列（入门必学的一个错误的递归算法），可以用递归来做，但是实际上效率比较低。 分治算法：将一个大问题分解为k个规模比较小的子问题，并且这些子问题互相独立，但是又与原问题相同。 PS：分治思想是一个基本的思想，递归和其后的dp(动态规划)以及贪心算法很多时候都是基于分治来做。 从分治出发设计出来的算法，一般是递归算法。 常见的分治算法：排序算法中的归并排序，快速排序，二分搜索 动态规划算法 动态规划算法：与分治算法思想类似，两者不同点在于，dp的子问题并不是互相独立的，在分治算法中，有些子问题会被重复计算，效率很低，因此，动态规划，对分治有了优化，记录子问题，去掉重复性。 PS：动态规划是对分治算法的优化 动态规划适用于最优化算法 使用动态规划特征： 求一个问题的最优解 大问题可以分解为子问题，子问题还有重叠的更小的子问题 整体问题最优解取决于子问题的最优解（状态转移方程） 从上往下分析问题，从下往上解决问题 讨论底层的边界问题 注意上面的三点：子问题重叠，状态转移方程，边界问题 PS：尝试使用动态规划去解斐波那契数列 常见的动态规划题目：最长公共子串，最长公共子序列，最长回文串，矩阵乘法，背包问题 贪心算法 贪心算法：类似于动态规划，两者不同点在于，贪心算法考虑局部最优，动态规划考虑全局最优，如果一个问题，全局最优是通过局部最优逼近的，那么此时可以考虑贪心算法 PS：贪心算法不能对所有的问题都能得到整体最优解，但对某些问题可以。 贪心算法是对某些动态规划问题的一个简化 常见的贪心算法：图的单源最短路径，最小生成树的问题。 PS：如何区别贪心算法和动态规划呢？ 回溯算法 回溯算法：回溯算法具有\"通用解题法\"之称， 回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。 PS：它是用于求解组合数较大的问题。 回溯法的解题步骤： 针对所给问题，确定问题的解空间： 首先应明确定义问题的解空间，问题的解空间应至少包含问题的一个（最优）解。 确定结点的扩展搜索规则。 以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。 常见的回溯算法：皇后问题，输出不重复数字的全排列，求一个集合的所有子集 分支限界算法 分支限界算法：分支限界算法类似于回溯算法，两者不同点在于，回溯算法是找出解空间中的所有解，分支限界算法是找出某一个解(这个解或者是最优的)，其次，回溯算法是用深度优先方式搜索解空间，分支限界算法以广度优先的方式搜索解空间。 PS：某种意义上来说，分支限界算法也是寻找最优解。 常见的分支限界算法：TSP（旅行商问题），单源最短路径 概率算法 概率算法分类： 数值概率算法：用于求解没有精确解的数值问题 蒙特卡罗算法：求解问题的准确解 拉斯维加斯算法：不会得到不正确的解，眼下之意，要么不存在，要么正确 舍伍德算法：总能求问题的某一个解，且必然正确 常见的概率算法： 数值概率：设计伪随机数 舍伍德算法：跳跃表 拉斯维加斯算法：皇后问题 蒙特卡罗算法：素数测试 其他的未提到的算法 字符串的算法： KMP算法 Manacher算法 最长回文串，公共子串，公共子序列 全排列 字符串替换 正则表达式 数组 丑数 水仙花 剩余定理 乘积数组 矩阵中路径 逆序对 连续数组最大和 链表 链表的合并 重复节点删除 反转链表 链表环的入口 查找 二叉搜索 约瑟夫环 二进制1的个数 二维数组的查找 排序 大约9个排序算法，分析时间空间复杂度，分析稳定性 二叉树 对称二叉树 层次遍历 序列话二叉树 非递归的前中后序遍历 二叉搜索树的第k个节点 map,set,array在java、C++的用法 剩余定理 快速幂取模 欧几里德算法 ","date":"2018-12-27","objectID":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:0","series":null,"tags":["算法","数据结构"],"title":"算法与数据结构","uri":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/#算法"},{"categories":["算法","数据结构"],"content":" 定义 算法: 算法中的指令描述的是一个计算，当其运行时能从一个初始状态和（可能为空的）初始输入开始，经过一系列有限而清晰定义的状态，最终产生输出并停止于一个终态。一个状态到另一个状态的转移不一定是确定的。随机化算法在内的一些算法，包含了一些随机输入. 数据结构: 数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率. 先说数据结构,毕竟算法也得有东西算,才又算法,所谓先有鸡后有蛋 数据结构:既然是结构，里面肯定是有不同的数据的，与其叫数据结构，不如叫数据流,数据流中包含了各种数据，数据结构构成流，流亦是数据结构，面向对象就是，万物皆数据，以流的形式呈现. 算法就是解决问题的方法,以函数或者是类的形式出现，它决定传入的参数，对数据流进行修改，留下需要的，抛弃不需要的，将数据和其他数据组成在一起，形成数据流，将数据流和数据流组成在一起，形成新的数据流，将数据流拆分成数据，再聚合，这就是算法的作用。 算法就是数据流的灵魂,没有算法的数据其实没有灵魂的 数据结构 数据结构的分类： 线性结构：静态数组，动态数组，链表，队列，栈，字符串 树形结构：二叉树，B/B+/B-树，红黑树，哈夫曼树 图形结构：图 其他：跳跃表，map，set 基于数据结构的基本算法： 数组的算法：插入，删除，新建，查找 链表的算法：插入，删除，合并，查找 栈的算法：新建，进栈，出栈 队列的算法：新建，入队，出队，优先队列 字符串的算法：查找，字串匹配 二叉树的算法：前中序遍历，查找，深度，平衡二叉树的插入删除等等 图的算法：DFS/BFS，图的结构，Prim算法，Dijkstra算法，kruskal算法，拓扑排序 排序算法：各种排序算法 以上是的这些算法都是比较基本的，在学习其数据结构时，就依附存在的算法 算法 算法是一种解题思想，解决问题的方法 算法的分类 算法的复杂度分析 递归与分治算法 动态规划 贪心算法 回溯算法 分支限界算法 概率算法 具体概念讲解 算法的复杂度 时间复杂度和空间复杂度的分析：应该会对自己写的代码进行复杂度分析，比如空间和时间复杂度是多少。 PS：排序的时间复杂度：最优的情况是O(nlogn)，最坏的情况是O(n^2),有一个特殊情况是O(n)–位排序和计数排序 递归和分治算法 **递归：直接或间接调用自身的算法成为递归算法。**PS：树结构非常适合递归算法 递归的一个例子：斐波那契数列（入门必学的一个错误的递归算法），可以用递归来做，但是实际上效率比较低。 分治算法：将一个大问题分解为k个规模比较小的子问题，并且这些子问题互相独立，但是又与原问题相同。 PS：分治思想是一个基本的思想，递归和其后的dp(动态规划)以及贪心算法很多时候都是基于分治来做。 从分治出发设计出来的算法，一般是递归算法。 常见的分治算法：排序算法中的归并排序，快速排序，二分搜索 动态规划算法 动态规划算法：与分治算法思想类似，两者不同点在于，dp的子问题并不是互相独立的，在分治算法中，有些子问题会被重复计算，效率很低，因此，动态规划，对分治有了优化，记录子问题，去掉重复性。 PS：动态规划是对分治算法的优化 动态规划适用于最优化算法 使用动态规划特征： 求一个问题的最优解 大问题可以分解为子问题，子问题还有重叠的更小的子问题 整体问题最优解取决于子问题的最优解（状态转移方程） 从上往下分析问题，从下往上解决问题 讨论底层的边界问题 注意上面的三点：子问题重叠，状态转移方程，边界问题 PS：尝试使用动态规划去解斐波那契数列 常见的动态规划题目：最长公共子串，最长公共子序列，最长回文串，矩阵乘法，背包问题 贪心算法 贪心算法：类似于动态规划，两者不同点在于，贪心算法考虑局部最优，动态规划考虑全局最优，如果一个问题，全局最优是通过局部最优逼近的，那么此时可以考虑贪心算法 PS：贪心算法不能对所有的问题都能得到整体最优解，但对某些问题可以。 贪心算法是对某些动态规划问题的一个简化 常见的贪心算法：图的单源最短路径，最小生成树的问题。 PS：如何区别贪心算法和动态规划呢？ 回溯算法 回溯算法：回溯算法具有\"通用解题法\"之称， 回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。 PS：它是用于求解组合数较大的问题。 回溯法的解题步骤： 针对所给问题，确定问题的解空间： 首先应明确定义问题的解空间，问题的解空间应至少包含问题的一个（最优）解。 确定结点的扩展搜索规则。 以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。 常见的回溯算法：皇后问题，输出不重复数字的全排列，求一个集合的所有子集 分支限界算法 分支限界算法：分支限界算法类似于回溯算法，两者不同点在于，回溯算法是找出解空间中的所有解，分支限界算法是找出某一个解(这个解或者是最优的)，其次，回溯算法是用深度优先方式搜索解空间，分支限界算法以广度优先的方式搜索解空间。 PS：某种意义上来说，分支限界算法也是寻找最优解。 常见的分支限界算法：TSP（旅行商问题），单源最短路径 概率算法 概率算法分类： 数值概率算法：用于求解没有精确解的数值问题 蒙特卡罗算法：求解问题的准确解 拉斯维加斯算法：不会得到不正确的解，眼下之意，要么不存在，要么正确 舍伍德算法：总能求问题的某一个解，且必然正确 常见的概率算法： 数值概率：设计伪随机数 舍伍德算法：跳跃表 拉斯维加斯算法：皇后问题 蒙特卡罗算法：素数测试 其他的未提到的算法 字符串的算法： KMP算法 Manacher算法 最长回文串，公共子串，公共子序列 全排列 字符串替换 正则表达式 数组 丑数 水仙花 剩余定理 乘积数组 矩阵中路径 逆序对 连续数组最大和 链表 链表的合并 重复节点删除 反转链表 链表环的入口 查找 二叉搜索 约瑟夫环 二进制1的个数 二维数组的查找 排序 大约9个排序算法，分析时间空间复杂度，分析稳定性 二叉树 对称二叉树 层次遍历 序列话二叉树 非递归的前中后序遍历 二叉搜索树的第k个节点 map,set,array在java、C++的用法 剩余定理 快速幂取模 欧几里德算法 ","date":"2018-12-27","objectID":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:0","series":null,"tags":["算法","数据结构"],"title":"算法与数据结构","uri":"/%E7%AE%97%E6%B3%95%E4%B8%8E%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/#其他的未提到的算法"},{"categories":["深度学习"],"content":" keras基本介绍 Keras是由纯python编写的基于不同的深度学习后端开发的深度学习框架。 支持的后端有： 谷歌的 TensorFlow 后端 微软的 CNTK 后端 Theano 后端 Keras是一个高层神经网络API，支持快速实验，能够把你的idea迅速转换为结果，如果有如下需求，可以优先选择Keras： 简易和快速的原型设计（keras具有高度模块化，极简，和可扩充特性） 支持CNN和RNN，或二者的结合 无缝CPU和GPU切换 keras的优点： 用户友好：Keras是为人类而不是天顶星人设计的API。用户的使用体验始终是我们考虑的首要和中心内容。Keras遵循减少认知困难的最佳实践：Keras提供一致而简洁的API， 能够极大减少一般应用下用户的工作量，同时，Keras提供清晰和具有实践意义的bug反馈。 模块性：模型可理解为一个层的序列或数据的运算图，完全可配置的模块可以用最少的代价自由组合在一起。具体而言，网络层、损失函数、优化器、初始化策略、激活函数、正则化方法都是独立的模块，你可以使用它们来构建自己的模型。 易扩展性：添加新模块超级容易，只需要仿照现有的模块编写新的类或函数即可。创建新模块的便利性使得Keras更适合于先进的研究工作。 与Python协作：Keras没有单独的模型配置文件类型（作为对比，caffe有），模型由python代码描述，使其更紧凑和更易debug，并提供了扩展的便利性。 ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:1","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras基本介绍"},{"categories":["深度学习"],"content":" keras模块 keras的整体框架 keras搭建神经网络的步骤 ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:2","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras模块"},{"categories":["深度学习"],"content":" keras模块 keras的整体框架 keras搭建神经网络的步骤 ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:2","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras的整体框架"},{"categories":["深度学习"],"content":" keras模块 keras的整体框架 keras搭建神经网络的步骤 ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:2","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras搭建神经网络的步骤"},{"categories":["深度学习"],"content":" keras的安装 # GPU 版本 pip install --upgrade tensorflow-gpu # CPU 版本 pip install --upgrade tensorflow # Keras 安装 pip install keras -U --pre ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:3","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras的安装"},{"categories":["深度学习"],"content":" keras实例 keras入门实例之回归模型 Keras中定义一个单层全连接网络，进行线性回归模型的训练 import numpy as np np.random.seed(42) from keras.models import Sequential from keras.layers import Dense import matplotlib.pyplot as plt %matplotlib inline # 创建数据集,首先生成200个从-1,1的数据 X = np.linspace(-1, 1, 2000) np.random.shuffle(X) # 打乱数据 Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (2000, )) # 假设我们真实模型为：Y=0.5X+2 X_train, Y_train = X[:1600], Y[:1600] # 把前160个数据放到训练集 X_test, Y_test = X[1600:], Y[1600:] # 把后40个点放到测试集 # 定义一个model， model = Sequential () # Keras有两种类型的模型，序贯模型（Sequential）和函数式模型 # 比较常用的是Sequential，它是单输入单输出的 model.add(Dense(output_dim=1, input_dim=1)) # 通过add()方法一层层添加模型 # Dense是全连接层，第一层需要定义输入， # 第二层无需指定输入，一般第二层把第一层的输出作为输入 # 定义完模型就需要训练了，不过训练之前我们需要指定一些训练参数 # 通过compile()方法选择损失函数和优化器 # 这里我们用均方误差作为损失函数，随机梯度下降作为优化方法 model.compile(loss='mse', optimizer='sgd') # 开始训练 print('Training -----------') for step in range(3001): cost = model.train_on_batch(X_train, Y_train) # Keras有很多开始训练的函数，这里用train_on_batch（） if step % 500 == 0: print('train cost: ', cost) # 测试训练好的模型 print('\\nTesting ------------') cost = model.evaluate(X_test, Y_test, batch_size=40) print('test cost:', cost) W, b = model.layers[0].get_weights() # 查看训练出的网络参数 # 由于我们网络只有一层，且每次训练的输入只有一个，输出只有一个 # 因此第一层训练出Y=WX+B这个模型，其中W,b为训练出的参数 输出的结果为： Training ----------- train cost: 4.6456146 train cost: 0.0033609855 train cost: 0.0024392079 train cost: 0.0024380628 train cost: 0.002438061 train cost: 0.002438061 train cost: 0.002438061 Testing ------------ 400/400 [==============================] - 0s 916us/step test cost: 0.0025163212092593314 print('Weights=', W, '\\nbiases=', b) # plotting the prediction Y_pred = model.predict(X_test) plt.scatter(X_test, Y_test) plt.plot(X_test, Y_pred,color='red') plt.show() # 输出权重以及可视化结果 # Weights= [[0.5028866]] # biases= [2.0018716] 从上图可以看出，结果还是相对比较准确的。 keras入门实例之手写数字识别 使用全连接层神经网络来预测手写数字 from keras.models import Sequential # 采用贯序模型 from keras.layers import Input, Dense, Dropout, Activation from keras.models import Model from keras.optimizers import SGD,RMSprop from keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np %matplotlib inline # 参数设置 tBatchSize = 16 # 批处理的大小 tEpoches = 20 # 迭代次数 '''第一步：选择模型''' model = Sequential() # 采用贯序模型 '''第二步：构建网络层''' '''构建网络只是构建了一个网络结构，并定义网络的参数，此时还没有输入的数据集''' #构建的第一个层作为输入层 # Dense 这是第一个隐藏层，并附带定义了输入层，该隐含层有400个神经元。输入则是 784个节点 model.add(Dense(400,input_shape=(784,))) # 输入层，28*28=784 输入层将二维矩阵换成了一维向量输入 model.add(Activation('relu')) # 激活函数是relu #model.add(Dropout(0.5)) # 采用50%的dropout 随机取一半进行训练 #构建的第3个层作为输出层 model.add(Dense(10)) # 输出结果是10个类别，所以维度是10 model.add(Activation('softmax')) # 最后一层用softmax作为激活函数 '''第三步：网络优化和编译''' # lr：大于0的浮点数，学习率 rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # 只有通过了编译，model才真正的建立起来，这时候才能够被使用 #model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用交叉熵作为loss函数 这是原例子，但是执行出错 model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy']) print(model.summary()) # 模型的结果为： \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_16 (Dense) (None, 400) 314000 _________________________________________________________________ activation_16 (Activation) (None, 400) 0 _________________________________________________________________ dense_17 (Dense) (None, 10) 4010 _________________________________________________________________ activation_17 (Activation) (None, 10) 0 ================================================================= Total params: 318,010 Trainable params: 318,010 Non-trainable params: 0 _________________________________________________________________ \"\"\" '''第四步：训练''' # 数据集获取 mnist data = np.load(\"./mnist/mnist.npz\") X_train,y_train,X_test,y_test = data['x_train'],data['y_train'],data['x_test'],data['y_test'] # 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维 X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] *","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras实例"},{"categories":["深度学习"],"content":" keras实例 keras入门实例之回归模型 Keras中定义一个单层全连接网络，进行线性回归模型的训练 import numpy as np np.random.seed(42) from keras.models import Sequential from keras.layers import Dense import matplotlib.pyplot as plt %matplotlib inline # 创建数据集,首先生成200个从-1,1的数据 X = np.linspace(-1, 1, 2000) np.random.shuffle(X) # 打乱数据 Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (2000, )) # 假设我们真实模型为：Y=0.5X+2 X_train, Y_train = X[:1600], Y[:1600] # 把前160个数据放到训练集 X_test, Y_test = X[1600:], Y[1600:] # 把后40个点放到测试集 # 定义一个model， model = Sequential () # Keras有两种类型的模型，序贯模型（Sequential）和函数式模型 # 比较常用的是Sequential，它是单输入单输出的 model.add(Dense(output_dim=1, input_dim=1)) # 通过add()方法一层层添加模型 # Dense是全连接层，第一层需要定义输入， # 第二层无需指定输入，一般第二层把第一层的输出作为输入 # 定义完模型就需要训练了，不过训练之前我们需要指定一些训练参数 # 通过compile()方法选择损失函数和优化器 # 这里我们用均方误差作为损失函数，随机梯度下降作为优化方法 model.compile(loss='mse', optimizer='sgd') # 开始训练 print('Training -----------') for step in range(3001): cost = model.train_on_batch(X_train, Y_train) # Keras有很多开始训练的函数，这里用train_on_batch（） if step % 500 == 0: print('train cost: ', cost) # 测试训练好的模型 print('\\nTesting ------------') cost = model.evaluate(X_test, Y_test, batch_size=40) print('test cost:', cost) W, b = model.layers[0].get_weights() # 查看训练出的网络参数 # 由于我们网络只有一层，且每次训练的输入只有一个，输出只有一个 # 因此第一层训练出Y=WX+B这个模型，其中W,b为训练出的参数 输出的结果为： Training ----------- train cost: 4.6456146 train cost: 0.0033609855 train cost: 0.0024392079 train cost: 0.0024380628 train cost: 0.002438061 train cost: 0.002438061 train cost: 0.002438061 Testing ------------ 400/400 [==============================] - 0s 916us/step test cost: 0.0025163212092593314 print('Weights=', W, '\\nbiases=', b) # plotting the prediction Y_pred = model.predict(X_test) plt.scatter(X_test, Y_test) plt.plot(X_test, Y_pred,color='red') plt.show() # 输出权重以及可视化结果 # Weights= [[0.5028866]] # biases= [2.0018716] 从上图可以看出，结果还是相对比较准确的。 keras入门实例之手写数字识别 使用全连接层神经网络来预测手写数字 from keras.models import Sequential # 采用贯序模型 from keras.layers import Input, Dense, Dropout, Activation from keras.models import Model from keras.optimizers import SGD,RMSprop from keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np %matplotlib inline # 参数设置 tBatchSize = 16 # 批处理的大小 tEpoches = 20 # 迭代次数 '''第一步：选择模型''' model = Sequential() # 采用贯序模型 '''第二步：构建网络层''' '''构建网络只是构建了一个网络结构，并定义网络的参数，此时还没有输入的数据集''' #构建的第一个层作为输入层 # Dense 这是第一个隐藏层，并附带定义了输入层，该隐含层有400个神经元。输入则是 784个节点 model.add(Dense(400,input_shape=(784,))) # 输入层，28*28=784 输入层将二维矩阵换成了一维向量输入 model.add(Activation('relu')) # 激活函数是relu #model.add(Dropout(0.5)) # 采用50%的dropout 随机取一半进行训练 #构建的第3个层作为输出层 model.add(Dense(10)) # 输出结果是10个类别，所以维度是10 model.add(Activation('softmax')) # 最后一层用softmax作为激活函数 '''第三步：网络优化和编译''' # lr：大于0的浮点数，学习率 rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # 只有通过了编译，model才真正的建立起来，这时候才能够被使用 #model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用交叉熵作为loss函数 这是原例子，但是执行出错 model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy']) print(model.summary()) # 模型的结果为： \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_16 (Dense) (None, 400) 314000 _________________________________________________________________ activation_16 (Activation) (None, 400) 0 _________________________________________________________________ dense_17 (Dense) (None, 10) 4010 _________________________________________________________________ activation_17 (Activation) (None, 10) 0 ================================================================= Total params: 318,010 Trainable params: 318,010 Non-trainable params: 0 _________________________________________________________________ \"\"\" '''第四步：训练''' # 数据集获取 mnist data = np.load(\"./mnist/mnist.npz\") X_train,y_train,X_test,y_test = data['x_train'],data['y_train'],data['x_test'],data['y_test'] # 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维 X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] *","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras入门实例之回归模型"},{"categories":["深度学习"],"content":" keras实例 keras入门实例之回归模型 Keras中定义一个单层全连接网络，进行线性回归模型的训练 import numpy as np np.random.seed(42) from keras.models import Sequential from keras.layers import Dense import matplotlib.pyplot as plt %matplotlib inline # 创建数据集,首先生成200个从-1,1的数据 X = np.linspace(-1, 1, 2000) np.random.shuffle(X) # 打乱数据 Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (2000, )) # 假设我们真实模型为：Y=0.5X+2 X_train, Y_train = X[:1600], Y[:1600] # 把前160个数据放到训练集 X_test, Y_test = X[1600:], Y[1600:] # 把后40个点放到测试集 # 定义一个model， model = Sequential () # Keras有两种类型的模型，序贯模型（Sequential）和函数式模型 # 比较常用的是Sequential，它是单输入单输出的 model.add(Dense(output_dim=1, input_dim=1)) # 通过add()方法一层层添加模型 # Dense是全连接层，第一层需要定义输入， # 第二层无需指定输入，一般第二层把第一层的输出作为输入 # 定义完模型就需要训练了，不过训练之前我们需要指定一些训练参数 # 通过compile()方法选择损失函数和优化器 # 这里我们用均方误差作为损失函数，随机梯度下降作为优化方法 model.compile(loss='mse', optimizer='sgd') # 开始训练 print('Training -----------') for step in range(3001): cost = model.train_on_batch(X_train, Y_train) # Keras有很多开始训练的函数，这里用train_on_batch（） if step % 500 == 0: print('train cost: ', cost) # 测试训练好的模型 print('\\nTesting ------------') cost = model.evaluate(X_test, Y_test, batch_size=40) print('test cost:', cost) W, b = model.layers[0].get_weights() # 查看训练出的网络参数 # 由于我们网络只有一层，且每次训练的输入只有一个，输出只有一个 # 因此第一层训练出Y=WX+B这个模型，其中W,b为训练出的参数 输出的结果为： Training ----------- train cost: 4.6456146 train cost: 0.0033609855 train cost: 0.0024392079 train cost: 0.0024380628 train cost: 0.002438061 train cost: 0.002438061 train cost: 0.002438061 Testing ------------ 400/400 [==============================] - 0s 916us/step test cost: 0.0025163212092593314 print('Weights=', W, '\\nbiases=', b) # plotting the prediction Y_pred = model.predict(X_test) plt.scatter(X_test, Y_test) plt.plot(X_test, Y_pred,color='red') plt.show() # 输出权重以及可视化结果 # Weights= [[0.5028866]] # biases= [2.0018716] 从上图可以看出，结果还是相对比较准确的。 keras入门实例之手写数字识别 使用全连接层神经网络来预测手写数字 from keras.models import Sequential # 采用贯序模型 from keras.layers import Input, Dense, Dropout, Activation from keras.models import Model from keras.optimizers import SGD,RMSprop from keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np %matplotlib inline # 参数设置 tBatchSize = 16 # 批处理的大小 tEpoches = 20 # 迭代次数 '''第一步：选择模型''' model = Sequential() # 采用贯序模型 '''第二步：构建网络层''' '''构建网络只是构建了一个网络结构，并定义网络的参数，此时还没有输入的数据集''' #构建的第一个层作为输入层 # Dense 这是第一个隐藏层，并附带定义了输入层，该隐含层有400个神经元。输入则是 784个节点 model.add(Dense(400,input_shape=(784,))) # 输入层，28*28=784 输入层将二维矩阵换成了一维向量输入 model.add(Activation('relu')) # 激活函数是relu #model.add(Dropout(0.5)) # 采用50%的dropout 随机取一半进行训练 #构建的第3个层作为输出层 model.add(Dense(10)) # 输出结果是10个类别，所以维度是10 model.add(Activation('softmax')) # 最后一层用softmax作为激活函数 '''第三步：网络优化和编译''' # lr：大于0的浮点数，学习率 rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # 只有通过了编译，model才真正的建立起来，这时候才能够被使用 #model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用交叉熵作为loss函数 这是原例子，但是执行出错 model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy']) print(model.summary()) # 模型的结果为： \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_16 (Dense) (None, 400) 314000 _________________________________________________________________ activation_16 (Activation) (None, 400) 0 _________________________________________________________________ dense_17 (Dense) (None, 10) 4010 _________________________________________________________________ activation_17 (Activation) (None, 10) 0 ================================================================= Total params: 318,010 Trainable params: 318,010 Non-trainable params: 0 _________________________________________________________________ \"\"\" '''第四步：训练''' # 数据集获取 mnist data = np.load(\"./mnist/mnist.npz\") X_train,y_train,X_test,y_test = data['x_train'],data['y_train'],data['x_test'],data['y_test'] # 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维 X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] *","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras入门实例之手写数字识别"},{"categories":["深度学习"],"content":" keras实例 keras入门实例之回归模型 Keras中定义一个单层全连接网络，进行线性回归模型的训练 import numpy as np np.random.seed(42) from keras.models import Sequential from keras.layers import Dense import matplotlib.pyplot as plt %matplotlib inline # 创建数据集,首先生成200个从-1,1的数据 X = np.linspace(-1, 1, 2000) np.random.shuffle(X) # 打乱数据 Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (2000, )) # 假设我们真实模型为：Y=0.5X+2 X_train, Y_train = X[:1600], Y[:1600] # 把前160个数据放到训练集 X_test, Y_test = X[1600:], Y[1600:] # 把后40个点放到测试集 # 定义一个model， model = Sequential () # Keras有两种类型的模型，序贯模型（Sequential）和函数式模型 # 比较常用的是Sequential，它是单输入单输出的 model.add(Dense(output_dim=1, input_dim=1)) # 通过add()方法一层层添加模型 # Dense是全连接层，第一层需要定义输入， # 第二层无需指定输入，一般第二层把第一层的输出作为输入 # 定义完模型就需要训练了，不过训练之前我们需要指定一些训练参数 # 通过compile()方法选择损失函数和优化器 # 这里我们用均方误差作为损失函数，随机梯度下降作为优化方法 model.compile(loss='mse', optimizer='sgd') # 开始训练 print('Training -----------') for step in range(3001): cost = model.train_on_batch(X_train, Y_train) # Keras有很多开始训练的函数，这里用train_on_batch（） if step % 500 == 0: print('train cost: ', cost) # 测试训练好的模型 print('\\nTesting ------------') cost = model.evaluate(X_test, Y_test, batch_size=40) print('test cost:', cost) W, b = model.layers[0].get_weights() # 查看训练出的网络参数 # 由于我们网络只有一层，且每次训练的输入只有一个，输出只有一个 # 因此第一层训练出Y=WX+B这个模型，其中W,b为训练出的参数 输出的结果为： Training ----------- train cost: 4.6456146 train cost: 0.0033609855 train cost: 0.0024392079 train cost: 0.0024380628 train cost: 0.002438061 train cost: 0.002438061 train cost: 0.002438061 Testing ------------ 400/400 [==============================] - 0s 916us/step test cost: 0.0025163212092593314 print('Weights=', W, '\\nbiases=', b) # plotting the prediction Y_pred = model.predict(X_test) plt.scatter(X_test, Y_test) plt.plot(X_test, Y_pred,color='red') plt.show() # 输出权重以及可视化结果 # Weights= [[0.5028866]] # biases= [2.0018716] 从上图可以看出，结果还是相对比较准确的。 keras入门实例之手写数字识别 使用全连接层神经网络来预测手写数字 from keras.models import Sequential # 采用贯序模型 from keras.layers import Input, Dense, Dropout, Activation from keras.models import Model from keras.optimizers import SGD,RMSprop from keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np %matplotlib inline # 参数设置 tBatchSize = 16 # 批处理的大小 tEpoches = 20 # 迭代次数 '''第一步：选择模型''' model = Sequential() # 采用贯序模型 '''第二步：构建网络层''' '''构建网络只是构建了一个网络结构，并定义网络的参数，此时还没有输入的数据集''' #构建的第一个层作为输入层 # Dense 这是第一个隐藏层，并附带定义了输入层，该隐含层有400个神经元。输入则是 784个节点 model.add(Dense(400,input_shape=(784,))) # 输入层，28*28=784 输入层将二维矩阵换成了一维向量输入 model.add(Activation('relu')) # 激活函数是relu #model.add(Dropout(0.5)) # 采用50%的dropout 随机取一半进行训练 #构建的第3个层作为输出层 model.add(Dense(10)) # 输出结果是10个类别，所以维度是10 model.add(Activation('softmax')) # 最后一层用softmax作为激活函数 '''第三步：网络优化和编译''' # lr：大于0的浮点数，学习率 rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # 只有通过了编译，model才真正的建立起来，这时候才能够被使用 #model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用交叉熵作为loss函数 这是原例子，但是执行出错 model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy']) print(model.summary()) # 模型的结果为： \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_16 (Dense) (None, 400) 314000 _________________________________________________________________ activation_16 (Activation) (None, 400) 0 _________________________________________________________________ dense_17 (Dense) (None, 10) 4010 _________________________________________________________________ activation_17 (Activation) (None, 10) 0 ================================================================= Total params: 318,010 Trainable params: 318,010 Non-trainable params: 0 _________________________________________________________________ \"\"\" '''第四步：训练''' # 数据集获取 mnist data = np.load(\"./mnist/mnist.npz\") X_train,y_train,X_test,y_test = data['x_train'],data['y_train'],data['x_test'],data['y_test'] # 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维 X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] *","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras入门实例之卷积神经网络"},{"categories":["深度学习"],"content":" keras实例 keras入门实例之回归模型 Keras中定义一个单层全连接网络，进行线性回归模型的训练 import numpy as np np.random.seed(42) from keras.models import Sequential from keras.layers import Dense import matplotlib.pyplot as plt %matplotlib inline # 创建数据集,首先生成200个从-1,1的数据 X = np.linspace(-1, 1, 2000) np.random.shuffle(X) # 打乱数据 Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (2000, )) # 假设我们真实模型为：Y=0.5X+2 X_train, Y_train = X[:1600], Y[:1600] # 把前160个数据放到训练集 X_test, Y_test = X[1600:], Y[1600:] # 把后40个点放到测试集 # 定义一个model， model = Sequential () # Keras有两种类型的模型，序贯模型（Sequential）和函数式模型 # 比较常用的是Sequential，它是单输入单输出的 model.add(Dense(output_dim=1, input_dim=1)) # 通过add()方法一层层添加模型 # Dense是全连接层，第一层需要定义输入， # 第二层无需指定输入，一般第二层把第一层的输出作为输入 # 定义完模型就需要训练了，不过训练之前我们需要指定一些训练参数 # 通过compile()方法选择损失函数和优化器 # 这里我们用均方误差作为损失函数，随机梯度下降作为优化方法 model.compile(loss='mse', optimizer='sgd') # 开始训练 print('Training -----------') for step in range(3001): cost = model.train_on_batch(X_train, Y_train) # Keras有很多开始训练的函数，这里用train_on_batch（） if step % 500 == 0: print('train cost: ', cost) # 测试训练好的模型 print('\\nTesting ------------') cost = model.evaluate(X_test, Y_test, batch_size=40) print('test cost:', cost) W, b = model.layers[0].get_weights() # 查看训练出的网络参数 # 由于我们网络只有一层，且每次训练的输入只有一个，输出只有一个 # 因此第一层训练出Y=WX+B这个模型，其中W,b为训练出的参数 输出的结果为： Training ----------- train cost: 4.6456146 train cost: 0.0033609855 train cost: 0.0024392079 train cost: 0.0024380628 train cost: 0.002438061 train cost: 0.002438061 train cost: 0.002438061 Testing ------------ 400/400 [==============================] - 0s 916us/step test cost: 0.0025163212092593314 print('Weights=', W, '\\nbiases=', b) # plotting the prediction Y_pred = model.predict(X_test) plt.scatter(X_test, Y_test) plt.plot(X_test, Y_pred,color='red') plt.show() # 输出权重以及可视化结果 # Weights= [[0.5028866]] # biases= [2.0018716] 从上图可以看出，结果还是相对比较准确的。 keras入门实例之手写数字识别 使用全连接层神经网络来预测手写数字 from keras.models import Sequential # 采用贯序模型 from keras.layers import Input, Dense, Dropout, Activation from keras.models import Model from keras.optimizers import SGD,RMSprop from keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np %matplotlib inline # 参数设置 tBatchSize = 16 # 批处理的大小 tEpoches = 20 # 迭代次数 '''第一步：选择模型''' model = Sequential() # 采用贯序模型 '''第二步：构建网络层''' '''构建网络只是构建了一个网络结构，并定义网络的参数，此时还没有输入的数据集''' #构建的第一个层作为输入层 # Dense 这是第一个隐藏层，并附带定义了输入层，该隐含层有400个神经元。输入则是 784个节点 model.add(Dense(400,input_shape=(784,))) # 输入层，28*28=784 输入层将二维矩阵换成了一维向量输入 model.add(Activation('relu')) # 激活函数是relu #model.add(Dropout(0.5)) # 采用50%的dropout 随机取一半进行训练 #构建的第3个层作为输出层 model.add(Dense(10)) # 输出结果是10个类别，所以维度是10 model.add(Activation('softmax')) # 最后一层用softmax作为激活函数 '''第三步：网络优化和编译''' # lr：大于0的浮点数，学习率 rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # 只有通过了编译，model才真正的建立起来，这时候才能够被使用 #model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用交叉熵作为loss函数 这是原例子，但是执行出错 model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy']) print(model.summary()) # 模型的结果为： \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_16 (Dense) (None, 400) 314000 _________________________________________________________________ activation_16 (Activation) (None, 400) 0 _________________________________________________________________ dense_17 (Dense) (None, 10) 4010 _________________________________________________________________ activation_17 (Activation) (None, 10) 0 ================================================================= Total params: 318,010 Trainable params: 318,010 Non-trainable params: 0 _________________________________________________________________ \"\"\" '''第四步：训练''' # 数据集获取 mnist data = np.load(\"./mnist/mnist.npz\") X_train,y_train,X_test,y_test = data['x_train'],data['y_train'],data['x_test'],data['y_test'] # 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维 X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] *","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras入门实例之循环神经网络"},{"categories":["深度学习"],"content":" keras实例 keras入门实例之回归模型 Keras中定义一个单层全连接网络，进行线性回归模型的训练 import numpy as np np.random.seed(42) from keras.models import Sequential from keras.layers import Dense import matplotlib.pyplot as plt %matplotlib inline # 创建数据集,首先生成200个从-1,1的数据 X = np.linspace(-1, 1, 2000) np.random.shuffle(X) # 打乱数据 Y = 0.5 * X + 2 + np.random.normal(0, 0.05, (2000, )) # 假设我们真实模型为：Y=0.5X+2 X_train, Y_train = X[:1600], Y[:1600] # 把前160个数据放到训练集 X_test, Y_test = X[1600:], Y[1600:] # 把后40个点放到测试集 # 定义一个model， model = Sequential () # Keras有两种类型的模型，序贯模型（Sequential）和函数式模型 # 比较常用的是Sequential，它是单输入单输出的 model.add(Dense(output_dim=1, input_dim=1)) # 通过add()方法一层层添加模型 # Dense是全连接层，第一层需要定义输入， # 第二层无需指定输入，一般第二层把第一层的输出作为输入 # 定义完模型就需要训练了，不过训练之前我们需要指定一些训练参数 # 通过compile()方法选择损失函数和优化器 # 这里我们用均方误差作为损失函数，随机梯度下降作为优化方法 model.compile(loss='mse', optimizer='sgd') # 开始训练 print('Training -----------') for step in range(3001): cost = model.train_on_batch(X_train, Y_train) # Keras有很多开始训练的函数，这里用train_on_batch（） if step % 500 == 0: print('train cost: ', cost) # 测试训练好的模型 print('\\nTesting ------------') cost = model.evaluate(X_test, Y_test, batch_size=40) print('test cost:', cost) W, b = model.layers[0].get_weights() # 查看训练出的网络参数 # 由于我们网络只有一层，且每次训练的输入只有一个，输出只有一个 # 因此第一层训练出Y=WX+B这个模型，其中W,b为训练出的参数 输出的结果为： Training ----------- train cost: 4.6456146 train cost: 0.0033609855 train cost: 0.0024392079 train cost: 0.0024380628 train cost: 0.002438061 train cost: 0.002438061 train cost: 0.002438061 Testing ------------ 400/400 [==============================] - 0s 916us/step test cost: 0.0025163212092593314 print('Weights=', W, '\\nbiases=', b) # plotting the prediction Y_pred = model.predict(X_test) plt.scatter(X_test, Y_test) plt.plot(X_test, Y_pred,color='red') plt.show() # 输出权重以及可视化结果 # Weights= [[0.5028866]] # biases= [2.0018716] 从上图可以看出，结果还是相对比较准确的。 keras入门实例之手写数字识别 使用全连接层神经网络来预测手写数字 from keras.models import Sequential # 采用贯序模型 from keras.layers import Input, Dense, Dropout, Activation from keras.models import Model from keras.optimizers import SGD,RMSprop from keras.datasets import mnist import matplotlib.pyplot as plt import numpy as np %matplotlib inline # 参数设置 tBatchSize = 16 # 批处理的大小 tEpoches = 20 # 迭代次数 '''第一步：选择模型''' model = Sequential() # 采用贯序模型 '''第二步：构建网络层''' '''构建网络只是构建了一个网络结构，并定义网络的参数，此时还没有输入的数据集''' #构建的第一个层作为输入层 # Dense 这是第一个隐藏层，并附带定义了输入层，该隐含层有400个神经元。输入则是 784个节点 model.add(Dense(400,input_shape=(784,))) # 输入层，28*28=784 输入层将二维矩阵换成了一维向量输入 model.add(Activation('relu')) # 激活函数是relu #model.add(Dropout(0.5)) # 采用50%的dropout 随机取一半进行训练 #构建的第3个层作为输出层 model.add(Dense(10)) # 输出结果是10个类别，所以维度是10 model.add(Activation('softmax')) # 最后一层用softmax作为激活函数 '''第三步：网络优化和编译''' # lr：大于0的浮点数，学习率 rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) # 只有通过了编译，model才真正的建立起来，这时候才能够被使用 #model.compile(loss='categorical_crossentropy', optimizer=sgd, class_mode='categorical') # 使用交叉熵作为loss函数 这是原例子，但是执行出错 model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy']) print(model.summary()) # 模型的结果为： \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_16 (Dense) (None, 400) 314000 _________________________________________________________________ activation_16 (Activation) (None, 400) 0 _________________________________________________________________ dense_17 (Dense) (None, 10) 4010 _________________________________________________________________ activation_17 (Activation) (None, 10) 0 ================================================================= Total params: 318,010 Trainable params: 318,010 Non-trainable params: 0 _________________________________________________________________ \"\"\" '''第四步：训练''' # 数据集获取 mnist data = np.load(\"./mnist/mnist.npz\") X_train,y_train,X_test,y_test = data['x_train'],data['y_train'],data['x_test'],data['y_test'] # 由于mist的输入数据维度是(num, 28, 28)，这里需要把后面的维度直接拼起来变成784维 X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] *","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:4","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#keras入门实例之自编码神经网络"},{"categories":["深度学习"],"content":" 其他Keras使用细节 指定占用的GPU keras在使用GPU的时候有个特点，就是默认全部占满显存。 这样如果有多个模型都需要使用GPU跑的话，那么限制是很大的，而且对于GPU也是一种浪费。因此在使用keras时需要有意识的设置运行时使用那块显卡，需要使用多少容量。 这方面的设置一般有三种情况： 指定显卡 限制GPU用量 即指定显卡又限制GPU用量 指定显卡 import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" 限制GPU用量 # 如果出现显存的问题,那么需要使用以下的代码 import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' # 分配策略 config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 显存的占比 config.gpu_options.allow_growth = True # 允许显存增量使用 set_session(tf.Session(config=config)) 指定显卡和限制GPU用量 # 也就是将上面的两种情况结合起来使用 import os import tensorflow as tf import keras.backend.tensorflow_backend as KTF # 指定第一块GPU可用 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 sess = tf.Session(config=config) KTF.set_session(sess) 查看与保存模型结构 查看搭建的网络 model.summary() \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_6 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_31 (Dense) (None, 10) 510 _________________________________________________________________ activation_49 (Activation) (None, 10) 0 ================================================================= Total params: 4,460 Trainable params: 4,460 Non-trainable params: 0 _________________________________________________________________ \"\"\" 图片的方式保存模型的结构 from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file='./data/autoEncode.png') ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#其他keras使用细节"},{"categories":["深度学习"],"content":" 其他Keras使用细节 指定占用的GPU keras在使用GPU的时候有个特点，就是默认全部占满显存。 这样如果有多个模型都需要使用GPU跑的话，那么限制是很大的，而且对于GPU也是一种浪费。因此在使用keras时需要有意识的设置运行时使用那块显卡，需要使用多少容量。 这方面的设置一般有三种情况： 指定显卡 限制GPU用量 即指定显卡又限制GPU用量 指定显卡 import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" 限制GPU用量 # 如果出现显存的问题,那么需要使用以下的代码 import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' # 分配策略 config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 显存的占比 config.gpu_options.allow_growth = True # 允许显存增量使用 set_session(tf.Session(config=config)) 指定显卡和限制GPU用量 # 也就是将上面的两种情况结合起来使用 import os import tensorflow as tf import keras.backend.tensorflow_backend as KTF # 指定第一块GPU可用 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 sess = tf.Session(config=config) KTF.set_session(sess) 查看与保存模型结构 查看搭建的网络 model.summary() \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_6 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_31 (Dense) (None, 10) 510 _________________________________________________________________ activation_49 (Activation) (None, 10) 0 ================================================================= Total params: 4,460 Trainable params: 4,460 Non-trainable params: 0 _________________________________________________________________ \"\"\" 图片的方式保存模型的结构 from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file='./data/autoEncode.png') ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#指定占用的gpu"},{"categories":["深度学习"],"content":" 其他Keras使用细节 指定占用的GPU keras在使用GPU的时候有个特点，就是默认全部占满显存。 这样如果有多个模型都需要使用GPU跑的话，那么限制是很大的，而且对于GPU也是一种浪费。因此在使用keras时需要有意识的设置运行时使用那块显卡，需要使用多少容量。 这方面的设置一般有三种情况： 指定显卡 限制GPU用量 即指定显卡又限制GPU用量 指定显卡 import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" 限制GPU用量 # 如果出现显存的问题,那么需要使用以下的代码 import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' # 分配策略 config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 显存的占比 config.gpu_options.allow_growth = True # 允许显存增量使用 set_session(tf.Session(config=config)) 指定显卡和限制GPU用量 # 也就是将上面的两种情况结合起来使用 import os import tensorflow as tf import keras.backend.tensorflow_backend as KTF # 指定第一块GPU可用 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 sess = tf.Session(config=config) KTF.set_session(sess) 查看与保存模型结构 查看搭建的网络 model.summary() \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_6 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_31 (Dense) (None, 10) 510 _________________________________________________________________ activation_49 (Activation) (None, 10) 0 ================================================================= Total params: 4,460 Trainable params: 4,460 Non-trainable params: 0 _________________________________________________________________ \"\"\" 图片的方式保存模型的结构 from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file='./data/autoEncode.png') ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#指定显卡"},{"categories":["深度学习"],"content":" 其他Keras使用细节 指定占用的GPU keras在使用GPU的时候有个特点，就是默认全部占满显存。 这样如果有多个模型都需要使用GPU跑的话，那么限制是很大的，而且对于GPU也是一种浪费。因此在使用keras时需要有意识的设置运行时使用那块显卡，需要使用多少容量。 这方面的设置一般有三种情况： 指定显卡 限制GPU用量 即指定显卡又限制GPU用量 指定显卡 import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" 限制GPU用量 # 如果出现显存的问题,那么需要使用以下的代码 import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' # 分配策略 config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 显存的占比 config.gpu_options.allow_growth = True # 允许显存增量使用 set_session(tf.Session(config=config)) 指定显卡和限制GPU用量 # 也就是将上面的两种情况结合起来使用 import os import tensorflow as tf import keras.backend.tensorflow_backend as KTF # 指定第一块GPU可用 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 sess = tf.Session(config=config) KTF.set_session(sess) 查看与保存模型结构 查看搭建的网络 model.summary() \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_6 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_31 (Dense) (None, 10) 510 _________________________________________________________________ activation_49 (Activation) (None, 10) 0 ================================================================= Total params: 4,460 Trainable params: 4,460 Non-trainable params: 0 _________________________________________________________________ \"\"\" 图片的方式保存模型的结构 from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file='./data/autoEncode.png') ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#限制gpu用量"},{"categories":["深度学习"],"content":" 其他Keras使用细节 指定占用的GPU keras在使用GPU的时候有个特点，就是默认全部占满显存。 这样如果有多个模型都需要使用GPU跑的话，那么限制是很大的，而且对于GPU也是一种浪费。因此在使用keras时需要有意识的设置运行时使用那块显卡，需要使用多少容量。 这方面的设置一般有三种情况： 指定显卡 限制GPU用量 即指定显卡又限制GPU用量 指定显卡 import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" 限制GPU用量 # 如果出现显存的问题,那么需要使用以下的代码 import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' # 分配策略 config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 显存的占比 config.gpu_options.allow_growth = True # 允许显存增量使用 set_session(tf.Session(config=config)) 指定显卡和限制GPU用量 # 也就是将上面的两种情况结合起来使用 import os import tensorflow as tf import keras.backend.tensorflow_backend as KTF # 指定第一块GPU可用 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 sess = tf.Session(config=config) KTF.set_session(sess) 查看与保存模型结构 查看搭建的网络 model.summary() \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_6 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_31 (Dense) (None, 10) 510 _________________________________________________________________ activation_49 (Activation) (None, 10) 0 ================================================================= Total params: 4,460 Trainable params: 4,460 Non-trainable params: 0 _________________________________________________________________ \"\"\" 图片的方式保存模型的结构 from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file='./data/autoEncode.png') ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#指定显卡和限制gpu用量"},{"categories":["深度学习"],"content":" 其他Keras使用细节 指定占用的GPU keras在使用GPU的时候有个特点，就是默认全部占满显存。 这样如果有多个模型都需要使用GPU跑的话，那么限制是很大的，而且对于GPU也是一种浪费。因此在使用keras时需要有意识的设置运行时使用那块显卡，需要使用多少容量。 这方面的设置一般有三种情况： 指定显卡 限制GPU用量 即指定显卡又限制GPU用量 指定显卡 import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" 限制GPU用量 # 如果出现显存的问题,那么需要使用以下的代码 import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' # 分配策略 config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 显存的占比 config.gpu_options.allow_growth = True # 允许显存增量使用 set_session(tf.Session(config=config)) 指定显卡和限制GPU用量 # 也就是将上面的两种情况结合起来使用 import os import tensorflow as tf import keras.backend.tensorflow_backend as KTF # 指定第一块GPU可用 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 sess = tf.Session(config=config) KTF.set_session(sess) 查看与保存模型结构 查看搭建的网络 model.summary() \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_6 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_31 (Dense) (None, 10) 510 _________________________________________________________________ activation_49 (Activation) (None, 10) 0 ================================================================= Total params: 4,460 Trainable params: 4,460 Non-trainable params: 0 _________________________________________________________________ \"\"\" 图片的方式保存模型的结构 from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file='./data/autoEncode.png') ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#查看与保存模型结构"},{"categories":["深度学习"],"content":" 其他Keras使用细节 指定占用的GPU keras在使用GPU的时候有个特点，就是默认全部占满显存。 这样如果有多个模型都需要使用GPU跑的话，那么限制是很大的，而且对于GPU也是一种浪费。因此在使用keras时需要有意识的设置运行时使用那块显卡，需要使用多少容量。 这方面的设置一般有三种情况： 指定显卡 限制GPU用量 即指定显卡又限制GPU用量 指定显卡 import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" 限制GPU用量 # 如果出现显存的问题,那么需要使用以下的代码 import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' # 分配策略 config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 显存的占比 config.gpu_options.allow_growth = True # 允许显存增量使用 set_session(tf.Session(config=config)) 指定显卡和限制GPU用量 # 也就是将上面的两种情况结合起来使用 import os import tensorflow as tf import keras.backend.tensorflow_backend as KTF # 指定第一块GPU可用 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 sess = tf.Session(config=config) KTF.set_session(sess) 查看与保存模型结构 查看搭建的网络 model.summary() \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_6 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_31 (Dense) (None, 10) 510 _________________________________________________________________ activation_49 (Activation) (None, 10) 0 ================================================================= Total params: 4,460 Trainable params: 4,460 Non-trainable params: 0 _________________________________________________________________ \"\"\" 图片的方式保存模型的结构 from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file='./data/autoEncode.png') ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#查看搭建的网络"},{"categories":["深度学习"],"content":" 其他Keras使用细节 指定占用的GPU keras在使用GPU的时候有个特点，就是默认全部占满显存。 这样如果有多个模型都需要使用GPU跑的话，那么限制是很大的，而且对于GPU也是一种浪费。因此在使用keras时需要有意识的设置运行时使用那块显卡，需要使用多少容量。 这方面的设置一般有三种情况： 指定显卡 限制GPU用量 即指定显卡又限制GPU用量 指定显卡 import os os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\" 限制GPU用量 # 如果出现显存的问题,那么需要使用以下的代码 import tensorflow as tf from keras.backend.tensorflow_backend import set_session config = tf.ConfigProto() config.gpu_options.allocator_type = 'BFC' # 分配策略 config.gpu_options.per_process_gpu_memory_fraction = 0.3 # 显存的占比 config.gpu_options.allow_growth = True # 允许显存增量使用 set_session(tf.Session(config=config)) 指定显卡和限制GPU用量 # 也就是将上面的两种情况结合起来使用 import os import tensorflow as tf import keras.backend.tensorflow_backend as KTF # 指定第一块GPU可用 os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" config = tf.ConfigProto() config.gpu_options.allow_growth=True #不全部占满显存, 按需分配 sess = tf.Session(config=config) KTF.set_session(sess) 查看与保存模型结构 查看搭建的网络 model.summary() \"\"\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_6 (SimpleRNN) (None, 50) 3950 _________________________________________________________________ dense_31 (Dense) (None, 10) 510 _________________________________________________________________ activation_49 (Activation) (None, 10) 0 ================================================================= Total params: 4,460 Trainable params: 4,460 Non-trainable params: 0 _________________________________________________________________ \"\"\" 图片的方式保存模型的结构 from keras.utils.vis_utils import plot_model plot_model(autoencoder, to_file='./data/autoEncode.png') ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:5","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#图片的方式保存模型的结构"},{"categories":["深度学习"],"content":" 参考链接 keras中文文档（官方） keras中文文档（非官方） 莫烦keras教程代码 莫烦keras视频教程 Keras FAQ: Frequently Asked Keras Questions 一个不负责任的Keras介绍（上） 一个不负责任的Keras介绍（中） 一个不负责任的Keras介绍（下） ","date":"2018-12-26","objectID":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/:0:6","series":null,"tags":["python","深度学习","Keras"],"title":"keras基本入门","uri":"/keras%E5%9F%BA%E6%9C%AC%E5%85%A5%E9%97%A8/#参考链接"},{"categories":["python"],"content":" NumPy 是一个 Python 包。 它代表 “Numeric Python”。 它是一个由多维数组对象和用于处理数组的例程集合组成的库。 Numeric，即 NumPy 的前身，是由 Jim Hugunin 开发的。 也开发了另一个包 Numarray ，它拥有一些额外的功能。 2005年，Travis Oliphant 通过将 Numarray 的功能集成到 Numeric 包中来创建 NumPy 包。 这个开源项目有很多贡献者。 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:0","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#numpy-基础"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#创建数组"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#基本运算"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#索引-切片-迭代"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#形状操作"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#组合stack不同的数组"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#视图view和浅复制"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#深复制"},{"categories":["python"],"content":" Numpy 基础 NumPy的主要对象是同种元素的多维数组。这是一个所有的元素都是一种类型、通过一个正整数元组索引的元素表格(通常是元素是数字)。在NumPy中维度(dimensions)叫做轴(axes)，轴的个数叫做秩(rank)。 NumPy的数组类被称作ndarray。通常被称作数组。注意numpy.array和标准Python库类array.array并不相同，后者只处理一维数组和提供少量功能。更多重要ndarray对象属性有： ndarray.ndim: 数组轴的个数，在python的世界中，轴的个数被称作秩 ndarray.shape:数组的维度。这是一个指示数组在每个维度上大小的整数元组。例如一个n排m列的矩阵，它的shape属性将是(2,3),这个元组的长度显然是秩，即维度或者ndim属性 ndarray.size 数组元素的总个数，等于shape属性中元组元素的乘积。 ndarray.dtype 一个用来描述数组中元素类型的对象，可以通过创造或指定dtype使用标准Python类型。另外NumPy提供它自己的数据类型。 ndarray.itemsize 数组中每个元素的字节大小。例如，一个元素类型为float64的数组itemsiz属性值为8(=64/8),又如，一个元素类型为complex32的数组item属性为4(=32/8). ndarray.data 包含实际数组元素的缓冲区，通常我们不需要使用这个属性，因为我们总是通过索引来使用数组中的元素。 import numpy as np a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a.shape #(3,5) a.ndim #2 a.dtype.name #\"int64\" a.itemsize #8 a.size #15 type(a) #numpy.ndarray 创建数组 # 创建数组有多种方式 # 1.使用np.array(),将list转换成ndarray import numpy as np a = np.array([2,3,4]) # 2. 数组将序列包含序列转化成二维的数组，序列包含序列包含序列转化成三维数组等等。 b = np.array( [ (1.5,2,3), (4,5,6) ] ) # array([[1.5, 2. , 3. ], # [4. , 5. , 6. ]]) #3. 使用占位符 # 通常，数组的元素开始都是未知的，但是它的大小已知。因此，NumPy提供了一些使用占位符创建数组的函数。这最小化了扩展数组的需要和高昂的运算代价。 # 函数 zeros创建一个全是0的数组，函数 ones创建一个全1的数组，函数 empty创建一个内容随机并且依赖与内存状态的数组。默认创建的数组类型(dtype)都是float64。 c = np.zeros( (3,4) ) d = np.ones( (3,4),dtype=int16) # 4. 使用arrange函数 arange参数：(start,end,step) e = np.arange( 10, 30, 5 ) # 5. 使用linspace生成指定元素 linspace参数: (start,end,num) f = np.linspace(10,20,100) 基本运算 数组的算术运算是按元素的。新的数组被创建并且被结果填充。 import numpy as np a = np.array([20,30,40,50]) b = np.arange(4) # 1. 基本运算 b + a # array([20, 31, 42, 53]) a - b #array([20, 29, 38, 47]) b**2 #array([0, 1, 4, 9]) a \u003c 35 # array([ True, True, False, False]) # ２.NumPy中的乘法运算符*指示按元素计算，矩阵乘法可以使用dot函数或创建矩阵对象实现 ## 对应元素的乘法 A = np.array( [[1,1], [0,1]] ) # array([[1, 1], # [0, 1]]) B = np.array( [[2,0],[3,4]] ) # array([[2, 0], # [3, 4]]) A*B #array([[2, 0], # [0, 4]]) # 3. 指定 axis参数,在指定的轴上进行计算 ## axis :0代表行，1代表列 c = np.arange(12).reshape(3,4) # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) c.sum(axis = 0) # array([12, 15, 18, 21]) c.sum(axis = 1) # array([ 6, 22, 38]) 索引 切片 迭代 一维数组可以被索引、切片和迭代，就像列表和其它Python序列。 import numpy as np # 多维数组可以每个轴有一个索引。这些索引由一个逗号分割的元组给出。 a = np.arange(15).reshape(3, 5) a # array([[ 0, 1, 2, 3, 4], # [ 5, 6, 7, 8, 9], # [10, 11, 12, 13, 14]]) a[0,0] # 0 # 第一行 a[0,:] # array([0, 1, 2, 3, 4]) # 第一列 a[:,0] #array([ 0, 5, 10]) 形状操作 一个数组的形状可以被多种命令修改 flatten ravel transpose reshape 由 ravel()展平的数组元素的顺序通常是“C风格”的，就是说，最右边的索引变化得最快，所以元素a[0,0]之后是a[0,1]。如果数组被改变形状(reshape)成其它形状，数组仍然是“C风格”的。NumPy通常创建一个以这个顺序保存数据的数组，所以 ravel()将总是不需要复制它的参数3。但是如果数组是通过切片其它数组或有不同寻常的选项时，它可能需要被复制。函数 reshape()和 ravel()还可以被同过一些可选参数构建成FORTRAN风格的数组，即最左边的索引变化最快。 reshape函数改变参数形状并返回它，而resize函数改变数组自身。 如果在改变形状操作中一个维度被给做-1，其维度将自动被计算 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[5., 3., 7., 2.], # [6., 7., 7., 0.], # [1., 9., 9., 0.]]) a.shape # (3,4) a.ravel() # 展成一维数组 # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.flatten() # array([5., 3., 7., 2., 6., 7., 7., 0., 1., 9., 9., 0.]) a.transpose() # 等于a.T # array([[5., 6., 1.], # [3., 7., 9.], # [7., 7., 9.], # [2., 0., 0.]]) # 更改自身维度 a.resize((2,6)) a.shape # (2, 6) 组合(stack)不同的数组 import numpy as np a = np.floor(10*np.random.random((2,2))) # array([[0., 9.], # [3., 4.]]) b = np.floor(10*np.random.random((2,2))) # array([[4., 5.], # [2., 5.]]) ## vstack 垂直排列 等于 np.concatenate([a,b],axis = 0) np.vstack((a,b)) # array([[0., 9.], # [3., 4.], # [4., 5.], # [2., 5.]]) ## hstack 水平排列 等于 np.concatenate([a,b],axis = 1) np.hstack([a,b]) # array([[0., 9., 4., 5.], # [3., 4., 2., 5.]]) ## concatenate 根据参数stack 视图(view)和浅复制 不同的数组对象分享同一个数据。视图方法创造一个新的数组对象指向同一数据。 import numpy as np # 生成一个二维的数据 a = np.floor(10*np.random.random((3,4))) # array([[2., 9., 5., 9.], # [5., 7., 3., 6.], # [3., 1., 9., 0.]]) c = a.view() c is a # False c.base is a # True c.shape = 2,6 array([[ 2., 9., 5., 9., 5., 7.], [ 3., 6., 3., 1., 9., 0.]]) c.shape # (2,6) a.shape","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:1","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#常用方法总览"},{"categories":["python"],"content":" Numpy 进阶 线性代数 import numpy as np # 导入线性代数的库 from numpy import linalg as ll a = np.array([[1.0, 2.0], [3.0, 4.0]]) # array([[1., 2.], # [3., 4.]]) a.transpose() # 转置 a.T # array([[1., 3.], # [2., 4.]]) ll.inv(a) # 矩阵求逆 # array([[-2. , 1. ], # [ 1.5, -0.5]]) u = np.eye(2) # 生成一个单位矩阵 # array([[1., 0.], # [0., 1.]]) j = np.array([[0.0, -1.0], [1.0, 0.0]]) # array([[ 0., -1.], # [ 1., 0.]]) np.dot (j, j) #矩阵乘法运算 # array([[-1., 0.], # [ 0., -1.]]) np.trace(u) # trace # 2.0 y = np.array([[5.], [7.]]) # array([[5.], # [7.]]) ll.solve(a, y) #计算a*y的解 # array([[-3.], # [ 4.]]) ll.eig(j) #向量的特征值和特征向量 # (array([0.+1.j, 0.-1.j]), # array([[0.70710678+0. j, 0.70710678-0. j], # [0. -0.70710678j, 0. +0.70710678j]])) 矩阵类 import numpy as np A = np.matrix('1.0 2.0; 3.0 4.0') ## 生成一个矩阵 # matrix([[1., 2.], # [3., 4.]]) type(A) # numpy.matrixlib.defmatrix.matrix A.T ## transpose # matrix([[1., 3.], # [2., 4.]]) X = np.matrix('5.0 7.0') # matrix([[5., 7.]]) Y = X.transpose() ## X.T # matrix([[5.], # [7.]]) A*Y ## 矩阵乘法，注意维度 # matrix([[19.], # [43.]]) A.I ## inverse # matrix([[-2. , 1. ], # [ 1.5, -0.5]]) ll.solve(A, Y) ## solve A*Y # matrix([[-3.], # [ 4.]]) 注意NumPy中数组和矩阵有些重要的区别。 NumPy提供了两个基本的对象：一个N维数组对象和一个通用函数对象。其它对象都是建构在它们之上 的。特别的，矩阵是继承自NumPy数组对象的二维数组对象。 对数组和矩阵，索引都必须包含合适的一个或多个这些组合：整数标量、省略号 (ellipses)、整数列表;布尔值，整数或布尔值构成的元组，和一个一维整数或布尔值数组。矩阵可以被用作矩阵的索引，但是通常需要数组、列表或者 其它形式来完成这个任务。 像平常在Python中一样，索引是从0开始的。传统上我们用矩形的行和列表示一个二维数组或矩阵，其中沿着0轴的方向被穿过的称作行，沿着1轴的方向被穿过的是列。 矩阵与二维数组的不同 import numpy as np A = np.arange(12) #array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) A.shape = (3,4) # A # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) M = mat(A.copy()) # matrix([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) A[:,1] # array([1, 5, 9]) A[:,1].shape #(3,) M[:,1] # matrix([[1], # [5], # [9]]) (3, 1) 注意最后两个结果的不同。 对二维数组使用一个冒号产生一个一维数组，然而矩阵产生了一个二维矩阵。 广播法则(rule) 广播法则能使通用函数有意义地处理不具有相同形状的输入。 当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播 广播的原则：如果两个数组的后缘维度（trailing dimension，即从末尾开始算起的维度）的轴长度相符，或其中的一方的长度为1，则认为它们是广播兼容的。广播会在缺失和（或）长度为1的维度上进行。 这句话乃是理解广播的核心。广播主要发生在两种情况，一种是两个数组的维数不相等，但是它们的后缘维度的轴长相符，另外一种是有一方的长度为1。 import numpy as np a = np.array([[ 0, 0, 0], [10,10,10], [20,20,20], [30,30,30]]) b = np.array([1,2,3]) print(a + b) ####### # [[ 1 2 3] # [11 12 13] # [21 22 23] # [31 32 33]] 4x3 的二维数组与长为 3 的一维数组相加，等效于把数组 b 在二维上重复 4 次再运算。 广播的规则: 让所有输入数组都向其中形状最长的数组看齐，形状中不足的部分都通过在前面加 1 补齐。 输出数组的形状是输入数组形状的各个维度上的最大值。 如果输入数组的某个维度和输出数组的对应维度的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错。 当输入数组的某个维度的长度为 1 时，沿着此维度运算时都用此维度上的第一组值。 **简单理解：**对两个数组，分别比较他们的每一个维度（若其中一个数组没有当前维度则忽略），满足： 数组拥有相同形状。 当前维度的值相等。 当前维度的值有一个是 1。 若条件不满足，抛出 “ValueError: frames are not aligned” 异常。 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#numpy-进阶"},{"categories":["python"],"content":" Numpy 进阶 线性代数 import numpy as np # 导入线性代数的库 from numpy import linalg as ll a = np.array([[1.0, 2.0], [3.0, 4.0]]) # array([[1., 2.], # [3., 4.]]) a.transpose() # 转置 a.T # array([[1., 3.], # [2., 4.]]) ll.inv(a) # 矩阵求逆 # array([[-2. , 1. ], # [ 1.5, -0.5]]) u = np.eye(2) # 生成一个单位矩阵 # array([[1., 0.], # [0., 1.]]) j = np.array([[0.0, -1.0], [1.0, 0.0]]) # array([[ 0., -1.], # [ 1., 0.]]) np.dot (j, j) #矩阵乘法运算 # array([[-1., 0.], # [ 0., -1.]]) np.trace(u) # trace # 2.0 y = np.array([[5.], [7.]]) # array([[5.], # [7.]]) ll.solve(a, y) #计算a*y的解 # array([[-3.], # [ 4.]]) ll.eig(j) #向量的特征值和特征向量 # (array([0.+1.j, 0.-1.j]), # array([[0.70710678+0. j, 0.70710678-0. j], # [0. -0.70710678j, 0. +0.70710678j]])) 矩阵类 import numpy as np A = np.matrix('1.0 2.0; 3.0 4.0') ## 生成一个矩阵 # matrix([[1., 2.], # [3., 4.]]) type(A) # numpy.matrixlib.defmatrix.matrix A.T ## transpose # matrix([[1., 3.], # [2., 4.]]) X = np.matrix('5.0 7.0') # matrix([[5., 7.]]) Y = X.transpose() ## X.T # matrix([[5.], # [7.]]) A*Y ## 矩阵乘法，注意维度 # matrix([[19.], # [43.]]) A.I ## inverse # matrix([[-2. , 1. ], # [ 1.5, -0.5]]) ll.solve(A, Y) ## solve A*Y # matrix([[-3.], # [ 4.]]) 注意NumPy中数组和矩阵有些重要的区别。 NumPy提供了两个基本的对象：一个N维数组对象和一个通用函数对象。其它对象都是建构在它们之上 的。特别的，矩阵是继承自NumPy数组对象的二维数组对象。 对数组和矩阵，索引都必须包含合适的一个或多个这些组合：整数标量、省略号 (ellipses)、整数列表;布尔值，整数或布尔值构成的元组，和一个一维整数或布尔值数组。矩阵可以被用作矩阵的索引，但是通常需要数组、列表或者 其它形式来完成这个任务。 像平常在Python中一样，索引是从0开始的。传统上我们用矩形的行和列表示一个二维数组或矩阵，其中沿着0轴的方向被穿过的称作行，沿着1轴的方向被穿过的是列。 矩阵与二维数组的不同 import numpy as np A = np.arange(12) #array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) A.shape = (3,4) # A # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) M = mat(A.copy()) # matrix([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) A[:,1] # array([1, 5, 9]) A[:,1].shape #(3,) M[:,1] # matrix([[1], # [5], # [9]]) (3, 1) 注意最后两个结果的不同。 对二维数组使用一个冒号产生一个一维数组，然而矩阵产生了一个二维矩阵。 广播法则(rule) 广播法则能使通用函数有意义地处理不具有相同形状的输入。 当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播 广播的原则：如果两个数组的后缘维度（trailing dimension，即从末尾开始算起的维度）的轴长度相符，或其中的一方的长度为1，则认为它们是广播兼容的。广播会在缺失和（或）长度为1的维度上进行。 这句话乃是理解广播的核心。广播主要发生在两种情况，一种是两个数组的维数不相等，但是它们的后缘维度的轴长相符，另外一种是有一方的长度为1。 import numpy as np a = np.array([[ 0, 0, 0], [10,10,10], [20,20,20], [30,30,30]]) b = np.array([1,2,3]) print(a + b) ####### # [[ 1 2 3] # [11 12 13] # [21 22 23] # [31 32 33]] 4x3 的二维数组与长为 3 的一维数组相加，等效于把数组 b 在二维上重复 4 次再运算。 广播的规则: 让所有输入数组都向其中形状最长的数组看齐，形状中不足的部分都通过在前面加 1 补齐。 输出数组的形状是输入数组形状的各个维度上的最大值。 如果输入数组的某个维度和输出数组的对应维度的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错。 当输入数组的某个维度的长度为 1 时，沿着此维度运算时都用此维度上的第一组值。 **简单理解：**对两个数组，分别比较他们的每一个维度（若其中一个数组没有当前维度则忽略），满足： 数组拥有相同形状。 当前维度的值相等。 当前维度的值有一个是 1。 若条件不满足，抛出 “ValueError: frames are not aligned” 异常。 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#线性代数"},{"categories":["python"],"content":" Numpy 进阶 线性代数 import numpy as np # 导入线性代数的库 from numpy import linalg as ll a = np.array([[1.0, 2.0], [3.0, 4.0]]) # array([[1., 2.], # [3., 4.]]) a.transpose() # 转置 a.T # array([[1., 3.], # [2., 4.]]) ll.inv(a) # 矩阵求逆 # array([[-2. , 1. ], # [ 1.5, -0.5]]) u = np.eye(2) # 生成一个单位矩阵 # array([[1., 0.], # [0., 1.]]) j = np.array([[0.0, -1.0], [1.0, 0.0]]) # array([[ 0., -1.], # [ 1., 0.]]) np.dot (j, j) #矩阵乘法运算 # array([[-1., 0.], # [ 0., -1.]]) np.trace(u) # trace # 2.0 y = np.array([[5.], [7.]]) # array([[5.], # [7.]]) ll.solve(a, y) #计算a*y的解 # array([[-3.], # [ 4.]]) ll.eig(j) #向量的特征值和特征向量 # (array([0.+1.j, 0.-1.j]), # array([[0.70710678+0. j, 0.70710678-0. j], # [0. -0.70710678j, 0. +0.70710678j]])) 矩阵类 import numpy as np A = np.matrix('1.0 2.0; 3.0 4.0') ## 生成一个矩阵 # matrix([[1., 2.], # [3., 4.]]) type(A) # numpy.matrixlib.defmatrix.matrix A.T ## transpose # matrix([[1., 3.], # [2., 4.]]) X = np.matrix('5.0 7.0') # matrix([[5., 7.]]) Y = X.transpose() ## X.T # matrix([[5.], # [7.]]) A*Y ## 矩阵乘法，注意维度 # matrix([[19.], # [43.]]) A.I ## inverse # matrix([[-2. , 1. ], # [ 1.5, -0.5]]) ll.solve(A, Y) ## solve A*Y # matrix([[-3.], # [ 4.]]) 注意NumPy中数组和矩阵有些重要的区别。 NumPy提供了两个基本的对象：一个N维数组对象和一个通用函数对象。其它对象都是建构在它们之上 的。特别的，矩阵是继承自NumPy数组对象的二维数组对象。 对数组和矩阵，索引都必须包含合适的一个或多个这些组合：整数标量、省略号 (ellipses)、整数列表;布尔值，整数或布尔值构成的元组，和一个一维整数或布尔值数组。矩阵可以被用作矩阵的索引，但是通常需要数组、列表或者 其它形式来完成这个任务。 像平常在Python中一样，索引是从0开始的。传统上我们用矩形的行和列表示一个二维数组或矩阵，其中沿着0轴的方向被穿过的称作行，沿着1轴的方向被穿过的是列。 矩阵与二维数组的不同 import numpy as np A = np.arange(12) #array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) A.shape = (3,4) # A # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) M = mat(A.copy()) # matrix([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) A[:,1] # array([1, 5, 9]) A[:,1].shape #(3,) M[:,1] # matrix([[1], # [5], # [9]]) (3, 1) 注意最后两个结果的不同。 对二维数组使用一个冒号产生一个一维数组，然而矩阵产生了一个二维矩阵。 广播法则(rule) 广播法则能使通用函数有意义地处理不具有相同形状的输入。 当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播 广播的原则：如果两个数组的后缘维度（trailing dimension，即从末尾开始算起的维度）的轴长度相符，或其中的一方的长度为1，则认为它们是广播兼容的。广播会在缺失和（或）长度为1的维度上进行。 这句话乃是理解广播的核心。广播主要发生在两种情况，一种是两个数组的维数不相等，但是它们的后缘维度的轴长相符，另外一种是有一方的长度为1。 import numpy as np a = np.array([[ 0, 0, 0], [10,10,10], [20,20,20], [30,30,30]]) b = np.array([1,2,3]) print(a + b) ####### # [[ 1 2 3] # [11 12 13] # [21 22 23] # [31 32 33]] 4x3 的二维数组与长为 3 的一维数组相加，等效于把数组 b 在二维上重复 4 次再运算。 广播的规则: 让所有输入数组都向其中形状最长的数组看齐，形状中不足的部分都通过在前面加 1 补齐。 输出数组的形状是输入数组形状的各个维度上的最大值。 如果输入数组的某个维度和输出数组的对应维度的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错。 当输入数组的某个维度的长度为 1 时，沿着此维度运算时都用此维度上的第一组值。 **简单理解：**对两个数组，分别比较他们的每一个维度（若其中一个数组没有当前维度则忽略），满足： 数组拥有相同形状。 当前维度的值相等。 当前维度的值有一个是 1。 若条件不满足，抛出 “ValueError: frames are not aligned” 异常。 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#矩阵类"},{"categories":["python"],"content":" Numpy 进阶 线性代数 import numpy as np # 导入线性代数的库 from numpy import linalg as ll a = np.array([[1.0, 2.0], [3.0, 4.0]]) # array([[1., 2.], # [3., 4.]]) a.transpose() # 转置 a.T # array([[1., 3.], # [2., 4.]]) ll.inv(a) # 矩阵求逆 # array([[-2. , 1. ], # [ 1.5, -0.5]]) u = np.eye(2) # 生成一个单位矩阵 # array([[1., 0.], # [0., 1.]]) j = np.array([[0.0, -1.0], [1.0, 0.0]]) # array([[ 0., -1.], # [ 1., 0.]]) np.dot (j, j) #矩阵乘法运算 # array([[-1., 0.], # [ 0., -1.]]) np.trace(u) # trace # 2.0 y = np.array([[5.], [7.]]) # array([[5.], # [7.]]) ll.solve(a, y) #计算a*y的解 # array([[-3.], # [ 4.]]) ll.eig(j) #向量的特征值和特征向量 # (array([0.+1.j, 0.-1.j]), # array([[0.70710678+0. j, 0.70710678-0. j], # [0. -0.70710678j, 0. +0.70710678j]])) 矩阵类 import numpy as np A = np.matrix('1.0 2.0; 3.0 4.0') ## 生成一个矩阵 # matrix([[1., 2.], # [3., 4.]]) type(A) # numpy.matrixlib.defmatrix.matrix A.T ## transpose # matrix([[1., 3.], # [2., 4.]]) X = np.matrix('5.0 7.0') # matrix([[5., 7.]]) Y = X.transpose() ## X.T # matrix([[5.], # [7.]]) A*Y ## 矩阵乘法，注意维度 # matrix([[19.], # [43.]]) A.I ## inverse # matrix([[-2. , 1. ], # [ 1.5, -0.5]]) ll.solve(A, Y) ## solve A*Y # matrix([[-3.], # [ 4.]]) 注意NumPy中数组和矩阵有些重要的区别。 NumPy提供了两个基本的对象：一个N维数组对象和一个通用函数对象。其它对象都是建构在它们之上 的。特别的，矩阵是继承自NumPy数组对象的二维数组对象。 对数组和矩阵，索引都必须包含合适的一个或多个这些组合：整数标量、省略号 (ellipses)、整数列表;布尔值，整数或布尔值构成的元组，和一个一维整数或布尔值数组。矩阵可以被用作矩阵的索引，但是通常需要数组、列表或者 其它形式来完成这个任务。 像平常在Python中一样，索引是从0开始的。传统上我们用矩形的行和列表示一个二维数组或矩阵，其中沿着0轴的方向被穿过的称作行，沿着1轴的方向被穿过的是列。 矩阵与二维数组的不同 import numpy as np A = np.arange(12) #array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) A.shape = (3,4) # A # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) M = mat(A.copy()) # matrix([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) A[:,1] # array([1, 5, 9]) A[:,1].shape #(3,) M[:,1] # matrix([[1], # [5], # [9]]) (3, 1) 注意最后两个结果的不同。 对二维数组使用一个冒号产生一个一维数组，然而矩阵产生了一个二维矩阵。 广播法则(rule) 广播法则能使通用函数有意义地处理不具有相同形状的输入。 当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播 广播的原则：如果两个数组的后缘维度（trailing dimension，即从末尾开始算起的维度）的轴长度相符，或其中的一方的长度为1，则认为它们是广播兼容的。广播会在缺失和（或）长度为1的维度上进行。 这句话乃是理解广播的核心。广播主要发生在两种情况，一种是两个数组的维数不相等，但是它们的后缘维度的轴长相符，另外一种是有一方的长度为1。 import numpy as np a = np.array([[ 0, 0, 0], [10,10,10], [20,20,20], [30,30,30]]) b = np.array([1,2,3]) print(a + b) ####### # [[ 1 2 3] # [11 12 13] # [21 22 23] # [31 32 33]] 4x3 的二维数组与长为 3 的一维数组相加，等效于把数组 b 在二维上重复 4 次再运算。 广播的规则: 让所有输入数组都向其中形状最长的数组看齐，形状中不足的部分都通过在前面加 1 补齐。 输出数组的形状是输入数组形状的各个维度上的最大值。 如果输入数组的某个维度和输出数组的对应维度的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错。 当输入数组的某个维度的长度为 1 时，沿着此维度运算时都用此维度上的第一组值。 **简单理解：**对两个数组，分别比较他们的每一个维度（若其中一个数组没有当前维度则忽略），满足： 数组拥有相同形状。 当前维度的值相等。 当前维度的值有一个是 1。 若条件不满足，抛出 “ValueError: frames are not aligned” 异常。 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#矩阵与二维数组的不同"},{"categories":["python"],"content":" Numpy 进阶 线性代数 import numpy as np # 导入线性代数的库 from numpy import linalg as ll a = np.array([[1.0, 2.0], [3.0, 4.0]]) # array([[1., 2.], # [3., 4.]]) a.transpose() # 转置 a.T # array([[1., 3.], # [2., 4.]]) ll.inv(a) # 矩阵求逆 # array([[-2. , 1. ], # [ 1.5, -0.5]]) u = np.eye(2) # 生成一个单位矩阵 # array([[1., 0.], # [0., 1.]]) j = np.array([[0.0, -1.0], [1.0, 0.0]]) # array([[ 0., -1.], # [ 1., 0.]]) np.dot (j, j) #矩阵乘法运算 # array([[-1., 0.], # [ 0., -1.]]) np.trace(u) # trace # 2.0 y = np.array([[5.], [7.]]) # array([[5.], # [7.]]) ll.solve(a, y) #计算a*y的解 # array([[-3.], # [ 4.]]) ll.eig(j) #向量的特征值和特征向量 # (array([0.+1.j, 0.-1.j]), # array([[0.70710678+0. j, 0.70710678-0. j], # [0. -0.70710678j, 0. +0.70710678j]])) 矩阵类 import numpy as np A = np.matrix('1.0 2.0; 3.0 4.0') ## 生成一个矩阵 # matrix([[1., 2.], # [3., 4.]]) type(A) # numpy.matrixlib.defmatrix.matrix A.T ## transpose # matrix([[1., 3.], # [2., 4.]]) X = np.matrix('5.0 7.0') # matrix([[5., 7.]]) Y = X.transpose() ## X.T # matrix([[5.], # [7.]]) A*Y ## 矩阵乘法，注意维度 # matrix([[19.], # [43.]]) A.I ## inverse # matrix([[-2. , 1. ], # [ 1.5, -0.5]]) ll.solve(A, Y) ## solve A*Y # matrix([[-3.], # [ 4.]]) 注意NumPy中数组和矩阵有些重要的区别。 NumPy提供了两个基本的对象：一个N维数组对象和一个通用函数对象。其它对象都是建构在它们之上 的。特别的，矩阵是继承自NumPy数组对象的二维数组对象。 对数组和矩阵，索引都必须包含合适的一个或多个这些组合：整数标量、省略号 (ellipses)、整数列表;布尔值，整数或布尔值构成的元组，和一个一维整数或布尔值数组。矩阵可以被用作矩阵的索引，但是通常需要数组、列表或者 其它形式来完成这个任务。 像平常在Python中一样，索引是从0开始的。传统上我们用矩形的行和列表示一个二维数组或矩阵，其中沿着0轴的方向被穿过的称作行，沿着1轴的方向被穿过的是列。 矩阵与二维数组的不同 import numpy as np A = np.arange(12) #array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) A.shape = (3,4) # A # array([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) M = mat(A.copy()) # matrix([[ 0, 1, 2, 3], # [ 4, 5, 6, 7], # [ 8, 9, 10, 11]]) A[:,1] # array([1, 5, 9]) A[:,1].shape #(3,) M[:,1] # matrix([[1], # [5], # [9]]) (3, 1) 注意最后两个结果的不同。 对二维数组使用一个冒号产生一个一维数组，然而矩阵产生了一个二维矩阵。 广播法则(rule) 广播法则能使通用函数有意义地处理不具有相同形状的输入。 当两个数组的形状并不相同的时候，我们可以通过扩展数组的方法来实现相加、相减、相乘等操作，这种机制叫做广播 广播的原则：如果两个数组的后缘维度（trailing dimension，即从末尾开始算起的维度）的轴长度相符，或其中的一方的长度为1，则认为它们是广播兼容的。广播会在缺失和（或）长度为1的维度上进行。 这句话乃是理解广播的核心。广播主要发生在两种情况，一种是两个数组的维数不相等，但是它们的后缘维度的轴长相符，另外一种是有一方的长度为1。 import numpy as np a = np.array([[ 0, 0, 0], [10,10,10], [20,20,20], [30,30,30]]) b = np.array([1,2,3]) print(a + b) ####### # [[ 1 2 3] # [11 12 13] # [21 22 23] # [31 32 33]] 4x3 的二维数组与长为 3 的一维数组相加，等效于把数组 b 在二维上重复 4 次再运算。 广播的规则: 让所有输入数组都向其中形状最长的数组看齐，形状中不足的部分都通过在前面加 1 补齐。 输出数组的形状是输入数组形状的各个维度上的最大值。 如果输入数组的某个维度和输出数组的对应维度的长度相同或者其长度为 1 时，这个数组能够用来计算，否则出错。 当输入数组的某个维度的长度为 1 时，沿着此维度运算时都用此维度上的第一组值。 **简单理解：**对两个数组，分别比较他们的每一个维度（若其中一个数组没有当前维度则忽略），满足： 数组拥有相同形状。 当前维度的值相等。 当前维度的值有一个是 1。 若条件不满足，抛出 “ValueError: frames are not aligned” 异常。 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:2","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#广播法则rule"},{"categories":["python"],"content":" Numpy 陷阱 陷阱一：数据结构混乱 Numpy中的array和matrix结构比较混乱，在计算时如果不注意会出问题。 如上图所示，我们创建一个二维的数组和二维的矩阵。 从 Out[101] 可以看到一个陷阱，a[:, 0] 过滤完应该是一个 3 x 1 的列向量，可是它变成了行向量。其实也不是真正意义上的行向量，因为行向量 shape 应该是 3 x 1，可是他的 shape 是 (3,) ，这其实已经退化为一个数组了。所以，导致最后 In [110] 出错。只有像 In [111] 那样 reshape 一下才可以。 相比之下，matrix 可以确保运算结果全部是二维的，结果相对好一点。 Out [114] 我们预期的输入结果应该是一个 2 x 1 的列向量，可是这里变成了 1 x 2 的行向量！ 在矩阵运算里，行向量和列向量是不同的。比如一个 m x 3 的矩阵可以和 3 x 1 的列向量叉乘，结果是 m x 1 的列向量。而如果一个 m x 3 的矩阵和 1 x 3 的行向量叉乘是会报错的。 陷阱二：数据处理能力不足 假设 X 是 5 x 2 的矩阵，Y 是 5 X 1 的 bool 矩阵，我们想用 Y 来过滤 X ，即取出 Y 值为 True 的项的索引，拿这些索引去 X 里找出对应的行，再组合成一个新矩阵。 我们预期 X 过滤完是 3 x 2 列的矩阵，但不幸的是从 Out[81] 来看 numpy 这样过滤完只会保留第一列的数据，且把它转化成了行向量，即变成了 1 x 3 的行向量。不知道你有没有抓狂的感觉。如果按照 In [85] 的写法，还会报错。如果要正确地过滤不同的列，需要写成 In [86] 和 In [87] 的形式。但是即使写成 In [86] 和 In [87] 的样式，还是一样把列向量转化成了行向量。所以，要实现这个目的，得复杂到按照 In [88] 那样才能达到目的。实际上，这个还达不到目的，因为那里面写了好多硬编码的数字，要处理通用的过滤情况，还需要写个函数来实现。 陷阱三：数值运算句法混乱 在机器学习算法里，经常要做一些矩阵运算。有时候要做叉乘，有时候要做点乘。我们看一下 numpy 是如何满足这个需求的。 假设 x, y, theta 的值如下，我们要先让 x 和 y 点乘，再让结果与 theta 叉乘，最后的结果我们期望的是一个 5 x 1 的列向量。 直观地讲，我们应该会想这样做：(x 点乘 y) 叉乘 theta。但很不幸，当你输入 x * y 时妥妥地报错。那好吧，我们这样做总行了吧，x[:, 0] * y 这样两个列向量就可以点乘了吧，不幸的还是不行，因为 numpy 认为这是 matrix，所以执行的是矩阵相乘（叉乘），要做点乘，必须转为 array 。 所以，我们需要象 In [39] 那样一列列转为 array 和 y 执行点乘，然后再组合回 5 x 3 的矩阵。好不容易算出了 x 和 y 的点乘了，终于可以和 theta 叉乘了。 看起来结果还不错，但实际上这里面也是陷阱重重。 In [45] 会报错，因为在 array 里 * 运算符是点乘，而在 matrix 里 * 运算符是叉乘。如果要在 array 里算叉乘，需要用 dot 方法。看起来提供了灵活性，实际上增加了使用者的大脑负担。 如果想要避免array和matrix的点乘和叉乘，最好使用numpy提供的方法np.multiply（点乘）和np.dot（叉乘）；而不是使用默认的***** 参考链接 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:3","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#numpy-陷阱"},{"categories":["python"],"content":" Numpy 陷阱 陷阱一：数据结构混乱 Numpy中的array和matrix结构比较混乱，在计算时如果不注意会出问题。 如上图所示，我们创建一个二维的数组和二维的矩阵。 从 Out[101] 可以看到一个陷阱，a[:, 0] 过滤完应该是一个 3 x 1 的列向量，可是它变成了行向量。其实也不是真正意义上的行向量，因为行向量 shape 应该是 3 x 1，可是他的 shape 是 (3,) ，这其实已经退化为一个数组了。所以，导致最后 In [110] 出错。只有像 In [111] 那样 reshape 一下才可以。 相比之下，matrix 可以确保运算结果全部是二维的，结果相对好一点。 Out [114] 我们预期的输入结果应该是一个 2 x 1 的列向量，可是这里变成了 1 x 2 的行向量！ 在矩阵运算里，行向量和列向量是不同的。比如一个 m x 3 的矩阵可以和 3 x 1 的列向量叉乘，结果是 m x 1 的列向量。而如果一个 m x 3 的矩阵和 1 x 3 的行向量叉乘是会报错的。 陷阱二：数据处理能力不足 假设 X 是 5 x 2 的矩阵，Y 是 5 X 1 的 bool 矩阵，我们想用 Y 来过滤 X ，即取出 Y 值为 True 的项的索引，拿这些索引去 X 里找出对应的行，再组合成一个新矩阵。 我们预期 X 过滤完是 3 x 2 列的矩阵，但不幸的是从 Out[81] 来看 numpy 这样过滤完只会保留第一列的数据，且把它转化成了行向量，即变成了 1 x 3 的行向量。不知道你有没有抓狂的感觉。如果按照 In [85] 的写法，还会报错。如果要正确地过滤不同的列，需要写成 In [86] 和 In [87] 的形式。但是即使写成 In [86] 和 In [87] 的样式，还是一样把列向量转化成了行向量。所以，要实现这个目的，得复杂到按照 In [88] 那样才能达到目的。实际上，这个还达不到目的，因为那里面写了好多硬编码的数字，要处理通用的过滤情况，还需要写个函数来实现。 陷阱三：数值运算句法混乱 在机器学习算法里，经常要做一些矩阵运算。有时候要做叉乘，有时候要做点乘。我们看一下 numpy 是如何满足这个需求的。 假设 x, y, theta 的值如下，我们要先让 x 和 y 点乘，再让结果与 theta 叉乘，最后的结果我们期望的是一个 5 x 1 的列向量。 直观地讲，我们应该会想这样做：(x 点乘 y) 叉乘 theta。但很不幸，当你输入 x * y 时妥妥地报错。那好吧，我们这样做总行了吧，x[:, 0] * y 这样两个列向量就可以点乘了吧，不幸的还是不行，因为 numpy 认为这是 matrix，所以执行的是矩阵相乘（叉乘），要做点乘，必须转为 array 。 所以，我们需要象 In [39] 那样一列列转为 array 和 y 执行点乘，然后再组合回 5 x 3 的矩阵。好不容易算出了 x 和 y 的点乘了，终于可以和 theta 叉乘了。 看起来结果还不错，但实际上这里面也是陷阱重重。 In [45] 会报错，因为在 array 里 * 运算符是点乘，而在 matrix 里 * 运算符是叉乘。如果要在 array 里算叉乘，需要用 dot 方法。看起来提供了灵活性，实际上增加了使用者的大脑负担。 如果想要避免array和matrix的点乘和叉乘，最好使用numpy提供的方法np.multiply（点乘）和np.dot（叉乘）；而不是使用默认的***** 参考链接 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:3","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#陷阱一数据结构混乱"},{"categories":["python"],"content":" Numpy 陷阱 陷阱一：数据结构混乱 Numpy中的array和matrix结构比较混乱，在计算时如果不注意会出问题。 如上图所示，我们创建一个二维的数组和二维的矩阵。 从 Out[101] 可以看到一个陷阱，a[:, 0] 过滤完应该是一个 3 x 1 的列向量，可是它变成了行向量。其实也不是真正意义上的行向量，因为行向量 shape 应该是 3 x 1，可是他的 shape 是 (3,) ，这其实已经退化为一个数组了。所以，导致最后 In [110] 出错。只有像 In [111] 那样 reshape 一下才可以。 相比之下，matrix 可以确保运算结果全部是二维的，结果相对好一点。 Out [114] 我们预期的输入结果应该是一个 2 x 1 的列向量，可是这里变成了 1 x 2 的行向量！ 在矩阵运算里，行向量和列向量是不同的。比如一个 m x 3 的矩阵可以和 3 x 1 的列向量叉乘，结果是 m x 1 的列向量。而如果一个 m x 3 的矩阵和 1 x 3 的行向量叉乘是会报错的。 陷阱二：数据处理能力不足 假设 X 是 5 x 2 的矩阵，Y 是 5 X 1 的 bool 矩阵，我们想用 Y 来过滤 X ，即取出 Y 值为 True 的项的索引，拿这些索引去 X 里找出对应的行，再组合成一个新矩阵。 我们预期 X 过滤完是 3 x 2 列的矩阵，但不幸的是从 Out[81] 来看 numpy 这样过滤完只会保留第一列的数据，且把它转化成了行向量，即变成了 1 x 3 的行向量。不知道你有没有抓狂的感觉。如果按照 In [85] 的写法，还会报错。如果要正确地过滤不同的列，需要写成 In [86] 和 In [87] 的形式。但是即使写成 In [86] 和 In [87] 的样式，还是一样把列向量转化成了行向量。所以，要实现这个目的，得复杂到按照 In [88] 那样才能达到目的。实际上，这个还达不到目的，因为那里面写了好多硬编码的数字，要处理通用的过滤情况，还需要写个函数来实现。 陷阱三：数值运算句法混乱 在机器学习算法里，经常要做一些矩阵运算。有时候要做叉乘，有时候要做点乘。我们看一下 numpy 是如何满足这个需求的。 假设 x, y, theta 的值如下，我们要先让 x 和 y 点乘，再让结果与 theta 叉乘，最后的结果我们期望的是一个 5 x 1 的列向量。 直观地讲，我们应该会想这样做：(x 点乘 y) 叉乘 theta。但很不幸，当你输入 x * y 时妥妥地报错。那好吧，我们这样做总行了吧，x[:, 0] * y 这样两个列向量就可以点乘了吧，不幸的还是不行，因为 numpy 认为这是 matrix，所以执行的是矩阵相乘（叉乘），要做点乘，必须转为 array 。 所以，我们需要象 In [39] 那样一列列转为 array 和 y 执行点乘，然后再组合回 5 x 3 的矩阵。好不容易算出了 x 和 y 的点乘了，终于可以和 theta 叉乘了。 看起来结果还不错，但实际上这里面也是陷阱重重。 In [45] 会报错，因为在 array 里 * 运算符是点乘，而在 matrix 里 * 运算符是叉乘。如果要在 array 里算叉乘，需要用 dot 方法。看起来提供了灵活性，实际上增加了使用者的大脑负担。 如果想要避免array和matrix的点乘和叉乘，最好使用numpy提供的方法np.multiply（点乘）和np.dot（叉乘）；而不是使用默认的***** 参考链接 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:3","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#陷阱二数据处理能力不足"},{"categories":["python"],"content":" Numpy 陷阱 陷阱一：数据结构混乱 Numpy中的array和matrix结构比较混乱，在计算时如果不注意会出问题。 如上图所示，我们创建一个二维的数组和二维的矩阵。 从 Out[101] 可以看到一个陷阱，a[:, 0] 过滤完应该是一个 3 x 1 的列向量，可是它变成了行向量。其实也不是真正意义上的行向量，因为行向量 shape 应该是 3 x 1，可是他的 shape 是 (3,) ，这其实已经退化为一个数组了。所以，导致最后 In [110] 出错。只有像 In [111] 那样 reshape 一下才可以。 相比之下，matrix 可以确保运算结果全部是二维的，结果相对好一点。 Out [114] 我们预期的输入结果应该是一个 2 x 1 的列向量，可是这里变成了 1 x 2 的行向量！ 在矩阵运算里，行向量和列向量是不同的。比如一个 m x 3 的矩阵可以和 3 x 1 的列向量叉乘，结果是 m x 1 的列向量。而如果一个 m x 3 的矩阵和 1 x 3 的行向量叉乘是会报错的。 陷阱二：数据处理能力不足 假设 X 是 5 x 2 的矩阵，Y 是 5 X 1 的 bool 矩阵，我们想用 Y 来过滤 X ，即取出 Y 值为 True 的项的索引，拿这些索引去 X 里找出对应的行，再组合成一个新矩阵。 我们预期 X 过滤完是 3 x 2 列的矩阵，但不幸的是从 Out[81] 来看 numpy 这样过滤完只会保留第一列的数据，且把它转化成了行向量，即变成了 1 x 3 的行向量。不知道你有没有抓狂的感觉。如果按照 In [85] 的写法，还会报错。如果要正确地过滤不同的列，需要写成 In [86] 和 In [87] 的形式。但是即使写成 In [86] 和 In [87] 的样式，还是一样把列向量转化成了行向量。所以，要实现这个目的，得复杂到按照 In [88] 那样才能达到目的。实际上，这个还达不到目的，因为那里面写了好多硬编码的数字，要处理通用的过滤情况，还需要写个函数来实现。 陷阱三：数值运算句法混乱 在机器学习算法里，经常要做一些矩阵运算。有时候要做叉乘，有时候要做点乘。我们看一下 numpy 是如何满足这个需求的。 假设 x, y, theta 的值如下，我们要先让 x 和 y 点乘，再让结果与 theta 叉乘，最后的结果我们期望的是一个 5 x 1 的列向量。 直观地讲，我们应该会想这样做：(x 点乘 y) 叉乘 theta。但很不幸，当你输入 x * y 时妥妥地报错。那好吧，我们这样做总行了吧，x[:, 0] * y 这样两个列向量就可以点乘了吧，不幸的还是不行，因为 numpy 认为这是 matrix，所以执行的是矩阵相乘（叉乘），要做点乘，必须转为 array 。 所以，我们需要象 In [39] 那样一列列转为 array 和 y 执行点乘，然后再组合回 5 x 3 的矩阵。好不容易算出了 x 和 y 的点乘了，终于可以和 theta 叉乘了。 看起来结果还不错，但实际上这里面也是陷阱重重。 In [45] 会报错，因为在 array 里 * 运算符是点乘，而在 matrix 里 * 运算符是叉乘。如果要在 array 里算叉乘，需要用 dot 方法。看起来提供了灵活性，实际上增加了使用者的大脑负担。 如果想要避免array和matrix的点乘和叉乘，最好使用numpy提供的方法np.multiply（点乘）和np.dot（叉乘）；而不是使用默认的***** 参考链接 ","date":"2018-12-25","objectID":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:0:3","series":null,"tags":["python","Numpy"],"title":"python之Numpy学习笔记","uri":"/python%E4%B9%8Bnumpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/#陷阱三数值运算句法混乱"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#1变量与类型"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#2运算符"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#3流程控制"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#4函数与模块"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#5容器"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#51-list容器"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#52-tuple"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#53-dict"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#54-set"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#6常用的库"},{"categories":["python"],"content":"做一个笔记，记录一下python的入门代码 目录内容如下： 变量与类型 运算符 流程控制 函数与模块 容器 常用的库 1.变量与类型 Python 中的变量赋值不需要类型声明。每个变量在内存中创建，都包括变量的标识，名称和数据这些信息。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 Python有五个标准的数据类型： Numbers（数字） int（有符号整型） long（长整型[也可以代表八进制和十六进制]）:python3已经移除 float（浮点型） complex（复数） String（字符串） Python不支持单字符类型，单字符在 Python 中也是作为一个字符串使用。 python三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。 List（列表） Set (集合) Tuple（元组） Dictionary（字典） # 变量例子 # 数值类型和字符串类型 counter = 100 # 赋值整型变量 miles = 1000.0 # 浮点型 name = \"John\" # 字符串 # 多变量同时赋值，可以使用这种方法来进行两个数的交换 a, b, c = 1, 2, \"john\" # 下面是list，tuple和dict的例子 l = list() # l = [] t = tuple() # t = () d = dict() # d = {} # 类型的转换 # int(x [,base ]) 将x转换为一个整数 # long(x [,base ]) 将x转换为一个长整数 # float(x ) 将x转换到一个浮点数 # complex(real [,imag ]) 创建一个复数 # str(x ) 将对象 x 转换为字符串 # repr(x ) 将对象 x 转换为表达式字符串 # eval(str ) 用来计算在字符串中的有效Python表达式,并返回一个对象 # tuple(s ) 将序列 s 转换为一个元组 # list(s ) 将序列 s 转换为一个列表 # chr(x ) 将一个整数转换为一个字符 # unichr(x ) 将一个整数转换为Unicode字符 # ord(x ) 将一个字符转换为它的整数值 # hex(x ) 将一个整数转换为一个十六进制字符串 # oct(x ) 将一个整数转换为一个八进制字符串 # # 字符串 # a = \"Hello\";b = \"Python\" # a + b # 'HelloPython' # a * 2 # 'HelloHello' # a[1] # 'e' # a[1:4] # 'ell' # \"H\" in a # True # \"M\" not in a # True 2.运算符 和其他语言一样，python也支持以下类型的运算符: 算术运算符 比较（关系）运算符 赋值运算符 逻辑运算符 位运算符 成员运算符 身份运算符 运算符优先级 ## 算术运算符的例子 # a=10;b = 20 # a + b # 30 # a - b # -10 # a * b # 200 # b / a # 2 # # 取模 # b % a # 0 # # 乘方 # a**b # 100000000000000000000 ## 比较运算符 # (a == b) # False。 # (a != b) # true. # (a \u003e b) # False。 # (a \u003c b) # true。 # (a \u003e= b) # False。 # (a \u003c= b) # true。 # # 赋值运算符 # c = a + b # c # c += a # c = c + a # c -= a # c = c - a # c *= a # c = c * a # c /= a # c = c / a # c %= a # c = c % a # c **= a # c = c ** a # c //= a # c = c // a # # 位运算符 # a = 60;b = 13 # (a \u0026 b) # 12 ，二进制解释： 0000 1100 # (a | b) # 61 ，二进制解释： 0011 1101 # (a ^ b) # 49 ，二进制解释： 0011 0001 # (~a ) # -61 ，二进制解释： 1100 0011，在一个有符号二进制数的补码形式。 # a \u003c\u003c 2 # 240 ，二进制解释： 1111 0000 # a \u003e\u003e 2 # 15 ，二进制解释： 0000 1111 # # 逻辑运算符,与或非 # a=20;b=10 # (a and b) # 20。 # (a or b) # 10。 # not(a and b) # False # # 成员运算符,主要用于容器查询中 # # in x 在 y 序列中返回 True。 # # not in x 不在 y 序列中 , 如果 x 不在 y 序列中返回 True。 # # is x is y, 类似 id(x) == id(y) , 如果引用的是同一个对象则返回 True，否则返回 False # # is not x is not y ， 类似 id(a) != id(b)。如果引用的不是同一个对象则返回结果 True，否则返回 False。 # a = 20;b = 20 # a is b # True # b = 30 # a is b # False 3.流程控制 流程控制：条件判断+循环语句 PS:没有switch 条件判断 第一种形式： ​```python\rif 判断条件： ​ 执行语句…… ​ else： ​ 执行语句…… ​ ​``` 第二种形式： if 判断条件1: 执行语句1…… elif 判断条件2: 执行语句2…… elif 判断条件3: 执行语句3…… else: 执行语句4…… 循环语句 while 循环 while 判断条件： 执行语句…… for 循环: for iterating_var in sequence: statements(s) 循环条件控制 break 语句 在语句块执行过程中终止循环，并且跳出整个循环 2. continue 语句在语句块执行过程中终止当前循环，跳出该次循环，执行下一次循环。 3. pass 语句pass是空语句，是为了保持程序结构的完整性。 4.函数与模块 函数定义的规则函数 函数代码块以 def关键词开头，后接函数标识符名称和圆括号()。 任何传入参数和自变量必须放在圆括号中间。圆括号之间可以用于定义参数。 函数的第一行语句可以选择性地使用文档字符串—用于存放函数说明。 函数内容以冒号起始，并且缩进。 return表达式结束函数，选择性地返回一个值给调用方。不带表达式的return相当于返回None。 def functionname( parameters ): \"函数_文档字符串\" function_suite return [expression] 参数传递 调用函数时，默认参数的值如果没有传入，则被认为是默认值.注意：默认参数一定要放在后面 不定长参数:加了星号（*）的变量名会存放所有未命名的变量参数 def functionname([formal_args,] *var_args_tuple ): \"函数_文档字符串\" function_suite return [expression] 匿名函数: lambda [arg1 [,arg2,.....argn]]:expression 变量作用域 定义在函数内部的变量拥有一个局部作用域，定义在函数外的拥有全局作用域。 局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。 global关键字能在函数内使用全局变量，而不用先去初始化 模块 当你导入一个模块，Python 解析器对模块位置的搜索顺序是： 1、当前目录 2、如果不在当前目录，Python 则搜索在 shell 变量 PYTHONPATH 下的每个目录。 3、如果都找不到，Python会察看默认路径。UNIX下，默认路径一般为/usr/local/lib/python/。 模块搜索路径存储在 system 模块的 sys.path 变量中。变量里包含当前目录，PYTHONPATH和由安装过程决定的默认目录。 5.容器 在python中，提供了四种容器，分别是list，tuple，set，dict 5.1 list容器 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。 列表的数据项不需要具有相同的类型 list2 = [1, 2, 3, 4, 5 ] list3 = [\"a\", \"b\", \"c\", \"d\"]``` list的常用方法： append:尾部添加 inser:插入 del: 删除 pop: 删除最后一个 remove;删除指定元素 PS:非常重要的是list可以切片索引 5.2 tuple 元组与列表类似，不同之处在于元组的元素不能修改。换句话说：元组可以添加元素，但是不能修改和删除里面的元素，其他操作和list基本。 PS:元组中只包含一个元素时，需要在元素后面添加逗号 tup1 = (50,) 5.3 dict 字典是另一种可变容器模型，且可存储任意类型对象。 字典的每个键值 key=\u003evalue 对用冒号 : 分割，每个键值对之间用逗号 , 分割 键一般是唯一的，如果重复最后的一个键值对会","date":"2018-12-06","objectID":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/:0:0","series":null,"tags":["python"],"title":"python 入门代码","uri":"/python%E5%85%A5%E9%97%A8%E4%BB%A3%E7%A0%81/#python中的对象"},{"categories":["hexo"],"content":" 1.找到github上面的博客仓库 xxx.github.io 这个仓库的名字 2.在这个仓库上，新建一个分支，名字随便起，比如，我们起名为hexo 3.然后进入到设置里面，将hexo分支设为默认分支 4.在本地新建一个文件夹，然后将xxx.github.io这个仓库clone下来 git clone git@github.com:xxx/xxxx.github.io.git 5.进入这个clone下来的仓库，此时这个仓库里面什么都没有，我们将原来的hexo项目里面的所有文件，复制到这个仓库里面，然后push到远程仓库上，在push之前，进入themes文件夹下面，删除主题的.git文件夹 git add . git commit -m \"xxxx\" git push 6.此时可以去远程仓库查看，上面已经全是hexo项目的源码了 7.在另一台电脑上面，新建一个文件夹，然后clone这个仓库 git clone git@github.com:xxx/xxxx.github.io.git 8.进入仓库，就能看到在其他电脑上创建的hexo项目了，然后就可以在这台电脑写博客，写完，git提交。 写博客并部署 hexo new \"xxxx\" hexo g hexo d 将代码上传到git上 git add . git commit -m \"xx\" git push PS：记住每次写博客之前，先git pull拉取仓库，然后在写博客。 ","date":"2018-12-05","objectID":"/%E5%A4%9A%E5%8F%B0%E7%94%B5%E8%84%91%E5%90%8C%E6%97%B6%E5%86%99hexo%E5%8D%9A%E5%AE%A2/:0:0","series":null,"tags":["hexo"],"title":"多台电脑同时写hexo博客","uri":"/%E5%A4%9A%E5%8F%B0%E7%94%B5%E8%84%91%E5%90%8C%E6%97%B6%E5%86%99hexo%E5%8D%9A%E5%AE%A2/#"},{"categories":["C++"],"content":" 前言 在学习C++以及其他编程语言的时候，经常会碰到一个问题，那就是变量定义、声明、初始化与赋值都有什么样的区别呢？ 变量的定义、声明、初始化和赋值这四个概念在C++中是很容易区分，所以，从C++入手来学习并且这四个概念，在其他的编程语言里面，也不会迷惑了。 ","date":"2018-12-04","objectID":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/:0:1","series":null,"tags":["C++"],"title":"C++之定义声明初始化与赋值","uri":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/#center-前言"},{"categories":["C++"],"content":" 1.变量的定义与初始化 变量的定义：顾名思义，变量的定义就是指在使用这个变量之前，先对它进行定义并申请存储空间，在变量定义的时候，还可以对它进行初始化。 PS：变量只能定义一次，也只能初始化一次。 比如说看下面的代码： int sum; // 这一行代码就是对sum这个变量的定义，此时sum并没有被赋值和初始化，但是已经被分配存储空间，sum可能会根据不同的编译器来决定是否被默认的去初始化，有可能sum里面的数是垃圾数字。 int count = 0; // 这一行代码是对count变量的定义以及初始化为0; ","date":"2018-12-04","objectID":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/:0:2","series":null,"tags":["C++"],"title":"C++之定义声明初始化与赋值","uri":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/#1变量的定义与初始化"},{"categories":["C++"],"content":" 2.变量的声明 变量的声明：变量的声明仅仅只是规定了变量的类型和名字，并不分配存储空间。 PS:一般我们也可以把定义当做声明，变量可以被多次声明，但是只能被定义一次。 在C++中的声明以及定义： extern int i; // 使用关键字extern，我们就可以声明一个变量了，注意此处的i一定会在其他地方被定义，此处这是声明变量i int j; // 声明并且定义了变量j ","date":"2018-12-04","objectID":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/:0:3","series":null,"tags":["C++"],"title":"C++之定义声明初始化与赋值","uri":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/#2变量的声明"},{"categories":["C++"],"content":" 3.变量的赋值与初始化 在前面说到，初始化只能是在定义的时候进行，并且变量也只能初始化一次，而赋值可以进行多次。 int i = 0; // 定义声明变量i，并且将i初始化为0 i = 9; // i赋值为9 i = 6; // i赋值为6 ","date":"2018-12-04","objectID":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/:0:4","series":null,"tags":["C++"],"title":"C++之定义声明初始化与赋值","uri":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/#3变量的赋值与初始化"},{"categories":["C++"],"content":" 在java语言中的定义、声明、初始化和赋值 在java中，变量只有两种，一种是基本类型，一种是对象。基本类型变量的声明和定义（初始化）是同时产生的；而对于对象来说，声明和定义是分开的。 在java中，变量的类型和各种修饰符的搭配有很多，因此，变量的初始化的过程也很复杂，在这里，只是记录一下，基本类型和其他引用类型在类成员和成员函数里面的时候的默认初始化规则。 总结一句话：如果基本类型是在成员函数里面，那么，一定要初始化，否则会报错。其他情况，编译器会自动根据类型来初始化。 /************************************************************************* \u003e File Name: demo.java \u003e Author: \u003e Mail: \u003e Created Time: 二 12/ 4 13:59:23 2018 ************************************************************************/ public class demo { public static void main(String[] args) { test t = new test(); t.test(); } } class test { int i; int j = 9; public void test() { int k; int l = 7; System.out.println(i); System.out.println(j); System.out.println(k); System.out.println(l); } } // 程序编译错误，原因在于k没有去初始化。 // i变量默认为0 ","date":"2018-12-04","objectID":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/:0:5","series":null,"tags":["C++"],"title":"C++之定义声明初始化与赋值","uri":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/#在java语言中的定义声明初始化和赋值"},{"categories":["C++"],"content":" 在Python中的定义、声明、初始化和赋值 Python是解释型的动态语言，因此，在使用变量的时候，可以不用声明和定义，如果想要使用一个变量，直接去初始化一个变量或者对某个变量赋值。 i = 9 //定义初始化一个变量i j = i ** 2 // 定义初始化变量j ","date":"2018-12-04","objectID":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/:0:6","series":null,"tags":["C++"],"title":"C++之定义声明初始化与赋值","uri":"/c-%E4%B9%8B%E5%AE%9A%E4%B9%89%E5%A3%B0%E6%98%8E%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%8E%E8%B5%8B%E5%80%BC/#在python中的定义声明初始化和赋值"},{"categories":["深度学习"],"content":"**前言**\r多层感知器算法，网上有各种解析，以后有时间自己在来慢慢补充。 ","date":"2017-12-05","objectID":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-mlp/:0:0","series":null,"tags":["mxnet","深度学习"],"title":"mxnet学习之多层感知机(MLP)","uri":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-mlp/#"},{"categories":["深度学习"],"content":" 第二节课 mxnet之多层感知机直接上代码： # codin=utf-8 ''' 从零开始的多层感知机算法实现 ''' import mxnet as mx from mxnet import gluon from mxnet import autograd from mxnet import ndarray as nd def getModel(): net = gluon.nn.Sequential() with net.name_scope(): net.add(gluon.nn.Flatten()) net.add(gluon.nn.Dense(256, activation=\"relu\")) net.add(gluon.nn.Dense(10)) net.initialize() return net # 数据格式转换 def transform(data, label): return data.astype('float32') / 255, label.astype('float32') / 255 # 加载数据 def readData(): print(\"load data...\") mnsit_train = gluon.data.vision.FashionMNIST(root=\"~/.mxnet/datasets/fashion-mnist\", train=True, transform=transform) mnist_test = gluon.data.vision.FashionMNIST(root=\"~/.mxnet/datasets/fashion-mnist\", train=False , transform=transform) return mnsit_train, mnist_test # 画出图像 def drawPlot(images): import matplotlib.pyplot as plt n = images.shape[0] _, figs = plt.subplots(1, n, figsize=(15, 15)) for i in range(n): figs[i].imshow(images[i].reshape((28, 28)).asnumpy()) figs[i].axes.get_xaxis().set_visible(False) figs[i].axes.get_yaxis().set_visible(False) plt.show() def trainData(net, train, test, batch_size=128, epoches=5): train_data = gluon.data.DataLoader( train, batch_size=batch_size, shuffle=True) softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss() trainer = gluon.Trainer(net.collect_params(), 'sgd', { \"learning_rate\": 0.5}) for epoech in range(epoches): train_loss = 0 train_acc = 0 test_acc = 0 for data, label in train_data: with autograd.record(): output = net(data) loss = softmax_cross_entropy(output, label) loss.backward() trainer.step(batch_size) train_loss += nd.mean(loss).asscalar() print(\"Epoech is {0},train_loss is {1}.\".format( epoech + 1, train_loss / len(train_data))) # 准确率计算 def accuarcy(output, label): return nd.mean(output.argmax(axis=1) == label).asscalar() # 评价函数 def evaluateData(net, test, batch_size=128): acc = 0 test_data = gluon.data.DataLoader( test, batch_size=batch_size, shuffle=False) for data, label in test_data: output = net(data) acc += accuarcy(output.astype(\"float32\"), label.astype(\"float32\")) acc = acc / len(test_data) print(\"test acc is {0}\".format(acc)) train, test = readData() net = getModel() batch_size = 256 epoches = 5 trainData(net, train, test, batch_size, epoches) evaluateData(net, test) 最后来个结果：多分类，这个结果真是惨不忍睹 ","date":"2017-12-05","objectID":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-mlp/:0:1","series":null,"tags":["mxnet","深度学习"],"title":"mxnet学习之多层感知机(MLP)","uri":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-mlp/#第二节课-mxnet之多层感知机"},{"categories":["图像处理"],"content":"前言\r在图像处理和计算机视觉领域，opencv是个好工具，这学期刚学完图像处理，正好，用opencv把常用的算法调用一下，学习一下图像处理。 安装软件1.首先说明，电脑是win8.1，32位系统 2.安装方法： pip install opencv-python 如果报以下错误： ImportError: DLL load failed: 找不到指定的程序。 那么，你有以下几个方法来解决： 去微软官网下载vsredist.exe,下载你电脑对应的版本号。 去python库下载whl 运行例子 # coding=utf-8 import cv2 import numpy as np def main(): x = np.ones((512, 512), np.float) cv2.imshow(\"Demo\", x) cv2.waitKey(0) cv2.destroyAllWindows() if __name__ == '__main__': main() 可以看到一个白色的窗口出现。 ","date":"2017-12-04","objectID":"/opencv-python%E5%AE%89%E8%A3%85/:0:0","series":null,"tags":["python","opencv"],"title":"opencv-python安装","uri":"/opencv-python%E5%AE%89%E8%A3%85/#"},{"categories":["图像处理"],"content":"前言\r在图像处理和计算机视觉领域，opencv是个好工具，这学期刚学完图像处理，正好，用opencv把常用的算法调用一下，学习一下图像处理。 安装软件1.首先说明，电脑是win8.1，32位系统 2.安装方法： pip install opencv-python 如果报以下错误： ImportError: DLL load failed: 找不到指定的程序。 那么，你有以下几个方法来解决： 去微软官网下载vsredist.exe,下载你电脑对应的版本号。 去python库下载whl 运行例子 # coding=utf-8 import cv2 import numpy as np def main(): x = np.ones((512, 512), np.float) cv2.imshow(\"Demo\", x) cv2.waitKey(0) cv2.destroyAllWindows() if __name__ == '__main__': main() 可以看到一个白色的窗口出现。 ","date":"2017-12-04","objectID":"/opencv-python%E5%AE%89%E8%A3%85/:0:0","series":null,"tags":["python","opencv"],"title":"opencv-python安装","uri":"/opencv-python%E5%AE%89%E8%A3%85/#安装软件"},{"categories":["图像处理"],"content":"前言\r在图像处理和计算机视觉领域，opencv是个好工具，这学期刚学完图像处理，正好，用opencv把常用的算法调用一下，学习一下图像处理。 安装软件1.首先说明，电脑是win8.1，32位系统 2.安装方法： pip install opencv-python 如果报以下错误： ImportError: DLL load failed: 找不到指定的程序。 那么，你有以下几个方法来解决： 去微软官网下载vsredist.exe,下载你电脑对应的版本号。 去python库下载whl 运行例子 # coding=utf-8 import cv2 import numpy as np def main(): x = np.ones((512, 512), np.float) cv2.imshow(\"Demo\", x) cv2.waitKey(0) cv2.destroyAllWindows() if __name__ == '__main__': main() 可以看到一个白色的窗口出现。 ","date":"2017-12-04","objectID":"/opencv-python%E5%AE%89%E8%A3%85/:0:0","series":null,"tags":["python","opencv"],"title":"opencv-python安装","uri":"/opencv-python%E5%AE%89%E8%A3%85/#运行例子"},{"categories":["hexo"],"content":" 第一种方法参考自这篇文章 以下是通用的代码框架：前提是网站必须支持iframe \u003ciframe height=498 width=510 src=\"这里嵌入视频的来源，优酷，爱奇艺等\" frameborder=0 allowfullscreen\u003e\u003c/iframe\u003e 这是个视频标题： 第二种方法使用H5的video代码插入： \u003cvideo id=\"video\" controls=\"\" preload=\"none\" poster=\"http://media.w3.org/2010/05/sintel/poster.png\"\u003e \u003csource id=\"mp4\" src=\"http://media.w3.org/2010/05/sintel/trailer.mp4\" type=\"video/mp4\"\u003e \u003csource id=\"webm\" src=\"http://media.w3.org/2010/05/sintel/trailer.webm\" type=\"video/webm\"\u003e \u003csource id=\"ogv\" src=\"http://media.w3.org/2010/05/sintel/trailer.ogv\" type=\"video/ogg\"\u003e \u003cp\u003eYour user agent does not support the HTML5 Video element.\u003c/p\u003e \u003c/video\u003e Your user agent does not support the HTML5 Video element. ","date":"2017-11-27","objectID":"/%E5%A6%82%E4%BD%95%E4%B8%BAmarkdown%E6%B7%BB%E5%8A%A0%E8%A7%86%E9%A2%91/:0:0","series":null,"tags":["hexo","markdown"],"title":"如何为markdown添加视频","uri":"/%E5%A6%82%E4%BD%95%E4%B8%BAmarkdown%E6%B7%BB%E5%8A%A0%E8%A7%86%E9%A2%91/#"},{"categories":["hexo"],"content":" 第一种方法参考自这篇文章 以下是通用的代码框架：前提是网站必须支持iframe 这是个视频标题： 第二种方法使用H5的video代码插入： Your user agent does not support the HTML5 Video element. Your user agent does not support the HTML5 Video element. ","date":"2017-11-27","objectID":"/%E5%A6%82%E4%BD%95%E4%B8%BAmarkdown%E6%B7%BB%E5%8A%A0%E8%A7%86%E9%A2%91/:0:0","series":null,"tags":["hexo","markdown"],"title":"如何为markdown添加视频","uri":"/%E5%A6%82%E4%BD%95%E4%B8%BAmarkdown%E6%B7%BB%E5%8A%A0%E8%A7%86%E9%A2%91/#第一种方法"},{"categories":["hexo"],"content":" 第一种方法参考自这篇文章 以下是通用的代码框架：前提是网站必须支持iframe 这是个视频标题： 第二种方法使用H5的video代码插入： Your user agent does not support the HTML5 Video element. Your user agent does not support the HTML5 Video element. ","date":"2017-11-27","objectID":"/%E5%A6%82%E4%BD%95%E4%B8%BAmarkdown%E6%B7%BB%E5%8A%A0%E8%A7%86%E9%A2%91/:0:0","series":null,"tags":["hexo","markdown"],"title":"如何为markdown添加视频","uri":"/%E5%A6%82%E4%BD%95%E4%B8%BAmarkdown%E6%B7%BB%E5%8A%A0%E8%A7%86%E9%A2%91/#第二种方法"},{"categories":["深度学习"],"content":"**前言**\r深度学习模型太多了，tensorflow、theano、keras、mxnet等等，听说mxnet开发者们在斗鱼直播教学，所以趁机学习一波。 ","date":"2017-11-21","objectID":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:0","series":null,"tags":["mxnet","深度学习"],"title":"mxnet学习之线性回归","uri":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#"},{"categories":["深度学习"],"content":" 第一节课 从零开始之线性回归 先从零开始，搭一个线性回归 # coding=utf-8 from mxnet import gluon from mxnet import ndarray as nd from mxnet import autograd import matplotlib.pyplot as plt import random def data_iter(X, y, num_examples, batch_size): # 产生索引 idx = list(range(num_examples)) random.shuffle(idx) for i in range(0, num_examples, batch_size): j = nd.array(idx[i:min(i + batch_size, num_examples)]) yield nd.take(X, j), nd.take(y, j) def net(X, w, b): return nd.dot(X, w) + b def square_loss(yhat, y): return (yhat - y.reshape(yhat.shape)) ** 2 def SGD(params, lr): for param in params: param[:] = param - lr * param.grad def main(): num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 X = nd.random_normal(shape=(num_examples, num_inputs)) y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b y += 0.01 * nd.random_normal(shape=y.shape) batch_size = 10 # # 画图 # plt.scatter(X[:, 1].asnumpy(), y.asnumpy()) # plt.show() # for data, label in data_iter(X, y, num_examples, batch_size): # print(data, label) # break w = nd.random_normal(shape=(num_inputs, 1)) b = nd.zeros((1,)) params = [w, b] for param in params: param.attach_grad() epochs = 5 learning_rate = 0.01 for e in range(epochs): total_loss = 0 for data, label in data_iter(X, y, num_examples, batch_size): with autograd.record(): output = net(data, w, b) loss = square_loss(output, label) loss.backward() SGD(params, learning_rate) total_loss += nd.sum(loss).asscalar() print(\"Epoch %d,average loss is %f\" % (e, total_loss / num_examples)) print(true_w, w) print(true_b, b) pass if __name__ == '__main__': main() 使用gluon搭建线性回归 # coding=utf-8 from mxnet import ndarray as nd from mxnet import gluon from mxnet import autograd from mxnet.gluon.nn import Dense def main(): num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 X = nd.random_normal(shape=(num_examples, num_inputs)) y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b y += 0.01 * nd.random_normal(shape=y.shape) # print(\"x is \", X[:3]) # print(\"y is \", y[:3]) batch_size = 10 dataset = gluon.data.ArrayDataset(X, y) data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True) # for data, label in data_iter: # print(data, label) # break net = gluon.nn.Sequential() net.add(Dense(1)) net.initialize() square_loss = gluon.loss.L2Loss() trainer = gluon.Trainer(net.collect_params(), \"sgd\", { \"learning_rate\": 0.1}) epochs = 5 for e in range(epochs): total_loss = 0 for data, label in data_iter: with autograd.record(): output = net(data) loss = square_loss(output, label) loss.backward() trainer.step(batch_size) total_loss += nd.sum(loss).asscalar() print(\"Epoch %d,average loss is %f\" % (e, total_loss / num_examples)) dense = net[0] print(true_w, dense.weight.data()) print(true_b, dense.bias.data()) if __name__ == '__main__': main() ","date":"2017-11-21","objectID":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:1","series":null,"tags":["mxnet","深度学习"],"title":"mxnet学习之线性回归","uri":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#第一节课-从零开始之线性回归"},{"categories":["深度学习"],"content":" 第一节课 从零开始之线性回归 先从零开始，搭一个线性回归 # coding=utf-8 from mxnet import gluon from mxnet import ndarray as nd from mxnet import autograd import matplotlib.pyplot as plt import random def data_iter(X, y, num_examples, batch_size): # 产生索引 idx = list(range(num_examples)) random.shuffle(idx) for i in range(0, num_examples, batch_size): j = nd.array(idx[i:min(i + batch_size, num_examples)]) yield nd.take(X, j), nd.take(y, j) def net(X, w, b): return nd.dot(X, w) + b def square_loss(yhat, y): return (yhat - y.reshape(yhat.shape)) ** 2 def SGD(params, lr): for param in params: param[:] = param - lr * param.grad def main(): num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 X = nd.random_normal(shape=(num_examples, num_inputs)) y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b y += 0.01 * nd.random_normal(shape=y.shape) batch_size = 10 # # 画图 # plt.scatter(X[:, 1].asnumpy(), y.asnumpy()) # plt.show() # for data, label in data_iter(X, y, num_examples, batch_size): # print(data, label) # break w = nd.random_normal(shape=(num_inputs, 1)) b = nd.zeros((1,)) params = [w, b] for param in params: param.attach_grad() epochs = 5 learning_rate = 0.01 for e in range(epochs): total_loss = 0 for data, label in data_iter(X, y, num_examples, batch_size): with autograd.record(): output = net(data, w, b) loss = square_loss(output, label) loss.backward() SGD(params, learning_rate) total_loss += nd.sum(loss).asscalar() print(\"Epoch %d,average loss is %f\" % (e, total_loss / num_examples)) print(true_w, w) print(true_b, b) pass if __name__ == '__main__': main() 使用gluon搭建线性回归 # coding=utf-8 from mxnet import ndarray as nd from mxnet import gluon from mxnet import autograd from mxnet.gluon.nn import Dense def main(): num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 X = nd.random_normal(shape=(num_examples, num_inputs)) y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b y += 0.01 * nd.random_normal(shape=y.shape) # print(\"x is \", X[:3]) # print(\"y is \", y[:3]) batch_size = 10 dataset = gluon.data.ArrayDataset(X, y) data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True) # for data, label in data_iter: # print(data, label) # break net = gluon.nn.Sequential() net.add(Dense(1)) net.initialize() square_loss = gluon.loss.L2Loss() trainer = gluon.Trainer(net.collect_params(), \"sgd\", { \"learning_rate\": 0.1}) epochs = 5 for e in range(epochs): total_loss = 0 for data, label in data_iter: with autograd.record(): output = net(data) loss = square_loss(output, label) loss.backward() trainer.step(batch_size) total_loss += nd.sum(loss).asscalar() print(\"Epoch %d,average loss is %f\" % (e, total_loss / num_examples)) dense = net[0] print(true_w, dense.weight.data()) print(true_b, dense.bias.data()) if __name__ == '__main__': main() ","date":"2017-11-21","objectID":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:1","series":null,"tags":["mxnet","深度学习"],"title":"mxnet学习之线性回归","uri":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#先从零开始搭一个线性回归"},{"categories":["深度学习"],"content":" 第一节课 从零开始之线性回归 先从零开始，搭一个线性回归 # coding=utf-8 from mxnet import gluon from mxnet import ndarray as nd from mxnet import autograd import matplotlib.pyplot as plt import random def data_iter(X, y, num_examples, batch_size): # 产生索引 idx = list(range(num_examples)) random.shuffle(idx) for i in range(0, num_examples, batch_size): j = nd.array(idx[i:min(i + batch_size, num_examples)]) yield nd.take(X, j), nd.take(y, j) def net(X, w, b): return nd.dot(X, w) + b def square_loss(yhat, y): return (yhat - y.reshape(yhat.shape)) ** 2 def SGD(params, lr): for param in params: param[:] = param - lr * param.grad def main(): num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 X = nd.random_normal(shape=(num_examples, num_inputs)) y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b y += 0.01 * nd.random_normal(shape=y.shape) batch_size = 10 # # 画图 # plt.scatter(X[:, 1].asnumpy(), y.asnumpy()) # plt.show() # for data, label in data_iter(X, y, num_examples, batch_size): # print(data, label) # break w = nd.random_normal(shape=(num_inputs, 1)) b = nd.zeros((1,)) params = [w, b] for param in params: param.attach_grad() epochs = 5 learning_rate = 0.01 for e in range(epochs): total_loss = 0 for data, label in data_iter(X, y, num_examples, batch_size): with autograd.record(): output = net(data, w, b) loss = square_loss(output, label) loss.backward() SGD(params, learning_rate) total_loss += nd.sum(loss).asscalar() print(\"Epoch %d,average loss is %f\" % (e, total_loss / num_examples)) print(true_w, w) print(true_b, b) pass if __name__ == '__main__': main() 使用gluon搭建线性回归 # coding=utf-8 from mxnet import ndarray as nd from mxnet import gluon from mxnet import autograd from mxnet.gluon.nn import Dense def main(): num_inputs = 2 num_examples = 1000 true_w = [2, -3.4] true_b = 4.2 X = nd.random_normal(shape=(num_examples, num_inputs)) y = true_w[0] * X[:, 0] + true_w[1] * X[:, 1] + true_b y += 0.01 * nd.random_normal(shape=y.shape) # print(\"x is \", X[:3]) # print(\"y is \", y[:3]) batch_size = 10 dataset = gluon.data.ArrayDataset(X, y) data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True) # for data, label in data_iter: # print(data, label) # break net = gluon.nn.Sequential() net.add(Dense(1)) net.initialize() square_loss = gluon.loss.L2Loss() trainer = gluon.Trainer(net.collect_params(), \"sgd\", { \"learning_rate\": 0.1}) epochs = 5 for e in range(epochs): total_loss = 0 for data, label in data_iter: with autograd.record(): output = net(data) loss = square_loss(output, label) loss.backward() trainer.step(batch_size) total_loss += nd.sum(loss).asscalar() print(\"Epoch %d,average loss is %f\" % (e, total_loss / num_examples)) dense = net[0] print(true_w, dense.weight.data()) print(true_b, dense.bias.data()) if __name__ == '__main__': main() ","date":"2017-11-21","objectID":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/:0:1","series":null,"tags":["mxnet","深度学习"],"title":"mxnet学习之线性回归","uri":"/mxnet%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/#使用gluon搭建线性回归"},{"categories":["python"],"content":"前言\r自己动手，丰衣足食。本来的想法是爬取糗事百科的段子，然后发送到自己的邮箱，这样可以在电脑上，定时运行爬虫，然后，在手机端看段子了。但是，qq邮箱的smtp，一直没有配置好，所以只能先把段子保存到本地，有时间在优化。 talk is cheap,show my code: import requests from bs4 import BeautifulSoup import codecs import os def bs_textAnalysize(content): soup = BeautifulSoup(content,'lxml') content_list = soup.find_all('div',class_ = 'content') text_list = [] for text in content_list: span = text.find('span').text text_list.append(span) return text_list def main(url,data): text_list = [] for burl in url : response = requests.get(burl) response.encoding = 'utf-8' content = response.text text = bs_textAnalysize(content) text_list += text with codecs.open(data + '笑话.txt','w+','utf-8') as fp: for i,text in enumerate(text_list): fp.write('第'+ str(i + 1) + '个笑话:\\n') fp.write(text + '\\n') if __name__ == '__main__': base_url = 'https://www.qiushibaike.com/text/page/' data = '../data/' if not os.path.exists(data): os.mkdir(data) maxNum = 10 url = [base_url + str(i) for i in range(1,maxNum)] main(url,data) print('ok') 代码很简单，才36行。但是，代码爬取了，糗百的text分类下的10个页面的所有段子，大约有200多个。所以说，人生苦短，我用python。下面来一一解析。 引入库引入了requests和BeautifulSoup这两个最好用的第三方库，以及python自带的os和codecs库。 main函数其中url是个list，里面全是糗事百科的连接，然后用main来调用其他函数，main有两个参数，一个是url的list，一个是笑话的存储位置（data）。 bs_textAnalysize 函数使用BeautifulSoup来抽取段子的内容，然后将一个一个的段子加入到list中，并在main函数里，保存起来。 最后来一张效果图 ","date":"2017-10-26","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/:0:0","series":null,"tags":["python","爬虫"],"title":"爬虫之糗事百科","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/#"},{"categories":["python"],"content":"前言\r自己动手，丰衣足食。本来的想法是爬取糗事百科的段子，然后发送到自己的邮箱，这样可以在电脑上，定时运行爬虫，然后，在手机端看段子了。但是，qq邮箱的smtp，一直没有配置好，所以只能先把段子保存到本地，有时间在优化。 talk is cheap,show my code: import requests from bs4 import BeautifulSoup import codecs import os def bs_textAnalysize(content): soup = BeautifulSoup(content,'lxml') content_list = soup.find_all('div',class_ = 'content') text_list = [] for text in content_list: span = text.find('span').text text_list.append(span) return text_list def main(url,data): text_list = [] for burl in url : response = requests.get(burl) response.encoding = 'utf-8' content = response.text text = bs_textAnalysize(content) text_list += text with codecs.open(data + '笑话.txt','w+','utf-8') as fp: for i,text in enumerate(text_list): fp.write('第'+ str(i + 1) + '个笑话:\\n') fp.write(text + '\\n') if __name__ == '__main__': base_url = 'https://www.qiushibaike.com/text/page/' data = '../data/' if not os.path.exists(data): os.mkdir(data) maxNum = 10 url = [base_url + str(i) for i in range(1,maxNum)] main(url,data) print('ok') 代码很简单，才36行。但是，代码爬取了，糗百的text分类下的10个页面的所有段子，大约有200多个。所以说，人生苦短，我用python。下面来一一解析。 引入库引入了requests和BeautifulSoup这两个最好用的第三方库，以及python自带的os和codecs库。 main函数其中url是个list，里面全是糗事百科的连接，然后用main来调用其他函数，main有两个参数，一个是url的list，一个是笑话的存储位置（data）。 bs_textAnalysize 函数使用BeautifulSoup来抽取段子的内容，然后将一个一个的段子加入到list中，并在main函数里，保存起来。 最后来一张效果图 ","date":"2017-10-26","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/:0:0","series":null,"tags":["python","爬虫"],"title":"爬虫之糗事百科","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/#talk-is-cheapshow-my-code"},{"categories":["python"],"content":"前言\r自己动手，丰衣足食。本来的想法是爬取糗事百科的段子，然后发送到自己的邮箱，这样可以在电脑上，定时运行爬虫，然后，在手机端看段子了。但是，qq邮箱的smtp，一直没有配置好，所以只能先把段子保存到本地，有时间在优化。 talk is cheap,show my code: import requests from bs4 import BeautifulSoup import codecs import os def bs_textAnalysize(content): soup = BeautifulSoup(content,'lxml') content_list = soup.find_all('div',class_ = 'content') text_list = [] for text in content_list: span = text.find('span').text text_list.append(span) return text_list def main(url,data): text_list = [] for burl in url : response = requests.get(burl) response.encoding = 'utf-8' content = response.text text = bs_textAnalysize(content) text_list += text with codecs.open(data + '笑话.txt','w+','utf-8') as fp: for i,text in enumerate(text_list): fp.write('第'+ str(i + 1) + '个笑话:\\n') fp.write(text + '\\n') if __name__ == '__main__': base_url = 'https://www.qiushibaike.com/text/page/' data = '../data/' if not os.path.exists(data): os.mkdir(data) maxNum = 10 url = [base_url + str(i) for i in range(1,maxNum)] main(url,data) print('ok') 代码很简单，才36行。但是，代码爬取了，糗百的text分类下的10个页面的所有段子，大约有200多个。所以说，人生苦短，我用python。下面来一一解析。 引入库引入了requests和BeautifulSoup这两个最好用的第三方库，以及python自带的os和codecs库。 main函数其中url是个list，里面全是糗事百科的连接，然后用main来调用其他函数，main有两个参数，一个是url的list，一个是笑话的存储位置（data）。 bs_textAnalysize 函数使用BeautifulSoup来抽取段子的内容，然后将一个一个的段子加入到list中，并在main函数里，保存起来。 最后来一张效果图 ","date":"2017-10-26","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/:0:0","series":null,"tags":["python","爬虫"],"title":"爬虫之糗事百科","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/#引入库"},{"categories":["python"],"content":"前言\r自己动手，丰衣足食。本来的想法是爬取糗事百科的段子，然后发送到自己的邮箱，这样可以在电脑上，定时运行爬虫，然后，在手机端看段子了。但是，qq邮箱的smtp，一直没有配置好，所以只能先把段子保存到本地，有时间在优化。 talk is cheap,show my code: import requests from bs4 import BeautifulSoup import codecs import os def bs_textAnalysize(content): soup = BeautifulSoup(content,'lxml') content_list = soup.find_all('div',class_ = 'content') text_list = [] for text in content_list: span = text.find('span').text text_list.append(span) return text_list def main(url,data): text_list = [] for burl in url : response = requests.get(burl) response.encoding = 'utf-8' content = response.text text = bs_textAnalysize(content) text_list += text with codecs.open(data + '笑话.txt','w+','utf-8') as fp: for i,text in enumerate(text_list): fp.write('第'+ str(i + 1) + '个笑话:\\n') fp.write(text + '\\n') if __name__ == '__main__': base_url = 'https://www.qiushibaike.com/text/page/' data = '../data/' if not os.path.exists(data): os.mkdir(data) maxNum = 10 url = [base_url + str(i) for i in range(1,maxNum)] main(url,data) print('ok') 代码很简单，才36行。但是，代码爬取了，糗百的text分类下的10个页面的所有段子，大约有200多个。所以说，人生苦短，我用python。下面来一一解析。 引入库引入了requests和BeautifulSoup这两个最好用的第三方库，以及python自带的os和codecs库。 main函数其中url是个list，里面全是糗事百科的连接，然后用main来调用其他函数，main有两个参数，一个是url的list，一个是笑话的存储位置（data）。 bs_textAnalysize 函数使用BeautifulSoup来抽取段子的内容，然后将一个一个的段子加入到list中，并在main函数里，保存起来。 最后来一张效果图 ","date":"2017-10-26","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/:0:0","series":null,"tags":["python","爬虫"],"title":"爬虫之糗事百科","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/#main函数"},{"categories":["python"],"content":"前言\r自己动手，丰衣足食。本来的想法是爬取糗事百科的段子，然后发送到自己的邮箱，这样可以在电脑上，定时运行爬虫，然后，在手机端看段子了。但是，qq邮箱的smtp，一直没有配置好，所以只能先把段子保存到本地，有时间在优化。 talk is cheap,show my code: import requests from bs4 import BeautifulSoup import codecs import os def bs_textAnalysize(content): soup = BeautifulSoup(content,'lxml') content_list = soup.find_all('div',class_ = 'content') text_list = [] for text in content_list: span = text.find('span').text text_list.append(span) return text_list def main(url,data): text_list = [] for burl in url : response = requests.get(burl) response.encoding = 'utf-8' content = response.text text = bs_textAnalysize(content) text_list += text with codecs.open(data + '笑话.txt','w+','utf-8') as fp: for i,text in enumerate(text_list): fp.write('第'+ str(i + 1) + '个笑话:\\n') fp.write(text + '\\n') if __name__ == '__main__': base_url = 'https://www.qiushibaike.com/text/page/' data = '../data/' if not os.path.exists(data): os.mkdir(data) maxNum = 10 url = [base_url + str(i) for i in range(1,maxNum)] main(url,data) print('ok') 代码很简单，才36行。但是，代码爬取了，糗百的text分类下的10个页面的所有段子，大约有200多个。所以说，人生苦短，我用python。下面来一一解析。 引入库引入了requests和BeautifulSoup这两个最好用的第三方库，以及python自带的os和codecs库。 main函数其中url是个list，里面全是糗事百科的连接，然后用main来调用其他函数，main有两个参数，一个是url的list，一个是笑话的存储位置（data）。 bs_textAnalysize 函数使用BeautifulSoup来抽取段子的内容，然后将一个一个的段子加入到list中，并在main函数里，保存起来。 最后来一张效果图 ","date":"2017-10-26","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/:0:0","series":null,"tags":["python","爬虫"],"title":"爬虫之糗事百科","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/#bs_textanalysize-函数"},{"categories":["python"],"content":"前言\r自己动手，丰衣足食。本来的想法是爬取糗事百科的段子，然后发送到自己的邮箱，这样可以在电脑上，定时运行爬虫，然后，在手机端看段子了。但是，qq邮箱的smtp，一直没有配置好，所以只能先把段子保存到本地，有时间在优化。 talk is cheap,show my code: import requests from bs4 import BeautifulSoup import codecs import os def bs_textAnalysize(content): soup = BeautifulSoup(content,'lxml') content_list = soup.find_all('div',class_ = 'content') text_list = [] for text in content_list: span = text.find('span').text text_list.append(span) return text_list def main(url,data): text_list = [] for burl in url : response = requests.get(burl) response.encoding = 'utf-8' content = response.text text = bs_textAnalysize(content) text_list += text with codecs.open(data + '笑话.txt','w+','utf-8') as fp: for i,text in enumerate(text_list): fp.write('第'+ str(i + 1) + '个笑话:\\n') fp.write(text + '\\n') if __name__ == '__main__': base_url = 'https://www.qiushibaike.com/text/page/' data = '../data/' if not os.path.exists(data): os.mkdir(data) maxNum = 10 url = [base_url + str(i) for i in range(1,maxNum)] main(url,data) print('ok') 代码很简单，才36行。但是，代码爬取了，糗百的text分类下的10个页面的所有段子，大约有200多个。所以说，人生苦短，我用python。下面来一一解析。 引入库引入了requests和BeautifulSoup这两个最好用的第三方库，以及python自带的os和codecs库。 main函数其中url是个list，里面全是糗事百科的连接，然后用main来调用其他函数，main有两个参数，一个是url的list，一个是笑话的存储位置（data）。 bs_textAnalysize 函数使用BeautifulSoup来抽取段子的内容，然后将一个一个的段子加入到list中，并在main函数里，保存起来。 最后来一张效果图 ","date":"2017-10-26","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/:0:0","series":null,"tags":["python","爬虫"],"title":"爬虫之糗事百科","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%B3%97%E4%BA%8B%E7%99%BE%E7%A7%91/#最后来一张效果图"},{"categories":["python"],"content":" 前言 好久没写博客了，懒死了，为了培养同学对python的喜爱，顺手写了一个爬虫，也是对自己知识的巩固，其实，这个爬虫挺简单的，半小时就能写完，但是2.x的python，各种字符编码的转换，真心累，而且，转到3.x系列还很多bug，暂时就先这样吧，等有时间再优化。 这份代码，定向爬取了奇书网的女频言情小说（其实，我应该爬仙侠小说的），代码里面有注释，所及，就这样了。 show code： # coding: utf-8 import requests from bs4 import BeautifulSoup import os import multiprocessing import urllib ''' 说明： 需要安装beautifulsoup4,lxml,requests 直接在cmd命令行下运行： pip install beautifulsoup4 pip install lxml pip install requests 开发环境为python 2.7.13 ''' def Schedule(a,b,c): ''''' a:已经下载的数据块 b:数据块的大小 c:远程文件的大小 ''' per = 100.0 * a * b / c if per \u003e 100 : per = 100 print('%.2f%%' % per) def has_class_but_no_target(tag): ''' tag:筛选tag函数 ''' return tag.has_attr('href') and not tag.has_attr('target') def get_all_download_link(url,data): ''' url:小说详情页面，例如：https://www.qisuu.com/36457.html data:小说存放位置 ''' # s = requests.Session() response = requests.get(url) response.encoding = 'utf-8' html = response.text soup = BeautifulSoup(html,'lxml') div_list = soup.find('div',class_ = 'showDown') a_list = div_list.find_all('a') for k,v in enumerate(a_list): href = v.get('href').encode('utf-8') if 'txt' in href: title = v.get('title') data = os.path.join(data,title + '.txt') print(title) ###下载小说 urllib.urlretrieve(href,data,Schedule) def get_all_link(url): ''' url:每个主页面,例如:https://www.qisuu.com/soft/sort03/index_2.html ''' s = requests.Session() response = s.get(url) response.encoding = 'utf-8' # print(response.text) html = response.text soup = BeautifulSoup(html,'lxml') div_list = soup.find('div',class_ = 'listBox') a_list = div_list.find_all(has_class_but_no_target) # print(a_list) a_all = [] for a in a_list: link = a.get('href') if not 'soft' in link: a_all.append(link) return a_all def main(url_links,base_url,data): ''' url_links:所有的主页面的list base_url:基础url，做拼接用 data:小说存放位置 ''' print('start runing:') all_links = [] for url in url_links: all_link = get_all_link(url) all_links = all_links + all_link all_links = [base_url + link for link in all_links] # pool = multiprocessing.Pool(multiprocessing.cpu_count()) for i,link in enumerate(all_links): print('第{0}本小说正在下载：'.format(i + 1)) get_all_download_link(link,data) # pool.apply_async(get_all_download_link, (url,data)) print('第{0}本小说下载完成.'.format(i + 1)) print('所有的小说下载完成。') if __name__ == \"__main__\": url = 'https://www.qisuu.com/soft/sort03/' base_url = 'https://www.qisuu.com' ##小说存放的地方 data = '../data' if not os.path.exists(data): os.mkdir(data) ##此处可以修改，设置爬取多少个页面，每个页面有15个小说，最大值为50 maxNum = 2 number = ['index_' + str(i) + '.html' for i in range(1,maxNum)] ##修正第一页的url number[0] = 'index.html' url_links = [url + i for i in number] main(url_links,base_url,data) 最后来张效果图 ","date":"2017-10-21","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4/:1:0","series":null,"tags":["python","爬虫"],"title":"爬虫之爬取小说","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4/#center-前言center"},{"categories":["python"],"content":" 前言 好久没写博客了，懒死了，为了培养同学对python的喜爱，顺手写了一个爬虫，也是对自己知识的巩固，其实，这个爬虫挺简单的，半小时就能写完，但是2.x的python，各种字符编码的转换，真心累，而且，转到3.x系列还很多bug，暂时就先这样吧，等有时间再优化。 这份代码，定向爬取了奇书网的女频言情小说（其实，我应该爬仙侠小说的），代码里面有注释，所及，就这样了。 show code： # coding: utf-8 import requests from bs4 import BeautifulSoup import os import multiprocessing import urllib ''' 说明： 需要安装beautifulsoup4,lxml,requests 直接在cmd命令行下运行： pip install beautifulsoup4 pip install lxml pip install requests 开发环境为python 2.7.13 ''' def Schedule(a,b,c): ''''' a:已经下载的数据块 b:数据块的大小 c:远程文件的大小 ''' per = 100.0 * a * b / c if per \u003e 100 : per = 100 print('%.2f%%' % per) def has_class_but_no_target(tag): ''' tag:筛选tag函数 ''' return tag.has_attr('href') and not tag.has_attr('target') def get_all_download_link(url,data): ''' url:小说详情页面，例如：https://www.qisuu.com/36457.html data:小说存放位置 ''' # s = requests.Session() response = requests.get(url) response.encoding = 'utf-8' html = response.text soup = BeautifulSoup(html,'lxml') div_list = soup.find('div',class_ = 'showDown') a_list = div_list.find_all('a') for k,v in enumerate(a_list): href = v.get('href').encode('utf-8') if 'txt' in href: title = v.get('title') data = os.path.join(data,title + '.txt') print(title) ###下载小说 urllib.urlretrieve(href,data,Schedule) def get_all_link(url): ''' url:每个主页面,例如:https://www.qisuu.com/soft/sort03/index_2.html ''' s = requests.Session() response = s.get(url) response.encoding = 'utf-8' # print(response.text) html = response.text soup = BeautifulSoup(html,'lxml') div_list = soup.find('div',class_ = 'listBox') a_list = div_list.find_all(has_class_but_no_target) # print(a_list) a_all = [] for a in a_list: link = a.get('href') if not 'soft' in link: a_all.append(link) return a_all def main(url_links,base_url,data): ''' url_links:所有的主页面的list base_url:基础url，做拼接用 data:小说存放位置 ''' print('start runing:') all_links = [] for url in url_links: all_link = get_all_link(url) all_links = all_links + all_link all_links = [base_url + link for link in all_links] # pool = multiprocessing.Pool(multiprocessing.cpu_count()) for i,link in enumerate(all_links): print('第{0}本小说正在下载：'.format(i + 1)) get_all_download_link(link,data) # pool.apply_async(get_all_download_link, (url,data)) print('第{0}本小说下载完成.'.format(i + 1)) print('所有的小说下载完成。') if __name__ == \"__main__\": url = 'https://www.qisuu.com/soft/sort03/' base_url = 'https://www.qisuu.com' ##小说存放的地方 data = '../data' if not os.path.exists(data): os.mkdir(data) ##此处可以修改，设置爬取多少个页面，每个页面有15个小说，最大值为50 maxNum = 2 number = ['index_' + str(i) + '.html' for i in range(1,maxNum)] ##修正第一页的url number[0] = 'index.html' url_links = [url + i for i in number] main(url_links,base_url,data) 最后来张效果图 ","date":"2017-10-21","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4/:1:0","series":null,"tags":["python","爬虫"],"title":"爬虫之爬取小说","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4/#show-code"},{"categories":["python"],"content":" 前言 好久没写博客了，懒死了，为了培养同学对python的喜爱，顺手写了一个爬虫，也是对自己知识的巩固，其实，这个爬虫挺简单的，半小时就能写完，但是2.x的python，各种字符编码的转换，真心累，而且，转到3.x系列还很多bug，暂时就先这样吧，等有时间再优化。 这份代码，定向爬取了奇书网的女频言情小说（其实，我应该爬仙侠小说的），代码里面有注释，所及，就这样了。 show code： # coding: utf-8 import requests from bs4 import BeautifulSoup import os import multiprocessing import urllib ''' 说明： 需要安装beautifulsoup4,lxml,requests 直接在cmd命令行下运行： pip install beautifulsoup4 pip install lxml pip install requests 开发环境为python 2.7.13 ''' def Schedule(a,b,c): ''''' a:已经下载的数据块 b:数据块的大小 c:远程文件的大小 ''' per = 100.0 * a * b / c if per \u003e 100 : per = 100 print('%.2f%%' % per) def has_class_but_no_target(tag): ''' tag:筛选tag函数 ''' return tag.has_attr('href') and not tag.has_attr('target') def get_all_download_link(url,data): ''' url:小说详情页面，例如：https://www.qisuu.com/36457.html data:小说存放位置 ''' # s = requests.Session() response = requests.get(url) response.encoding = 'utf-8' html = response.text soup = BeautifulSoup(html,'lxml') div_list = soup.find('div',class_ = 'showDown') a_list = div_list.find_all('a') for k,v in enumerate(a_list): href = v.get('href').encode('utf-8') if 'txt' in href: title = v.get('title') data = os.path.join(data,title + '.txt') print(title) ###下载小说 urllib.urlretrieve(href,data,Schedule) def get_all_link(url): ''' url:每个主页面,例如:https://www.qisuu.com/soft/sort03/index_2.html ''' s = requests.Session() response = s.get(url) response.encoding = 'utf-8' # print(response.text) html = response.text soup = BeautifulSoup(html,'lxml') div_list = soup.find('div',class_ = 'listBox') a_list = div_list.find_all(has_class_but_no_target) # print(a_list) a_all = [] for a in a_list: link = a.get('href') if not 'soft' in link: a_all.append(link) return a_all def main(url_links,base_url,data): ''' url_links:所有的主页面的list base_url:基础url，做拼接用 data:小说存放位置 ''' print('start runing:') all_links = [] for url in url_links: all_link = get_all_link(url) all_links = all_links + all_link all_links = [base_url + link for link in all_links] # pool = multiprocessing.Pool(multiprocessing.cpu_count()) for i,link in enumerate(all_links): print('第{0}本小说正在下载：'.format(i + 1)) get_all_download_link(link,data) # pool.apply_async(get_all_download_link, (url,data)) print('第{0}本小说下载完成.'.format(i + 1)) print('所有的小说下载完成。') if __name__ == \"__main__\": url = 'https://www.qisuu.com/soft/sort03/' base_url = 'https://www.qisuu.com' ##小说存放的地方 data = '../data' if not os.path.exists(data): os.mkdir(data) ##此处可以修改，设置爬取多少个页面，每个页面有15个小说，最大值为50 maxNum = 2 number = ['index_' + str(i) + '.html' for i in range(1,maxNum)] ##修正第一页的url number[0] = 'index.html' url_links = [url + i for i in number] main(url_links,base_url,data) 最后来张效果图 ","date":"2017-10-21","objectID":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4/:1:0","series":null,"tags":["python","爬虫"],"title":"爬虫之爬取小说","uri":"/%E7%88%AC%E8%99%AB%E4%B9%8B%E7%88%AC%E5%8F%96%E5%B0%8F%E8%AF%B4/#最后来张效果图"},{"categories":["算法","数据结构"],"content":" 前言 队列是一种特殊的线性表，特殊之处在于它只允许在表的前端（front）进行删除操作，而在表的后端（rear）进行插入操作，和栈一样，队列是一种操作受限制的线性表。进行插入操作的端称为队尾，进行删除操作的端称为队头。队列中没有元素时，称为空队列。 队列的数据元素又称为队列元素。在队列中插入一个队列元素称为入队，从队列中删除一个队列元素称为出队。因为队列只允许在一端插入，在另一端删除，所以只有最早进入队列的元素才能最先从队列中删除，故队列又称为先进先出（FIFO—first in first out）线性表。 ","date":"2017-04-09","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%98%9F%E5%88%97/:1:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之队列","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%98%9F%E5%88%97/#center-前言"},{"categories":["算法","数据结构"],"content":" 顺序队列的C语言实现show code /* 说明：循环队列的C语言实现 */ #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #define ElemType int #define MaxSize 100 //顺序栈的数据结构 typedef struct { ElemType data[MaxSize]; int rear; //指示进队的位置 int front; //指示出队的位置 }SqQueue; //初始化队列 void InitQueue(SqQueue *queue); //进队 void EnQueue(SqQueue *queue,ElemType e); //出队 void DeQueue(SqQueue *queue,ElemType *e); //队是否为空 int IsEmpty(SqQueue queue); //队是否为满 int IsFull(SqQueue queue); //遍历元素 void Traverse(SqQueue queue); int main(int argc, char const *argv[]) { SqQueue queue; int e = 0; //初始化队列 InitQueue(\u0026queue); //进队元素 EnQueue(\u0026queue,1); EnQueue(\u0026queue,2); //出队元素 DeQueue(\u0026queue,\u0026e); //遍历元素 Traverse(queue); return 0; } //初始化队列 void InitQueue(SqQueue *queue) { queue-\u003efront = queue-\u003erear = 0; } //进队 void EnQueue(SqQueue *queue,ElemType e) { printf(\"Begin EnQueue data :%d\\n\",e); if(IsFull(*queue)) { puts(\"It is not enough memory.\"); } else { queue-\u003erear = (queue-\u003erear + 1)%MaxSize; queue-\u003edata[queue-\u003erear] = e; puts(\"End EnQueue.\"); } } //出队 void DeQueue(SqQueue *queue,ElemType *e) { puts(\"Begin DeQueue data:\"); if(IsEmpty(*queue)) { puts(\"It is not data\"); } else { queue-\u003efront = (queue-\u003efront + 1)%MaxSize; *e = queue-\u003edata[queue-\u003efront]; printf(\"End DeQueue .Data is :%d\\n\",*e); } } //队是否为空 int IsEmpty(SqQueue queue) { if(queue.front == queue.rear) { return 1; } else { return 0; } } //队是否为满 int IsFull(SqQueue queue) { if(queue.front == (queue.rear+1)%MaxSize) { return 1; } else { return 0; } } //遍历元素 void Traverse(SqQueue queue) { puts(\"Begin Traverse data.\"); if(IsEmpty(queue)) { puts(\"There is no data.\"); } else { int i = queue.front; while(i != queue.rear) { i = (i+1)%MaxSize; printf(\"%d\\t\",queue.data[i]); } putchar('\\n'); } } ","date":"2017-04-09","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%98%9F%E5%88%97/:2:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之队列","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%98%9F%E5%88%97/#顺序队列的c语言实现"},{"categories":["算法","数据结构"],"content":" 链队列的C语言实现基本show code /* 说明：链队的C语言实现 */ #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #define ElemType int #define MaxSize 100 //链队的数据结构 //首先是队的节点结构 typedef struct QNode { ElemType data; struct QNode *next; }QNode; //定义类型 typedef struct { QNode *front; QNode *rear; }LinkQueue; //初始化链队 void InitQueue(LinkQueue **queue); //进队 void EnQueue(LinkQueue **queue,ElemType e); //出队 void DeQueue(LinkQueue **queue,ElemType *e); //队是否为空 int IsEmpty(LinkQueue *queue); //遍历元素 void Traverse(LinkQueue *queue); int main(int argc, char const *argv[]) { LinkQueue *queue; int e = 0; //初始化链队 InitQueue(\u0026queue); //进队 EnQueue(\u0026queue,1); EnQueue(\u0026queue,2); //出队 DeQueue(\u0026queue,\u0026e); //遍历元素 Traverse(queue); return 0; } //初始化链队 void InitQueue(LinkQueue **queue) { puts(\"Begin InitQueue.\"); *queue = malloc(sizeof(LinkQueue)); (*queue)-\u003efront = (*queue)-\u003erear = NULL; puts(\"End InitQueue.\"); } //遍历元素 void Traverse(LinkQueue *queue) { puts(\"Begin Traverse.\"); if(IsEmpty(queue)) { puts(\"It is a NULL Queue.\"); } else { QNode *q = queue-\u003efront; while(q\u0026\u0026q \u003c= queue-\u003erear) //注意这里容易出Bug { printf(\"%d\\t\",q-\u003edata); q = q-\u003enext; } putchar('\\n'); puts(\"End Traverse.\"); } } //队是否为空 int IsEmpty(LinkQueue *queue) { //注意这里的判空条件 if(queue-\u003efront == NULL || queue-\u003erear == NULL) { return 1; } else { return 0; } } //进队 void EnQueue(LinkQueue **queue,ElemType e) { printf(\"Begin DeQueue Data is %d\\n\",e); QNode *q = malloc(sizeof(QNode)); q-\u003edata = e; q-\u003enext = NULL; if(IsEmpty(*queue)) ///如果队列为空，则这是一个队头和队尾 { (*queue)-\u003efront = (*queue)-\u003erear = q; } else //队不空。，直接插到队尾 { (*queue)-\u003erear-\u003enext = q; (*queue)-\u003erear = q; //队尾指向q } puts(\"End EnQueue.\"); } //出队 void DeQueue(LinkQueue **queue,ElemType *e) { puts(\"Begin DeQueue.\"); if(IsEmpty(*queue)) { puts(\"It is a NULL Queue.\"); } else { QNode *q = malloc(sizeof(QNode)); q = (*queue)-\u003efront; if((*queue)-\u003efront == (*queue)-\u003erear)//说明队列里只有一个节点 { (*queue)-\u003efront = (*queue)-\u003erear = NULL; } else { *e = q-\u003edata; (*queue)-\u003efront = q-\u003enext; free(q); printf(\"End DeQueue Data is :%d\\n\",*e); } } } ","date":"2017-04-09","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%98%9F%E5%88%97/:3:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之队列","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%98%9F%E5%88%97/#链队列的c语言实现"},{"categories":["算法","数据结构"],"content":" 前言 栈（stack）又名堆栈，它是一种运算受限的线性表。其限制是仅允许在表的一端进行插入和删除运算。这一端被称为栈顶，相对地，把另一端称为栈底。向一个栈插入新元素又称作进栈、入栈或压栈，它是把新元素放到栈顶元素的上面，使之成为新的栈顶元素；从一个栈删除元素又称作出栈或退栈，它是把栈顶元素删除掉，使其相邻的元素成为新的栈顶元素。 栈作为一种数据结构，是一种只能在一端进行插入和删除操作的特殊线性表。它按照先进后出的原则存储数据，先进入的数据被压入栈底，最后的数据在栈顶，需要读数据的时候从栈顶开始弹出数据（最后一个数据被第一个读出来）。栈具有记忆作用，对栈的插入与删除操作中，不需要改变栈底指针。 ","date":"2017-04-09","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88/:1:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之栈","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88/#center-前言"},{"categories":["算法","数据结构"],"content":" 顺序栈的C语言实现show code: /* 说明：顺序栈的C语言实现 */ #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #define ElemType int #define MaxSize 100 //顺序栈的数据结构 typedef struct { ElemType data[MaxSize]; int top; }SqlStack; //初始化栈 void InitStack(SqlStack *stack); //栈顶插入元素 void push(SqlStack *stack,ElemType e); //栈顶删除元素 void pop(SqlStack *stack,ElemType *e); //栈是否为空 int IsEmpty(SqlStack stack); //栈是否为满 int IsFull(SqlStack stack); //输出栈内元素 void Traverse(SqlStack stack); //主函数 int main(int argc, char const *argv[]) { SqlStack stack; int e = 0; //初始化栈 InitStack(\u0026stack); //printf(\"%d\",stack.top); //进栈 push(\u0026stack,1); push(\u0026stack,2); //出栈 pop(\u0026stack,\u0026e); //遍历元素 Traverse(stack); return 0; } //初始化栈 void InitStack(SqlStack *stack) { puts(\"Begin InitStack:\"); stack-\u003etop = -1; //赋值为-1为了节省一个内存空间，也可为0 puts(\"End InitStack.\"); } //栈顶插入元素 void push(SqlStack *stack,ElemType e) { if(IsFull(*stack)) { puts(\"No Enough Memory.\"); } else { printf(\"Begin Push Data:%d\\n\",e); stack-\u003edata[++(stack-\u003etop)] = e; puts(\"End Push.\"); } } //栈顶删除元素 void pop(SqlStack *stack,ElemType *e) { puts(\"Begin Pop Data:\"); if(IsEmpty(*stack)) { puts(\"it is a NULL Stack.\"); } else { *e = stack-\u003edata[stack-\u003etop--]; printf(\"end pop data is :%d\\n\",*e); } } //栈是否为空 int IsEmpty(SqlStack stack) { if(stack.top == -1) { return 1; } else { return 0; } } //输出栈内元素 void Traverse(SqlStack stack) { if(!IsEmpty(stack)) { puts(\"Traverse:\"); for(int i = stack.top;i\u003e-1;i--) { printf(\"%d\\t\",stack.data[i]); } putchar('\\n'); } } //栈是否为满 int IsFull(SqlStack stack) { if(stack.top == MaxSize - 1) { return 1; } else { return 0; } } ","date":"2017-04-09","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88/:2:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之栈","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88/#顺序栈的c语言实现"},{"categories":["算法","数据结构"],"content":" 链栈的C语言实现show cod /* 说明:带有头节点链栈的C语言实现 */ #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #define ElemType int #define MaxSize 100 //链栈的数据结构 typedef struct StackNode { ElemType data; struct StackNode *next; }LinkStack; //初始化链栈 void InitStack(LinkStack **stack); //压栈 void push(LinkStack **stack,ElemType e); //出栈 void pop(LinkStack **stack,ElemType *e); //遍历栈元素 void Traverse(LinkStack *stack); //栈是否为空 int IsEmpty(LinkStack *stack); //获取栈顶元素 ElemType GetElem(LinkStack *stack); //主函数 int main(int argc, char const *argv[]) { LinkStack *stack; int e = 0; //初始化链栈 InitStack(\u0026stack); //进栈数据 push(\u0026stack,1); push(\u0026stack,2); push(\u0026stack,3); //获取栈顶元素 printf(\"GetElem pop is :%d\\n\",GetElem(stack)); //出栈 pop(\u0026stack,\u0026e); //遍历数据 Traverse(stack); return 0; } //初始化链栈 void InitStack(LinkStack **stack) { puts(\"Begin InitStack:\"); *stack = malloc(sizeof(LinkStack)); (*stack)-\u003enext = NULL; puts(\"End InitStack.\"); } //栈是否为空 int IsEmpty(LinkStack *stack) { if(stack-\u003enext == NULL) { return 1; } else { return 0; } } //压栈,采用头插法 void push(LinkStack **stack,ElemType e) { printf(\"Begin Push Data:%d\\n\",e); LinkStack *p = malloc(sizeof(LinkStack)); p-\u003edata = e; p-\u003enext = (*stack)-\u003enext; (*stack)-\u003enext = p; puts(\"End Push.\"); } //出栈 void pop(LinkStack **stack,ElemType *e) { puts(\"Begin Pop Data:\"); if(IsEmpty(*stack)) { puts(\"it is a NUll Stack\"); } else { LinkStack *temp = NULL; temp = (*stack)-\u003enext; *e = temp-\u003edata; (*stack)-\u003enext = temp-\u003enext; free(temp); //释放申请的资源 printf(\"end pop data is :%d\\n\",*e); } } //遍历栈元素 void Traverse(LinkStack *stack) { puts(\"Begin Traverse stack:\"); LinkStack *rNext = stack-\u003enext; while(rNext != NULL) { printf(\"%d\\t\",rNext-\u003edata); rNext = rNext-\u003enext; } putchar('\\n'); puts(\"End Traverse Data.\"); } //获取栈顶元素 ElemType GetElem(LinkStack *stack) { puts(\"Bengin GetElem:\"); if(IsEmpty(stack)) { puts(\"It is no Data.\"); return 0; } else { return stack-\u003enext-\u003edata; } } ","date":"2017-04-09","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88/:3:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之栈","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88/#链栈的c语言实现"},{"categories":["算法","数据结构"],"content":" 前言今天来写一下C语言对链表的实现，由于在严蔚敏老师的书里，引入了c++语言的引用类型，在c语言中，并没有这一种类型，我用二级指针来代替。 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#center-前言"},{"categories":["算法","数据结构"],"content":" 基本定义 #define ElemType int #define MaxSize 100 //定义链表节点 typedef struct Lnode { ElemType data; struct Lnode *next; }LinkList; ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:1","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#基本定义"},{"categories":["算法","数据结构"],"content":" 函数原型声明 //初始化链表 void InitList(LinkList **list); //遍历链表 void Traverse(LinkList *list); //查找指定位置元素 ElemType GetElem(LinkList *list,int p); //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e); //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e); //链表是否为空，空返回1 int IsEmpty(LinkList *list); //计算链表长度，返回链表长度 int LengthList(LinkList *list); ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:2","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#函数原型声明"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;i\u003cp-1;i++) { rNext = rNext-\u003enext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(i\u003cp-2) //注意是这里p-2,我们要找到删除元素的前一个节点,没毛病 { i++; rNext = rNext-\u003enext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#函数定义"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#初始化函数"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#遍历函数"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#查找函数"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#判空函数"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#链表长度函数"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#插入函数"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#删除函数"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#主函数"},{"categories":["算法","数据结构"],"content":" 函数定义 初始化函数 //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } 遍历函数 //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } 查找函数 //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;inext; } puts(\"End Find.\"); return rNext-\u003edata; } } 判空函数 //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } 链表长度函数 //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } 插入函数 //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } 删除函数 //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(inext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } 主函数 //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } 结果 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#结果"},{"categories":["算法","数据结构"],"content":" 说明：C语言是没有引用类型的，这是在c++中新定义的类型，其实引用和指针并没有太大的区别，所以我们直接用指针也可以，在本文里出现了二级指针是一个难点。 ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:1:4","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#说明"},{"categories":["算法","数据结构"],"content":" 附源码 /** 说明：带头节点链表的C语言实现 author : gsscsd data : 2017-3-13 **/ #include \u003cstdio.h\u003e #include \u003cstdlib.h\u003e #define ElemType int #define MaxSize 100 //定义链表节点 typedef struct Lnode { ElemType data; struct Lnode *next; }LinkList; //初始化链表 void InitList(LinkList **list); //遍历链表 void Traverse(LinkList *list); //查找指定位置元素 ElemType GetElem(LinkList *list,int p); //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e); //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e); //链表是否为空，空返回1 int IsEmpty(LinkList *list); //计算链表长度，返回链表长度 int LengthList(LinkList *list); //主函数 int main(int argc, char const *argv[]) { LinkList *list = NULL; ElemType e = 9; InitList(\u0026list); //插入元素 InsertList(\u0026list,4,e); //删除元素 DeleteList(\u0026list,4,\u0026e); //查找元素 e = GetElem(list,3); printf(\"Find Elem is :%d\\n\",e); Traverse(list); return 0; } //初始化链表 void InitList(LinkList **list) { puts(\"InitList\"); int i = 0,n = 0; *list = malloc(sizeof(LinkList)); LinkList *temp,*rNext; // (*list)-\u003enext = NULL; rNext = *list; puts(\"Input InitList Numbers:\"); scanf(\"%d\",\u0026n); for(i = 0; i \u003c n; i++) { temp = malloc(sizeof(LinkList)); puts(\"Input Number:\"); scanf(\"%d\",\u0026(temp-\u003edata)); //接下来有两种方法，头插，尾差，这里采用尾差 rNext-\u003enext = temp; rNext = rNext-\u003enext; } rNext-\u003enext = NULL; } //遍历链表 void Traverse(LinkList *list) { puts(\"Begin Traverse:\"); LinkList *temp = list-\u003enext; //临时变量，方便遍历 while(temp != NULL) { printf(\"%d\\t\",temp-\u003edata); temp = temp-\u003enext; } putchar('\\n'); puts(\"Finished Traverse.\"); } //查找指定位置的元素 ElemType GetElem(LinkList *list,int p) { printf(\"Find %d Elem:\\n\",p); if((p\u003c0)||(p\u003eLengthList(list))) { return -1; } else { LinkList *rNext = list-\u003enext; for(int i = 0;i\u003cp-1;i++) { rNext = rNext-\u003enext; } puts(\"End Find.\"); return rNext-\u003edata; } } //在指定位置插入元素,失败返回-1，成功返回1 int InsertList(LinkList **list,int p,ElemType e) { if((p\u003c0)||(p\u003eLengthList(*list)+1)) { return -1; } else { printf(\"Begin Insert Elem:%d\\n\",e); int i = 0; LinkList *temp = malloc(sizeof(LinkList)); //一个暂时变量 LinkList *rNext = *list; //其实感觉这里应该是(*list)-\u003enext LinkList *q; temp-\u003edata = e; while(i \u003c p-1) //p-1,这里的bug是运算符优先级，找了好久 { rNext = rNext-\u003enext; i++; } //以下是尾插法 if(!IsEmpty(rNext)) { q = rNext-\u003enext; rNext-\u003enext = temp; temp-\u003enext = q; puts(\"End Insert.\"); return 1; } return -1; } } //删除指定位置的元素,失败返回-1，成功返回1 int DeleteList(LinkList **list,int p,ElemType *e) { if((p\u003c0)||(p\u003eLengthList(*list))) { return -1; } else { puts(\"Begin Delete Elem:\"); int i = 0; LinkList *rNext = (*list)-\u003enext; LinkList *temp; while(i\u003cp-2) //注意是这里p-2,我们要找到删除元素的前一个节点,没毛病 { i++; rNext = rNext-\u003enext; } if(!IsEmpty(rNext)) { temp = rNext-\u003enext; *e = temp-\u003edata; rNext-\u003enext = temp-\u003enext; free(temp); printf(\"End Delete:%d\\n\",*e); return 1; } } } //链表是否为空，空返回1 int IsEmpty(LinkList *list) { if(list-\u003enext == NULL) { return 1; } return 0; } //计算链表长度，成功返回链表长度 int LengthList(LinkList *list) { LinkList *rNext = list-\u003enext; int length = 0; while(rNext != NULL) { length++; rNext = rNext-\u003enext; } return length; } ","date":"2017-03-15","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/:2:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之链表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/#附源码"},{"categories":["算法","数据结构"],"content":" 前言严蔚敏的《数据结构》书里面全是类c代码，是c也不是c，感觉好别扭，于是用c语言重新写一遍，遇到好些bug，记录一下。 以下出自百度百科 顺序表是在计算机内存中以数组的形式保存的线性表，是指用一组地址连续的存储单元依次存储数据元素的线性结构。线性表采用顺序存储的方式存储就称之为顺序表。顺序表是将表中的结点依次存放在计算机内存中一组地址连续的存储单元中。 ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#center-前言"},{"categories":["算法","数据结构"],"content":" 首先给出基本数据定义 这是宏定义及顺序表定义： #define ElemType int //数据类型 #define MaxSize 100 //最大存储量 //定义顺序表的数据结构s typedef struct { ElemType data[MaxSize]; int length; }SqList; ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:1","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#首先给出基本数据定义"},{"categories":["算法","数据结构"],"content":" 给出基本函数定义： //初始化顺序表 void InitSqLsit(SqList *list); //获取指定位置的元素 ElemType GetElem(SqList list,int p); //获取元素在顺序表中的位置 int locationElem(SqList list,ElemType e); //插入元素在指定位置 int InsertElem(SqList *list,int p,ElemType e); //删除指定位置的元素 int DeletElem(SqList *list,int p,ElemType *e); //遍历顺序表 void Traverse(SqList list); ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:2","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#给出基本函数定义"},{"categories":["算法","数据结构"],"content":" 给出每个函数的具体实现： 初始化： //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } 获取元素位置： /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } 获取指定位置的元素： /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } 插入元素： /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } 删除元素： /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } 遍历顺序表： //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } 最后给出main函数： int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#给出每个函数的具体实现"},{"categories":["算法","数据结构"],"content":" 给出每个函数的具体实现： 初始化： //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } 获取元素位置： /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } 获取指定位置的元素： /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } 插入元素： /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } 删除元素： /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } 遍历顺序表： //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } 最后给出main函数： int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#初始化"},{"categories":["算法","数据结构"],"content":" 给出每个函数的具体实现： 初始化： //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } 获取元素位置： /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } 获取指定位置的元素： /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } 插入元素： /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } 删除元素： /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } 遍历顺序表： //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } 最后给出main函数： int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#获取元素位置"},{"categories":["算法","数据结构"],"content":" 给出每个函数的具体实现： 初始化： //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } 获取元素位置： /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } 获取指定位置的元素： /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } 插入元素： /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } 删除元素： /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } 遍历顺序表： //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } 最后给出main函数： int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#获取指定位置的元素"},{"categories":["算法","数据结构"],"content":" 给出每个函数的具体实现： 初始化： //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } 获取元素位置： /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } 获取指定位置的元素： /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } 插入元素： /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } 删除元素： /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } 遍历顺序表： //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } 最后给出main函数： int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#插入元素"},{"categories":["算法","数据结构"],"content":" 给出每个函数的具体实现： 初始化： //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } 获取元素位置： /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } 获取指定位置的元素： /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } 插入元素： /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } 删除元素： /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } 遍历顺序表： //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } 最后给出main函数： int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#删除元素"},{"categories":["算法","数据结构"],"content":" 给出每个函数的具体实现： 初始化： //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } 获取元素位置： /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } 获取指定位置的元素： /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } 插入元素： /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } 删除元素： /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } 遍历顺序表： //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } 最后给出main函数： int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#遍历顺序表"},{"categories":["算法","数据结构"],"content":" 给出每个函数的具体实现： 初始化： //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } 获取元素位置： /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } 获取指定位置的元素： /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } 插入元素： /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } 删除元素： /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } 遍历顺序表： //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } 最后给出main函数： int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:3","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#最后给出main函数"},{"categories":["算法","数据结构"],"content":" 输出结果： ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:1:4","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#输出结果"},{"categories":["算法","数据结构"],"content":" 以下附上全部代码： /** 说明：顺序表的C语言实现 **/ #include \u003cstdio.h\u003e #define ElemType int #define MaxSize 100 //定义顺序表的数据结构s typedef struct { ElemType data[MaxSize]; int length; }SqList; //初始化顺序表 void InitSqLsit(SqList *list); //获取指定位置的元素 ElemType GetElem(SqList list,int p); //获取元素在顺序表中的位置 int locationElem(SqList list,ElemType e); //插入元素在指定位置 int InsertElem(SqList *list,int p,ElemType e); //删除指定位置的元素 int DeletElem(SqList *list,int p,ElemType *e); //遍历顺序表 void Traverse(SqList list); int main(int argc, char const *argv[]) { int i = 0; ElemType e; SqList list; //初始化顺序表 InitSqLsit(\u0026list); //测试数据 puts(\"随机插入4个数据：\"); for(i = 0; i \u003c 4; i++) { scanf(\"%d\",\u0026e); list.data[i] = e; list.length++; } //插入数据测试 InsertElem(\u0026list,2,6); //删除数据测试 DeletElem(\u0026list,5,\u0026e); Traverse(list); e = GetElem(list,3); printf(\"%d is lie in %d\\n\",3,e); printf(\"4 is lie in :%d\\n\",locationElem(list,4)); return 0; } //初始化顺序表，无返回值； void InitSqLsit(SqList *list) { puts(\"初始化顺序表\"); list-\u003elength = 0; } //遍历顺序表 void Traverse(SqList list) { int i = 0; puts(\"遍历顺序表\"); for(i = 0;i \u003c list.length;i++) printf(\"%d\\t\",list.data[i]); putchar('\\n'); } /** *获取元素在表中的位置，失败返回-1 **/ int locationElem(SqList list,ElemType e) { int i; for( i = 0; i \u003c list.length; i++) { if(e == list.data[i]) { return i+1; //实际位置要加1 } } return -1; } /** *获取指定位置的元素,成功返回元素值，失败返回-1 **/ ElemType GetElem(SqList list,int p) { if(p\u003c0||p\u003elist.length-1) { return -1; } else { return list.data[p - 1]; //p要减1 } } /** *在表中插入元素，失败返回-1，成功返回1 **/ int InsertElem(SqList *list,int p,ElemType e) { if(p\u003c0||p\u003elist-\u003elength||list-\u003elength==MaxSize-1) { return -1; //插入失败，返回-1 } else { puts(\"开始插入数据：\"); int i; for( i = list-\u003elength-1; i\u003e=p-1; i--) //由于从0开始存储所以要减1 { list-\u003edata[i+1] = list-\u003edata[i]; } list-\u003edata[p-1] = e; //这里也要减1 list-\u003elength++; puts(\"插入数据完成。\"); return 1; } } /** *删除指定元素，并保存在e中,成功返回1，失败返回-1 **/ int DeletElem(SqList *list,int p,ElemType *e) { if(p\u003c0||p\u003elist-\u003elength) { return -1; } else { puts(\"开始删除数据：\"); *e = list-\u003edata[p-1]; int i; for( i = p-1; i \u003c list-\u003elength; i++) { list-\u003edata[i] = list-\u003edata[i+1]; } list-\u003elength--; puts(\"删除数据完成。\"); return 1; } } ","date":"2017-03-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/:2:0","series":null,"tags":["算法","数据结构"],"title":"数据结构之顺序表","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%A1%BA%E5%BA%8F%E8%A1%A8/#以下附上全部代码"}]